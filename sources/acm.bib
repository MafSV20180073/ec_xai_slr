@inbook{10.1145/3712256.3726450,
author = {Zhang, Yisong and Yi, Guoxing},
title = {LAOS: Large Language Model-Driven Adaptive Operator Selection for Evolutionary Algorithms},
year = {2025},
isbn = {9798400714658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712256.3726450},
abstract = {Adaptive Operator Selection (AOS) is a strategy in Evolutionary Algorithms (EAs) that dynamically adjusts the application frequency of operators to enhance search efficiency based on online performance feedback. This paper introduces LAOS, an AOS framework driven by Large Language Models (LLMs). We design a meta-prompt to provide optimization state information (such as optimization progress, best fitness, and population diversity) and operator credit assignment, assisting LLMs in making adaptive decisions. Furthermore, LAOS maintains a dual-layer replay buffer structure: the offline layer records historical experiences under fixed operator strategies, while the online layer accumulates dynamically generated experiences during execution. By employing a similar experience sampling strategy, the framework can provide decision-making support for LLMs, enhancing both the efficiency and accuracy of search strategies. Experimental results on continuous numerical optimization and three categories of combinatorial optimization problems validate the effectiveness and generalization capability of LAOS. This study demonstrates the feasibility of leveraging LLMs for AOS, showcasing their potential in enhancing optimization performance and supporting automated algorithm design.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {517–526},
numpages = {10}
}

@inproceedings{10.1145/3520304.3533974,
author = {Bacardit, Jaume and Brownlee, Alexander E. I. and Cagnoni, Stefano and Iacca, Giovanni and McCall, John and Walker, David},
title = {The intersection of evolutionary computation and explainable AI},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3533974},
doi = {10.1145/3520304.3533974},
abstract = {In the past decade, Explainable Artificial Intelligence (XAI) has attracted a great interest in the research community, motivated by the need for explanations in critical AI applications. Some recent advances in XAI are based on Evolutionary Computation (EC) techniques, such as Genetic Programming. We call this trend EC for XAI. We argue that the full potential of EC methods has not been fully exploited yet in XAI, and call the community for future efforts in this field. Likewise, we find that there is a growing concern in EC regarding the explanation of population-based methods, i.e., their search process and outcomes. While some attempts have been done in this direction (although, in most cases, those are not explicitly put in the context of XAI), we believe that there are still several research opportunities and open research questions that, in principle, may promote a safer and broader adoption of EC in real-world applications. We call this trend XAI within EC. In this position paper, we briefly overview the main results in the two above trends, and suggest that the EC community may play a major role in the achievement of XAI.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1757–1762},
numpages = {6},
keywords = {evolutionary computation, explainable artificial intelligence, machine learning, optimization},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3638530.3664179,
author = {Pluhacek, Michal and Kovac, Jozef and Janku, Peter and Kadavy, Tomas and Senkerik, Roman and Viktorin, Adam},
title = {A Critical Examination of Large Language Model Capabilities in Iteratively Refining Differential Evolution Algorithm},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664179},
doi = {10.1145/3638530.3664179},
abstract = {In this study, we investigate the applicability, challenges, and effectiveness of the advanced large language model GPT 4 Turbo in enhancing the selected metaheuristic algorithm, which is Differential Evolution. Our research primarily examines whether iterative, repetitive prompting could lead to progressive improvements in algorithm performance. We also explore the potential of developing enhanced algorithms through this process that markedly surpass the established baseline in terms of performance. In addition, the impact of the model's temperature parameter on these improvements is evaluated. As part of our diverse testing approach, we conduct an experiment where the best-performing algorithm from the initial phase is used as a new baseline. This step is to determine if further refinement via GPT 4 Turbo can achieve even better algorithmic efficiency. Finally, we have performed the benchmarking comparison of selected enhanced variants against the top three algorithms from the CEC 2022 competition.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1855–1862},
numpages = {8},
keywords = {evolutionary computation, large language models, metaheuristic optimization, automatic algorithm design, GPT},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@article{10.1145/3610388.3610389,
author = {Bacardit, Jaume and Brownlee, Alexander and Cagnoni, Stefano and Iacca, Giovanni and McCall, John and Walker, David},
title = {Evolutionary Computation and Explainable AI Towards ECXAI 2023: A Year in Review},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
url = {https://doi.org/10.1145/3610388.3610389},
doi = {10.1145/3610388.3610389},
abstract = {At GECCO 2022, we organized the first Workshop on Evolutionary Computation and Explainable AI (ECXAI). With no pretence at completeness, this paper briefly comments on its outcome, what has happened recently in the field, and our expectations for the near future and, in particular, for the upcoming second edition of EXCAI, at GECCO 2023.},
journal = {SIGEVOlution},
month = jul,
articleno = {1},
numpages = {6},
keywords = {evolutionary computation and optimisation, explainable artificial intelligence, machine learning}
}

@inproceedings{10.1145/3583131.3590444,
author = {Andersen, Hayden and Lensen, Andrew and Browne, Will and Mei, Yi},
title = {Producing Diverse Rashomon Sets of Counterfactual Explanations with Niching Particle Swarm Optimization Algorithms},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590444},
doi = {10.1145/3583131.3590444},
abstract = {Counterfactual explanation is a popular eXplainable AI technique, that gives contrastive explanations to answer potential "what-if" questions about the workings of machine learning models. However, research into how explanations are understood by human beings has shown that an optimal explanation should be both selected and social, providing multiple varying explanations for the same event that allow a user to select specific explanations based on prior beliefs and cognitive biases. In order to provide such explanations, a Rashomon set of explanations can be created: a set of explanations utilising different features in the data. Current work to generate counterfactual explanations does not take this need into account, only focusing on producing a single optimal counterfactual.This work presents a novel method for generating a diverse Rashomon set of counterfactual explanations using the final population from a Particle Swarm Optimisation (PSO) algorithm. It explores a selection of PSO niching algorithms for PSO and evaluates the best algorithm to produce these sets. Finally, the ability of this method to be implemented and trusted by users is discussed.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {393–401},
numpages = {9},
keywords = {machine learning, particle swarm optimisation, explainable AI},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1145/3638530.3664173,
author = {Murphy, Jared and Desell, Travis},
title = {Minimizing the EXA-GP Graph-Based Genetic Programming Algorithm for Interpretable Time Series Forecasting},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664173},
doi = {10.1145/3638530.3664173},
abstract = {This work provides a modification to the Evolutionary eXploration of Augmenting Memory Models (EXA-GP) graph-based genetic programming (GGP) algorithm, enabling it to produce time series forecasting (TSF) equations that are vastly more simple and interpretable than the original implementation without heavily compromising on predictive ability. This is accomplished by eliminating a majority of all the trainable constants and initializing the algorithm with a seed computational graph in the form of using a parameter's value at time t as the forecast for the parameter's value at time t + 1. This minimal version of EXA-GP (EXA-GP-MIN) is compared to EXA-GP and EXAMM, a full blown neuroevolution algorithm for evolving recurrent neural networks for TSF, on a suite of six real world benchmark problems, with MIN-EXA-GP showing the best forecasting ability on four of the six benchmarks with significantly more interpretable genetic programs.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1686–1690},
numpages = {5},
keywords = {time series forecasting, graph-based genetic programming, neuroevolution, recurrent neural networks},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3638529.3654145,
author = {Sijben, Evi and Jansen, Jeroen and Bosman, Peter and Alderliesten, Tanja},
title = {Function Class Learning with Genetic Programming: Towards Explainable Meta Learning for Tumor Growth Functionals},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638529.3654145},
doi = {10.1145/3638529.3654145},
abstract = {Paragangliomas are rare, primarily slow-growing tumors for which the underlying growth pattern is unknown. Therefore, determining the best care for a patient is hard. Currently, if no significant tumor growth is observed, treatment is often delayed, as treatment itself is not without risk. However, by doing so, the risk of (irreversible) adverse effects due to tumor growth may increase. Being able to predict the growth accurately could assist in determining whether a patient will need treatment during their lifetime and, if so, the timing of this treatment. The aim of this work is to learn the general underlying growth pattern of paragangliomas from multiple tumor growth data sets, in which each data set contains a tumor's volume over time. To do so, we propose a novel approach based on genetic programming to learn a function class, i.e., a parameterized function that can be fit anew for each tumor. We do so in a unique, multi-modal, multi-objective fashion to find multiple potentially interesting function classes in a single run. We evaluate our approach on a synthetic and a real-world data set. By analyzing the resulting function classes, we can effectively explain the general patterns in the data.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1354–1362},
numpages = {9},
keywords = {genetic programming, function class learning, explainable AI},
location = {Melbourne, VIC, Australia},
series = {GECCO '24}
}

@inproceedings{10.1145/3512290.3528849,
author = {Uriot, Thomas and Virgolin, Marco and Alderliesten, Tanja and Bosman, Peter A. N.},
title = {On genetic programming representations and fitness functions for interpretable dimensionality reduction},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528849},
doi = {10.1145/3512290.3528849},
abstract = {Dimensionality reduction (DR) is an important technique for data exploration and knowledge discovery. However, most of the main DR methods are either linear (e.g., PCA), do not provide an explicit mapping between the original data and its lower-dimensional representation (e.g., MDS, t-SNE, isomap), or produce mappings that cannot be easily interpreted (e.g., kernel PCA, neural-based autoencoder). Recently genetic programming (GP) has been used to evolve interpretable DR mappings in the form of symbolic expressions. There exists a number of ways in which GP can be used to this end and no study exists that performs a comparison. In this paper, we fill this gap by comparing existing GP methods as well as devising new ones. We evaluate our methods on several benchmark datasets based on predictive accuracy and on how well the original features can be reconstructed using the lower-dimensional representation only. Finally we qualitatively assess the resulting expressions and their complexity. We find that various GP methods can be competitive with state-of-the-art DR algorithms and that they have the potential to produce interpretable DR mappings.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {458–466},
numpages = {9},
keywords = {dimensionality reduction, genetic programming, interpretability unsupervised learning},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@article{10.1145/3643688,
author = {Nadizar, Giorgia and Rovito, Luigi and De Lorenzo, Andrea and Medvet, Eric and Virgolin, Marco},
title = {An Analysis of the Ingredients for Learning Interpretable Symbolic Regression Models with Human-in-the-loop and Genetic Programming},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3643688},
doi = {10.1145/3643688},
abstract = {Interpretability is a critical aspect to ensure a fair and responsible use of machine learning (ML) in high-stakes applications. Genetic programming (GP) has been used to obtain interpretable ML models because it operates at the level of functional building blocks: if these building blocks are interpretable, there is a chance that their composition (i.e., the entire ML model) is also interpretable. However, the degree to which a model is interpretable depends on the observer. Motivated by this, we study a recently-introduced human-in-the-loop system that allows the user to steer GP’s generation process to their preferences, which shall be online-learned by an artificial neural network (ANN). We focus on the generation of ML models as analytical functions (i.e., symbolic regression) as this is a key problem in interpretable ML, and propose a two-fold contribution. First, we devise more general representations for the ML models for the ANN to learn upon, to enable the application of the system to a wider range of problems. Second, we delve into a deeper analysis of the system’s components. To this end, we propose an incremental experimental evaluation, aimed at (1) studying the effectiveness by which an ANN can capture the perceived interpretability for simulated users, (2) investigating how the GP’s outcome is affected across different simulated user feedback profiles, and (3) determining whether humans participants would prefer models that were generated with or without their involvement. Our results pose clarity on pros and cons of using a human-in-the-loop approach to discover interpretable ML models with GP.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = feb,
articleno = {5},
numpages = {30},
keywords = {Explainable artificial intelligence, interpretable machine learning, active learning, neural networks, genetic programming, deep learning, evolutionary computation, evolutionary algorithms, explainable evolutionary computation}
}

@inproceedings{10.1145/3512290.3528723,
author = {Wang, Shaolin and Mei, Yi and Zhang, Mengjie},
title = {Local ranking explanation for genetic programming evolved routing policies for uncertain capacitated Arc routing problems},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528723},
doi = {10.1145/3512290.3528723},
abstract = {The Uncertain Capacitated Arc Routing Problem (UCARP) is a well-known combinatorial optimisation problem that has many real-world applications. Genetic Programming is usually utilised to handle UCARP by evolving effective routing policies, which can respond to the uncertain environment in real-time. Previous studies mainly focus on the effectiveness of the routing policies but ignore the interpretability. In this paper, we focus on post-hoc interpretability, which explains a pre-trained complex routing policy. Unlike the existing explanation methods for classification/regression models, the behaviour of a routing policy is characterised as a ranking process rather than predicting a single output. To address this issue, this paper proposes a Local Ranking Explanation (LRE) method for GP-evolved routing policies for UCARP. Given a UCARP decision situation, LRE trains a linear model that gives the same ranks of the candidate tasks as those of the explained routing policy. The experimental results demonstrate that LRE can obtain more interpretable linear models that have highly correlated and consistent behaviours with the original routing policy in most decision situations. By analysing coefficients and attribute importance of the linear model, we managed to provide a local explanation of the original routing policy in a decision situation.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {314–322},
numpages = {9},
keywords = {genetic programming, hyper-heuristic, local explanation, uncertain capacitated Arc routing problem},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3638530.3664155,
author = {Frazier, James Gunder and Helmuth, Thomas},
title = {Explaining Automatically Designed Software Defined Perimeters with a Two Phase Evolutionary Computation System},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664155},
doi = {10.1145/3638530.3664155},
abstract = {Software Defined Perimeter (SDP) is a zero-trust network-isolation defense technique which aims to limit security risks by giving dynamic account type assignments to network users. Despite SDP being proven as an effective defense strategy in various domains, it has yet to see wide-spread use due to its drawbacks. One of SDP's most pressing issues is the need for an expert to manually configure it for each unique application. Here we describe a novel system for designing SDP networks called SDPush which can automatically design and analyze possible configurations for a given network with user-specifications. Since there is not a systematic approach for account type design and assignment, we develop a two-step optimization system consisting of a bitstring genetic algorithm and a genetic programming sub-system for designing and evaluating SDP networks respectively. In order to evolve an SDP configuration exhibiting the user-specified characteristics while also minimizing security risk, we implement our system to support multi-objective search spaces by providing the system's training set with different cases aimed at evaluating different aspects of the network configuration. We present initial results of experiments on networks of varying size and characteristic requirements.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1527–1535},
numpages = {9},
keywords = {evolutionary computation, decision making, cybersecurity, networks},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3512290.3528801,
author = {Angrick, Sebastian and Bals, Ben and Hastrich, Niko and Kleissl, Maximilian and Schmidt, Jonas and Dosko\v{c}, Vanja and Molitor, Louise and Friedrich, Tobias and Katzmann, Maximilian},
title = {Towards explainable real estate valuation via evolutionary algorithms},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528801},
doi = {10.1145/3512290.3528801},
abstract = {Human lives are increasingly influenced by algorithms, which therefore need to meet higher standards not only in accuracy but also with respect to explainability. This is especially true for high-stakes areas such as real estate valuation. Unfortunately, the methods applied there often exhibit a trade-off between accuracy and explainability.One explainable approach is case-based reasoning (CBR), where each decision is supported by specific previous cases. However, such methods can be wanting in accuracy. The unexplainable machine learning approaches are often observed to provide higher accuracy but are not scrutable in their decision-making.In this paper, we apply evolutionary algorithms (EAs) to CBR predictors in order to improve their performance. In particular, we deploy EAs to the similarity functions (used in CBR to find comparable cases), which are fitted to the data set at hand. As a consequence, we achieve higher accuracy than state-of-the-art deep neural networks (DNNs), while keeping interpretability and explainability.These results stem from our empirical evaluation on a large data set of real estate offers where we compare known similarity functions, their EA-improved counterparts, and DNNs. Surprisingly, DNNs are only on par with standard CBR techniques. However, using EA-learned similarity functions does yield an improved performance.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1130–1138},
numpages = {9},
keywords = {case-based reasoning, evolutionary algorithms, neural networks, real estate valuation},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3510003.3510137,
author = {Fan, Ming and Wei, Wenying and Jin, Wuxia and Yang, Zijiang and Liu, Ting},
title = {Explanation-guided fairness testing through genetic algorithm},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510137},
doi = {10.1145/3510003.3510137},
abstract = {The fairness characteristic is a critical attribute of trusted AI systems. A plethora of research has proposed diverse methods for individual fairness testing. However, they are suffering from three major limitations, i.e., low efficiency, low effectiveness, and model-specificity. This work proposes ExpGA, an explanation-guided fairness testing approach through a genetic algorithm (GA). ExpGA employs the explanation results generated by interpretable methods to collect high-quality initial seeds, which are prone to derive discriminatory samples by slightly modifying feature values. ExpGA then adopts GA to search discriminatory sample candidates by optimizing a fitness value. Benefiting from this combination of explanation results and GA, ExpGA is both efficient and effective to detect discriminatory individuals. Moreover, ExpGA only requires prediction probabilities of the tested model, resulting in a better generalization capability to various models. Experiments on multiple real-world benchmarks, including tabular and text datasets, show that ExpGA presents higher efficiency and effectiveness than four state-of-the-art approaches.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {871–882},
numpages = {12},
keywords = {explanation result, fairness testing, genetic algorithm},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3583133.3596353,
author = {Fyvie, Martin and Mccall, John and Christie, Lee and Brownlee, Alexander},
title = {Explaining a Staff Rostering Genetic Algorithm using Sensitivity Analysis and Trajectory Analysis.},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3596353},
doi = {10.1145/3583133.3596353},
abstract = {In the field of Explainable AI, population-based search metaheuristics are of growing interest as they become more widely used in critical applications. The ability to relate key information regarding algorithm behaviour and drivers of solution quality to an end-user is vital. This paper investigates a novel method of explanatory feature extraction based on analysis of the search trajectory and compares the results to those of sensitivity analysis using "Weighted Ranked Biased Overlap". We apply these techniques to search trajectories generated by a genetic algorithm as it solves a staff rostering problem. We show that there is a significant overlap between these two explainability methods when identifying subsets of rostered workers whose allocations are responsible for large portions of fitness change in an optimization run. Both methods identify similar patterns in sensitivity, but our method also draws out additional information. As the search progresses, the techniques reveal how individual workers increase or decrease in the influence on the overall rostering solution's quality. Our method also helps identify workers with a lower impact on overall solution fitness and at what stage in the search these individuals can be considered highly flexible in their roster assignment.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {1648–1656},
numpages = {9},
keywords = {evolutionary algorithms, principal component analysis, algorithm trajectories, sensitivity analysis, explainable AI (XAI)},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@article{10.1145/3731456,
author = {Fyvie, Martin and McCall, John A. W. and Christie, Lee A.},
title = {Towards Explainable Metaheuristics: Feature Mining of Search Trajectories through Principal Component Projection},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/3731456},
doi = {10.1145/3731456},
abstract = {While population-based metaheuristics have proven useful for refining and improving explainable AI systems, they are seldom the focus of explanatory approaches themselves. This stems from their inherently stochastic, population-driven searches, which complicate the use of standard explainability techniques. In this article, we present a method to identify which decision variables have the greatest impact during an algorithm’s trajectory from random initialsation to convergence. We apply Principal Component Analysis to project each population onto a lower-dimensional space, then introduce two metrics—Mean Variable Contribution and Proportion of Aligned Variables—to identify the variables most responsible for guiding the search. Using four different population-based methods (Particle Swarm Optimisation, Genetic Algorithm, Differential Evolution, and Covariance Matrix Adaptation Evolution Strategy) on 24 BBOB benchmark functions in 10 dimensions, we find that these metrics highlight meaningful variable relationships and provide a window into each method’s search dynamics. By comparing the features extracted across algorithms and problems, we illustrate how certain variable subsets consistently drive major improvements in solution quality. In doing so, new evolutionary algorithm variants can be designed to take advantage of these influential variables, while also identifying underutilised variables that may benefit alternative search strategies.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = may,
articleno = {12},
numpages = {30},
keywords = {Evolutionary Algorithms, Principal Component Analysis, Algorithm Trajectories, Visualisation, Population Diversity}
}

@inproceedings{10.1145/3520304.3533966,
author = {Singh, Manjinder and Brownlee, Alexander E. I. and Cairns, David},
title = {Towards explainable metaheuristic: mining surrogate fitness models for importance of variables},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3533966},
doi = {10.1145/3520304.3533966},
abstract = {Metaheuristic search algorithms look for solutions that either maximise or minimise a set of objectives, such as cost or performance. However most real-world optimisation problems consist of nonlinear problems with complex constraints and conflicting objectives.The process by which a GA arrives at a solution remains largely unexplained to the end-user. A poorly understood solution will dent the confidence a user has in the arrived at solution. We propose that investigation of the variables that strongly influence solution quality and their relationship would be a step toward providing an explanation of the near-optimal solution presented by a metaheuristic.Through the use of four benchmark problems we use the population data generated by a Genetic Algorithm (GA) to train a surrogate model, and investigate the learning of the search space by the surrogate model. We compare what the surrogate has learned after being trained on population data generated after the first generation and contrast this with a surrogate model trained on the population data from all generations.We show that the surrogate model picks out key characteristics of the problem as it is trained on population data from each generation. Through mining the surrogate model we can build a picture of the learning process of a GA, and thus an explanation of the solution presented by the GA. The aim being to build trust and confidence in the end-user about the solution presented by the GA, and encourage adoption of the model.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1785–1793},
numpages = {9},
keywords = {explainability, fitness function, genetic algorithms, interpretable, optimization, surrogate model},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

