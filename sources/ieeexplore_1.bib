@INPROCEEDINGS{9870283,
  author={Andersen, Hayden and Lensen, Andrew and Browne, Will N. and Mei, Yi},
  booktitle={2022 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Evolving Counterfactual Explanations with Particle Swarm Optimization and Differential Evolution}, 
  year={2022},
  volume={},
  number={},
  pages={01-08},
  abstract={Counterfactual explanations are a popular eXplainable AI technique, used to provide contrastive answers to “what-if” questions. These explanations are consistent with the way that an everyday person will explain an event, and have been shown to satisfy the ‘right to explanation’ of the European data regulations. Despite this, current work to generate counterfactual explanations either makes assumptions about the model being explained or utlises algorithms that perform suboptimally on continuous data. This work presents two novel algorithms to generate counterfactual explanations using Particle Swarm Optimization (PSO) and Differential Evolution (DE). These are shown to provide effective post-hoc explanations that make no assumptions about the underlying model or data structure. In particular, PSO is shown to generate counterfactual explanations that utilise significantly fewer features to generate sparser explanations when compared to previous related work.},
  keywords={Sociology;Europe;Production;Data structures;Data models;Regulation;Particle swarm optimization;explainable AI;counterfactual explanation;particle swarm optimization;differential evolution},
  doi={10.1109/CEC55065.2022.9870283},
  ISSN={},
  month={July},}@ARTICLE{10536083,
  author={AlJalaud, Ebtisam and Hosny, Manar},
  journal={IEEE Access}, 
  title={Counterfactual Explanation of AI Models Using an Adaptive Genetic Algorithm With Embedded Feature Weights}, 
  year={2024},
  volume={12},
  number={},
  pages={74993-75009},
  abstract={Explainable Artificial Intelligence (XAI) is a cutting-edge AI development motivated by the need for transparency of black-box models in AI systems. This transparency enhances user trust, facilitates accountability, and enables a better understanding of AI systems decisions, especially in critical applications where insights into decision processes are essential. These benefits have increased XAI research interest, aiming to provide techniques for interpreting and understanding the behavior of intelligent models. Counterfactual explanation is a popular technique for model interpretation based on updating a few features such that the outcome of an AI model is changed. Users can gain insights into the critical features or factors influencing the AI system’s decision by analyzing these counterfactuals. However, most counterfactual techniques require more qualifications, such as simplicity, robustness, and coherence. In this research, we propose a novel approach, Adaptive Feature Weight Genetic Explanation (AFWGE), for generating counterfactual explanations of AI models, where a custom genetic algorithm (GA) is employed, incorporating adaptive feature weights to enhance the algorithm’s performance. Experimental results on four benchmark datasets show that AFWGE allows for the adaptation of feature weights during the evolutionary process, producing more effective counterfactual explanations with superior proximity, sparsity, plausibility, and actionability. Furthermore, it emphasizes feature weights as reliable indicators of the significance of the model’s features, providing valuable insights for interpreting the model. AFWGE not only advances the field of counterfactual explanation generation but also establishes a robust framework for assessing feature importance in machine learning models.},
  keywords={Adaptation models;Genetic algorithms;Predictive models;Data models;Explainable AI;Decision making;Closed box;Machine learning;Artificial neural networks;Explainable artificial intelligence;counterfactual explanation;genetic algorithm;machine learning;artificial neural network},
  doi={10.1109/ACCESS.2024.3404043},
  ISSN={2169-3536},
  month={},}@ARTICLE{11018333,
  author={ELwahsh, Haitham and Bakhiet, Ali and Alirr, Omar Ibrahim and Khalifa, Tarek and Alsabaan, Maazen and Ibrahem, Mohamed I. and El-Shafeiy, Engy},
  journal={IEEE Access}, 
  title={Explainable Artificial Intelligence in Malignant Lymphoma Classification: Optimized DenseNet121 Deep Learning Approach With Particle Swarm Optimization and Genetic Algorithm}, 
  year={2025},
  volume={13},
  number={},
  pages={98639-98655},
  abstract={One of the forms of cancerous tumors that can be fatal is malignant lymphoma. Histopathological examination of lymphoma tissue images is a diagnostic technique for detecting malignant lymphomas. Differentiating lymphoma subtypes manually is challenging due to their similar morphological features. This paper introduces a cutting-edge methodology for classifying malignant lymphoma using Hematoxylin and Eosin (H&E)-stained biopsy images. It integrates feature reduction using Particle Swarm Optimization (PSO) and model optimization through a Genetic Algorithm (GA) in conjunction with DenseNet121 architecture. The DenseNet121 model, which employs four distinct freezing strategies, extracts features from the images, generating an initial 4096 features per image. These features are subsequently reduced by more than 54.4% using PSO, thereby enhancing the computational efficiency without sacrificing classification accuracy. The trained GA Deep Neural Network (DNN) model achieved a significant score of 96.77% testing accuracy with high precision, recall and F1 scores as well. The results after comparison with other existing state-of-the-art models showed the usefulness of the proposed approach in terms of performance as well as the consumption of resources for medical imaging classification. It is recommended that the combined application of PSO for feature reduction and GA for model optimization can be successfully used for improving accuracy rate of such algorithms while reducing computation time. This technique is expected to be further developed and applied in different tasks of computer aided diagnosis of malignant lymphoma in particular the tasks with requirements for explainable artificial intelligence.},
  keywords={Feature extraction;Cancer;Genetic algorithms;Computational modeling;Medical diagnostic imaging;Deep learning;Particle swarm optimization;Optimization;Explainable AI;Accuracy;Artificial intelligent;malignant lymphoma;optimized DenseNet121;deep learning;particle swarm optimization;genetic algorithm},
  doi={10.1109/ACCESS.2025.3575364},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10774879,
  author={Bhavana, Bs and Raju, Shrusti P and Kodipalli, Ashwini and Rao, Trupthi},
  booktitle={2024 3rd International Conference for Advancement in Technology (ICONAT)}, 
  title={Carbon Dioxide Emission by Cars: A Performance Analysis of Diverse Machine Learning Regressors and Interpretation using Explainable AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study introduces a creative paradigm for Emission of Carbon by integrating machine learning and explainable artificial intelligence (XAI) techniques. A variety of regression methodologies, including Ridge, Linear, Support Vector, and Lasso regressions, were used to select specific machine learning algorithms. Additionally, the regression and regressor ensemble learning framework were employed. Through a comparative analysis, boosting algorithms, especially XG Boost, demonstrate remarkable error. This enhanced the final XG Boost model’s error to a magnificent 9.195e-07. The incorporation of XAI methods, such as SHAP values and LIME, provides an intricate understanding of feature importance, feature contribution and local explanation, elevating model interpretability. This research’s significance lies in its potential to transform carbon emission screening, offering low error precision, resilience, and transparency. The combination of machine learning regressors and XAI techniques contributes to advanced screening tools, fostering trust among ecologist and researchers.},
  keywords={Machine learning algorithms;Explainable AI;Biological system modeling;Carbon dioxide;Machine learning;Transforms;Boosting;Vectors;Reliability;Resilience;Carbon Emission;Machine Learning;Boosting;Explainable AI;LIME;SHAP},
  doi={10.1109/ICONAT61936.2024.10774879},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11019825,
  author={Maram, Balajee and Maram, Rohan Raj},
  booktitle={2025 Fourth International Conference on Smart Technologies, Communication and Robotics (STCR)}, 
  title={Particle Swarm Optimization based Explainable Deep Learning Models for Celiac Disease Prediction}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Celiac disease is an autoimmune chronic disease, which occurs due to the ingestion of gluten, and affects about 1% of the world's population, diagnosed early in life to avoid potential damage afterward. Article on Particle Swarm Optimization-based explainable deep learning model for the prediction of celiac disease is discussed in this section. It applies PSO for feature selection and, therefore, does not offer redundant data to the extent of up to 35% with hyperparameter tuning that improves the efficiency of the model by up to 15%. The attention mechanisms along with SHAP (SHapley Additive exPlanations) would be incorporated in offering insights into the interpretable key genetic and serological factors making the predictions in the optimized deep learning model. It is experimented on some real datasets also, so as to get the validation on a large scale. Then, it also got 94.3% accuracy and resulted in 20% reduction of computation overhead. This approach makes clinical decision with good strong diagnostics.},
  keywords={Deep learning;Accuracy;Explainable AI;Computational modeling;Biological system modeling;Predictive models;Feature extraction;Particle swarm optimization;Tuning;Diseases;Celiac disease prediction;Particle Swarm Optimization;Explainable AI;Deep learning models;Clinical decision support},
  doi={10.1109/STCR62650.2025.11019825},
  ISSN={},
  month={May},}@INPROCEEDINGS{10928533,
  author={Wu, Youlan},
  booktitle={2024 4th International Conference on Communication Technology and Information Technology (ICCTIT)}, 
  title={Line Fault Detection Strategy for Multi-Microcontroller Systems Based on PSO-GA Optimized SVM}, 
  year={2024},
  volume={},
  number={},
  pages={490-495},
  abstract={In this paper, we propose an innovative real-time fault detection method for multi-monolithic computer (MCU) systems that combines a lightweight interpretable artificial intelligence (XAI)-enhanced support vector machine (SVM) and an improved hybrid particle swarm optimization (PSO)-genetic algorithm (GA) optimization technique. XAI-SVM achieves efficient and interpretable fault detection by integrating the high accuracy of a traditional SVM with the XAI-SVM achieves efficient and interpretable fault detection by combining the high accuracy of traditional SVM with the interpretability of Shapley Additive Properties (SHAP), while optimized for resource constrained MCU environments. The improved PSO-GA hybrid optimization algorithm effectively balances global exploration and local exploitation by introducing an adaptive weighting mechanism, which significantly improves the optimization efficiency of XAI-SVM hyperparameters. In addition, a distributed fault detection framework is developed in this paper, which improves the reliability and robustness of the overall system by coordinating multiple MCUs and introducing a lightweight consensus mechanism. Test results show that compared with existing methods, the method proposed in this paper achieves higher fault detection accuracy and interpretability in a multi-MCU system while maintaining excellent real-time performance. The method shows significant improvement in detection efficiency, result interpretability and system adaptability, and is particularly suitable for complex industrial application scenarios requiring fast and accurate fault identification and diagnosis.},
  keywords={Support vector machines;Accuracy;Additives;Fault detection;Simulation;Real-time systems;Robustness;Artificial intelligence;Particle swarm optimization;Optimization;XAI-SVM;PSO-GA hybrid optimization;multi-MCU system;real-time fault detection;interpretable artificial intelligence},
  doi={10.1109/ICCTIT64404.2024.10928533},
  ISSN={},
  month={Dec},}@ARTICLE{10730793,
  author={Zhou, Ryan and Bacardit, Jaume and Brownlee, Alexander E. I. and Cagnoni, Stefano and Fyvie, Martin and Iacca, Giovanni and McCall, John and van Stein, Niki and Walker, David J. and Hu, Ting},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Evolutionary Computation and Explainable AI: A Roadmap to Understandable Intelligent Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={Artificial intelligence methods are being increasingly applied across various domains, but their often opaque nature has raised concerns about accountability and trust. In response, the field of explainable AI (XAI) has emerged to address the need for human-understandable AI systems. Evolutionary computation (EC), a family of powerful optimization and learning algorithms, offers significant potential to contribute to XAI, and vice versa. This paper provides an introduction to XAI and reviews current techniques for explaining machine learning models. We then explore how EC can be leveraged in XAI and examine existing XAI approaches that incorporate EC techniques. Furthermore, we discuss the application of XAI principles within EC itself, investigating how these principles can illuminate the behavior and outcomes of EC algorithms, their (automatic) configuration, and the underlying problem landscapes they optimize. Finally, we discuss open challenges in XAI and highlight opportunities for future research at the intersection of XAI and EC. Our goal is to demonstrate EC’s suitability for addressing current explainability challenges and to encourage further exploration of these methods, ultimately contributing to the development of more understandable and trustworthy ML models and EC algorithms.},
  keywords={Artificial intelligence;Complexity theory;Optimization;Predictive models;Decision making;Data models;Measurement;Computational modeling;Numerical models;Machine learning algorithms;Explainability;Interpretability;Evolutionary Computation;Machine Learning},
  doi={10.1109/TEVC.2024.3476443},
  ISSN={1941-0026},
  month={},}@ARTICLE{10632141,
  author={Dastile, Xolani and Celik, Turgay},
  journal={IEEE Access}, 
  title={Counterfactual Explanations With Multiple Properties in Credit Scoring}, 
  year={2024},
  volume={12},
  number={},
  pages={110713-110728},
  abstract={EXplainable Artificial Intelligence (XAI) aims to reveal the reasons behind predictions from non-transparent classifiers. Explanations of automated decisions are important in critical domains such as finance, legal, and health. As a result, researchers and practitioners in recent years have actively worked on developing techniques that explain decisions from machine learning algorithms. For instance, an explanation technique called counterfactual explanation has recently been gaining traction in XAI. The interest in counterfactual explanations stems from the ability of the explanations to reveal what could have been different to achieve a desired outcome, as opposed to only highlighting important features. For instance, if a customer’s loan application is denied by the bank, a counterfactual will indicate the changes required for the customer to qualify for the loan in the future. For a counterfactual to be considered effective, several counterfactual properties must hold. This paper proposes a novel optimization formulation designed to generate counterfactual explanations that possess multiple properties concurrently. The efficacy of the proposed method is assessed on a publicly available credit dataset. The results showed a trade-off between validity and sparsity, which are both parts of a suite of counterfactual properties. Furthermore, the results showed that our proposed approach compromises validity to some degree but strikes a good balance between validity and sparsity.},
  keywords={Machine learning;Optimization methods;Explainable AI;Codes;Vectors;Random forests;Prototypes;Artificial intelligence;Counterfactual explanations;credit scoring;optimization;eXplainable Artificial Intelligence (XAI)},
  doi={10.1109/ACCESS.2024.3441037},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10862800,
  author={Chelliah, Balika J and Gahra, Sarghi Kaur and Senthilselvi, A.},
  booktitle={2024 International Conference on Intelligent Computing and Sustainable Innovations in Technology (IC-SIT)}, 
  title={Enhancing PCOS Prediction Using Machine Learning and Explainable AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Polycystic Ovary Syndrome (PCOS) is a prevalent endocrine disorder among women of reproductive age, requiring timely diagnosis and intervention to manage associated health risks. This research explores the application of various machine learning (ML) techniques for detecting PCOS, providing a comparative analysis of nine advanced methodologies: Extreme Learning Machine (ELM), Isolation Forest, Factorization Machines (FM), Multivariate Gaussian Process (MGP), Non-negative Matrix Factorization (NMF), Gaussian Processes (GP), Deep Belief Networks (DBN), Particle Swarm Optimization (PSO), and Long Short-Term Memory (LSTM). The integration of Explainable AI techniques enhances the interpretability of these models, offering greater transparency in the decision-making process. The study leverages a dataset containing PCOS-related data to validate the proposed approach, evaluating the strengths and limitations of each ML model in terms of accuracy, interpretability, and diagnostic capability. This research aims to bridge the gap between advanced computational techniques and clinical applications, contributing to the growing field of ML-driven healthcare diagnostics. The findings highlight the potential of combining diverse machine learning frameworks to improve early detection and personalized treatment strategies for PCOS, ultimately improving patient outcomes.},
  keywords={Machine learning algorithms;Frequency modulation;Explainable AI;Extreme learning machines;Computational modeling;Gaussian processes;Forestry;Medical services;Particle swarm optimization;Long short term memory;Extreme Learning Machine;Isolation Forest;Deep Belief Networks;Long Short-Term Memory},
  doi={10.1109/IC-SIT63503.2024.10862800},
  ISSN={},
  month={Nov},}@ARTICLE{10969993,
  author={Kim, Yonggab and Khir, Reem and Lee, Seokcheon},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Enhancing Genetic Algorithm with Explainable Artificial Intelligence for Last-Mile Routing}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Traditional evolutionary optimization algorithms often presuppose straightforward constraints and objective functions. However, in many real-world optimization problems, a clear-cut objective may be absent or hard to evaluate. Given these challenges, surrogate models have gained attention as proxies for evaluating objective functions or constraints. In this research, we leverage Machine Learning (ML) based surrogate modeling and Explainable Artificial Intelligence (XAI) to improve the performance of Genetic Algorithm (GA) for route sequencing problem. Our framework utilizes ML to capture the nuanced interplay within environmental data while using XAI to identify their relative importance, to ultimately uncover tacit knowledge embedded in desired solutions. The goal is to use ML and XAI to guide the GA search by identifying and utilizing significant genes, i.e., genes that produce high-quality solutions in a more efficient manner. The proposed approach is data-driven, focusing on enhancing core GA components, including (1) enhanced chromosome generation and initialization, (2) informed genetic operators, and (3) refined evaluation of fitness function. We demonstrate our framework using real-world data from the Amazon 2021 Last Mile Challenge as a case study. Our methodology extracts drivers’ preferred routing characteristics and applies these insights to generate new routes. Our method is shown to generate high-quality routes when compared to other approaches from the literature, demonstrating improvements in the convergence and effectiveness of GA enhanced by XAI. The code for this publication can be found at https://zenodo.org/records/15227301.},
  keywords={Genetic algorithms;Explainable AI;Evolutionary computation;Optimization;Routing;Computational modeling;Linear programming;Predictive models;Convergence;Biological cells;machine learning;explainable AI;genetic algorithm;last-mile routing},
  doi={10.1109/TEVC.2025.3562243},
  ISSN={1941-0026},
  month={},}@ARTICLE{10930810,
  author={Sanderson, Jacob and Mao, Hua and Woo, Wai Lok},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={GradCFA: A Hybrid Gradient-Based Counterfactual and Feature Attribution Explanation Algorithm for Local Interpretation of Neural Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Explainable Artificial Intelligence (XAI) is increasingly essential as AI systems are deployed in critical fields such as healthcare and finance, offering transparency into AI-driven decisions. Two major XAI paradigms, counterfactual explanations (CFX) and feature attribution (FA), serve distinct roles in model interpretability. This study introduces GradCFA, a hybrid framework combining CFX and FA to improve interpretability by explicitly optimizing feasibility, plausibility, and diversity—key qualities often unbalanced in existing methods. Unlike most CFX research focused on binary classification, GradCFA extends to multi-class scenarios, supporting a wider range of applications. We evaluate GradCFA’s validity, proximity, sparsity, plausibility, and diversity against state-of-the-art methods, including Wachter, DiCE, CARE for CFX, and SHAP for FA. Results show GradCFA effectively generates feasible, plausible, and diverse counterfactuals while offering valuable FA insights. By identifying influential features and validating their impact, GradCFA advances AI interpretability.},
  keywords={Artificial intelligence;Explainable AI;Computational modeling;Diversity reception;Medical services;Measurement;Law;Genetic algorithms;Training;Standards;Counterfactual Explanations;Feature Attribution;Explainable AI;Interpretable Machine Learning},
  doi={10.1109/TAI.2025.3552057},
  ISSN={2691-4581},
  month={},}@ARTICLE{9965435,
  author={Mei, Yi and Chen, Qi and Lensen, Andrew and Xue, Bing and Zhang, Mengjie},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Explainable Artificial Intelligence by Genetic Programming: A Survey}, 
  year={2023},
  volume={27},
  number={3},
  pages={621-641},
  abstract={Explainable artificial intelligence (XAI) has received great interest in the recent decade, due to its importance in critical application domains, such as self-driving cars, law, and healthcare. Genetic programming (GP) is a powerful evolutionary algorithm for machine learning. Compared with other standard machine learning models such as neural networks, the models evolved by GP tend to be more interpretable due to their model structure with symbolic components. However, interpretability has not been explicitly considered in GP until recently, following the surge in the popularity of XAI. This article provides a comprehensive review of the studies on GP that can potentially improve the model interpretability, both explicitly and implicitly, as a byproduct. We group the existing studies related to explainable artificial intelligence by GP into two categories. The first category considers the intrinsic interpretability, aiming to directly evolve more interpretable (and effective) models by GP. The second category focuses on post-hoc interpretability, which uses GP to explain other black-box machine learning models, or explain the models evolved by GP by simpler models such as linear models. This comprehensive survey demonstrates the strong potential of GP for improving the interpretability of machine learning models and balancing the complex tradeoff between model accuracy and interpretability.},
  keywords={Machine learning;Genetic programming;Task analysis;Predictive models;Adaptation models;Training;Measurement;Explainable artificial intelligence (XAI);Genetic programming (GP)},
  doi={10.1109/TEVC.2022.3225509},
  ISSN={1941-0026},
  month={June},}@INPROCEEDINGS{11011376,
  author={Chettier, Thiyagarajan Mani and Ashok Kumar Boyina, Venkata and Jorepalli, Sunil and Singh, Charanjit and Gupta, Neha},
  booktitle={2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT)}, 
  title={Scalable Explainable AI with a Cloud-Native Approach for Cybersecurity Threat Detection}, 
  year={2025},
  volume={},
  number={},
  pages={686-691},
  abstract={The growing complexity and number of cyber threats call for sophisticated detection approaches that provide both high performance and interpretability. The proposed hybrid AI approach (AE-RF-CNN-LSTM) is a promising hybrid up-to-date improving SEO framework for cybersecurity. All these models play a steady role in performance improvement. Autoencoders are trained to sense patterns from normal data to assist in anomaly detection, random forests help with the robustness of the model with overfitting reduction, and CNN-LSTMs assist greatly in recognizing the complex temporal dependencies in network traffic data. Experimental results show that this approach achieves substantially improved accuracy nearest neighbor anomaly detection and surpasses every single model in terms of accuracy. This hybrid framework, thus, helps to detect known and unknown cyber threats, leading to a huge decrease in false positive and false negative rates. Furthermore, integrating explainable AI (XAI) methods, including SHAP (SHapley Additive explanations) and LIME (Local Interpretable Model-Agnostic Explanations), to enhance decision interpretability. The methods enable security analysts to comprehend the most relevant features of making predictions and, therefore, trust the model and correct further cybersecurity actions. This not only allows for the improvement of defense mechanisms against cyberattacks but also builds confidence in AI-driven security solutions by providing interpretability and assurance of performance.},
  keywords={Accuracy;Explainable AI;Autoencoders;Telecommunication traffic;Threat assessment;Software;Robustness;Real-time systems;Software reliability;Anomaly detection;Scalable Explainable AI;Cloud-Native Security;Cyber Threat Intelligence;Anomaly Detection;Autoencoders;CNN-LSTM;Hybrid AI Models},
  doi={10.1109/InCACCT65424.2025.11011376},
  ISSN={},
  month={April},}@INPROCEEDINGS{11050799,
  author={Gyawali, Sohan and Jiang, Yili and Huang, Jiaqi},
  booktitle={2025 IEEE Security and Privacy Workshops (SPW)}, 
  title={In-Progress: Augmenting Explainable AI with LLMs to Enhance User Trust in Intelligent Transportation Systems}, 
  year={2025},
  volume={},
  number={},
  pages={358-360},
  abstract={Vehicular communication networks increasingly rely on automated systems for misbehavior detection; however, the opacity of machine learning models can diminish user trust. This paper introduces a novel framework that integrates Explainable Artificial Intelligence (XAI) with Large Language Models (LLMs) to deliver transparent, user-friendly explanations for misbehavior detection outcomes. Our approach utilizes XAI to quantify the contribution of individual features, providing clear, visual insights into model decision-making. Additionally, an LLM, augmented with contexts from SHAP and vector database, is employed to convert technical SHAP outputs into accessible natural language explanations. This ensures non-expert users, such as network operators or regulatory personnel, can understand and trust the system's predictions. Experimental evaluations on realistic vehicular network datasets demonstrate that our integrated approach sustains high detection performance while significantly enhancing explainability.},
  keywords={Visualization;Privacy;Explainable AI;Large language models;Natural languages;Decision making;Vectors;Security;Personnel;Intelligent transportation systems;misbehavior detection system;xai;llm;intelligent transportation system},
  doi={10.1109/SPW67851.2025.00051},
  ISSN={2770-8411},
  month={May},}@INPROCEEDINGS{10590490,
  author={Singh, Vishakha and Sharma, Ritesh and Kumar, Sushant and Singh, Sanjay Kumar},
  booktitle={2023 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Using Explainable AI and Genetic Algorithms to Drive the Discovery of Novel Antiviral Molecules}, 
  year={2023},
  volume={},
  number={},
  pages={348-352},
  abstract={The COVID-19 pandemic has prompted the scientific community to expedite therapeutic drug discovery. The partnership between biomedical scientists and artificial intelligence (AI) specialists has resulted in the creation of several computational tools for the preliminary assessment of antiviral drugs. One such category of drugs comprises antiviral peptides (AVPs) that are part of an organism's initial immune response to viral invasion and infection. The models developed to identify AVPs do not offer any insight into the characteristics that significantly contribute to their antiviral nature. So, a domain expert needs a thorough analysis to choose the most potent AVPs among the ones found by such models in antiviral proteins before going for synthesis. This work aims to accelerate this process by proposing a fully automated in silico framework that identifies AVPs in antiviral protein chains and optimizes them based on some desirable characteristics identified by analyzing the existing AVPs through an explainable machine learning model. This model has been built using extreme gradient boosting and has an accuracy of 90%, which is better than the existing classifiers. The two characteristics found by this explainable model conflict with each other's optimization; hence, non-dominated sorting genetic algorithms are used to find the pareto-optimal AVPs by establishing a trade-off between them. To evaluate the efficacy of the suggested framework, we found and optimized the AVPs present in some well-known antiviral proteins. The pareto-optimal AVPs were identified and proposed for synthesis and validation of antiviral activity. Lastly, a free online app has been deployed at https://avpdesign.anvil.app.},
  keywords={Proteins;Drugs;Analytical models;Computational modeling;Transformer cores;Transformers;Drug discovery;Explainable AI;Gradient boosting;Multi-objective optimization;NSGA-II;Pareto-optimality},
  doi={10.1109/CSCI62032.2023.00062},
  ISSN={2769-5654},
  month={Dec},}@INPROCEEDINGS{11050487,
  author={Choi, Sujung and Dubey, Rahul},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Clustering-Guided Counterfactual Design Evolution}, 
  year={2025},
  volume={},
  number={},
  pages={271-277},
  abstract={Counterfactuals provide valuable insights into how changes in input variables can affect outcomes. However, generating them in high-dimensional spaces presents significant challenges due to the data sparsity and increased complexity. Traditional methods often yield unrealistic or misleading explanations. This study tackles the specific challenge of generating reliable and actionable counterfactuals in vehicle chassis design problem by introducing clustering-guided counterfactual explanations (CCF). In the proposed approach, clustering helps to focus on more relevant regions of the search space, and a single-objective genetic algorithm (GA) searches for counterfactual designs. Experiments demonstrate that the proposed approach not only enhances the validity of counterfactuals but also improves the overall understanding of design decision spaces, in contrast to earlier methods that struggled with high-dimensional data. This research contributes to a broader framework for explainable AI and has the potential to address diverse problems. Ultimately, this work underscores the need for robust methodologies in counterfactual generation, which can advance knowledge across various scientific and engineering fields.},
  keywords={Knowledge engineering;Explainable AI;High dimensional data;Input variables;Reliability engineering;Complexity theory;Genetic algorithms;XAI;counterfactual explanations;genetic algorithm;engineering design},
  doi={10.1109/CAI64502.2025.00049},
  ISSN={},
  month={May},}@INPROCEEDINGS{11022503,
  author={Afroz, Tamanna and Shoumik, Tazwar Mohammed},
  booktitle={2024 27th International Conference on Computer and Information Technology (ICCIT)}, 
  title={Optimizing Energy Efficiency through Explainable AI: A Genetic Algorithm-Optimized Ensemble Method for Improved Decision-Making}, 
  year={2024},
  volume={},
  number={},
  pages={3468-3473},
  abstract={Accurate prediction of building energy consumption is crucial for optimizing energy usage and costs. This paper proposes an ensemble learning framework optimized with genetic algorithms and explainable AI to predict and interpret energy consumption. The UC Irvine Energy Efficiency dataset containing 768 samples with 8 features is leveraged. The ensemble model combines Random Forest, Decision Tree, and XGBoost regressors, tuned using genetic algorithms. Results show the ensemble model achieves the lowest RMSE of 0.434 and the highest R-squared of 0.998, outperforming individual models. Explainable AI via LIME is also implemented to locally interpret predictions and feature importance. The approach provides improved accuracy and model transparency for energy consumption forecasting. This demonstrates the efficacy of combining diverse machine learning models with optimization and explainability for multivariate time series regression tasks. The tunable ensemble framework with explainable predictions can enable smarter energy planning and decisions.},
  keywords={Energy consumption;Accuracy;Explainable AI;Computational modeling;Energy efficiency;Planning;Ensemble learning;Random forests;Tuning;Genetic algorithms;Energy Consumption;Genetic Algorithm;Random Forest;Decision Tree;XGBoost;LIME},
  doi={10.1109/ICCIT64611.2024.11022503},
  ISSN={2474-9656},
  month={Dec},}@INPROCEEDINGS{10611971,
  author={Mamatjan, Yasin},
  booktitle={2024 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Pan-Cancer Classification System with Explainable AI Interpretation: A Feasibility Study}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Cancer genomics identifies all genes playing critical roles in carcinogenesis. The state-of-the-art cancer genomics profiling characterized many clinically and biologically relevant patterns that are not resolvable by morphology nor distinguishable under the microscope for cancer diagnosis. With that genomic information, doctors can develop an individualized treatment plan for cancer patients and provide precision medicine. However, several technical challenges (such as low tumor purity, batch effects and formalin-fixed, paraffin-embedded (FFPE) tissue restoration) potentially led to ambiguous diagnoses that needed to be solved in the clinical setting. The purpose of this study is to develop a robust tumor classification framework to improve cancer diagnosis and provide Explainable Artificial Intelligence (XAI) based interpretable results with increased transparency model interpretability of the classification. We utilized a large set of over six thousand tumor samples (DNA methylation and gene expression) from The Cancer Genome Atlas (TCGA). We implemented realistic variable selection by separating the training and test datasets and removed artificial and technical sources of variabilities to overcome batch effect issues while identifying the biological variation and making the prediction meaningful and robust. The Random Forest classifier produced about 95 and 96% accuracy for mRNA and methylation-based models respectively with minimum features of 50 methylation probes and gene expression signatures. We further developed an XAI strategy and applied it to a large brain cancer patient group to make an explainable patient-specific decision while tailoring the provided recommendations based on each patient's characteristics. This strategy demonstrates more accurate and practical molecular subtype classification with explainable AI for model interpretation.},
  keywords={Training;Accuracy;Explainable AI;Genomics;Brain modeling;Gene expression;Bioinformatics;Pan-Cancer Classification;Cancer Genomics;Molecular Diagnostics;Gene Expression;Methylome;Machine Learning;Random Forest;Model Interpretation;Explainable AI},
  doi={10.1109/CEC60901.2024.10611971},
  ISSN={},
  month={June},}@INPROCEEDINGS{10263974,
  author={Smmarwar, Santosh K. and Gupta, Govind P. and Kumar, Sanjay},
  booktitle={2023 IEEE World Conference on Applied Intelligence and Computing (AIC)}, 
  title={XAI-AMD-DL: An Explainable AI Approach for Android Malware Detection System Using Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={423-428},
  abstract={Efficient malware identification is essential to safe the system resources and privacy of data for cybersecurity system. The use of android smartphones has increased tremendously that attracting various types of malware attacks. Nowadays, malware writers use Artificial Intelligence (AI)-enabled malware attack techniques to bypass the detection of malicious activities. Hence, designing an efficient, effective and robust malware detection system to identify malware variants remains a critical problem and challenge. However, number of deep learning (DL) models applied for effective android malware detection in existing methods at large scale, but these models actually lacks interpretability of the models to explain the contribution of each features to the detection system. Therefore, this paper propose an Explainable Artificial Intelligence (XAI) based hybrid Convolutional Neural network (CNN) and Bi-Gated Recurrent Unit (Bi-GRU) Android Malware Detection (AMD) System using DL models named as XAI-AMD-DL. The proposed model is evaluated the using the CICAndMal2019 android malware dataset. The results obtained by the proposed XAI-AMD-DL model is 97.98% accuracy, and 97.75 %, 97.76%, 97.75% precision, recall and f1score, respectively outperforms the existing DL models.},
  keywords={Deep learning;Data privacy;Metaheuristics;Feature extraction;Malware;Convolutional neural networks;Artificial intelligence;Explainable AI;Android Malware Detection;Deep Learning;Bi-GRU;Security and Privacy;CNN},
  doi={10.1109/AIC57670.2023.10263974},
  ISSN={},
  month={July},}@INPROCEEDINGS{10725514,
  author={Ali, Noman and Asif, Mohammad and Kaushal, Anshul and Singh, Uphaar and Tiwary, Uma Shanker},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Optimizing Emotion Recognition in EEG Data: A Genetic Algorithm Approach with XAI Insights}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Emotion recognition using electroencephalogram data presents a burgeoning area of research, yet navigates through intricate optimization hurdles, alongside the persistent challenge of rendering results interpretable. In this study, we employed a combination of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks and utilized a Genetic Algorithm (GA) for optimization and enhancing the model’s performance and robustness in deciphering emotional states from EEG signals. Our methodology encompasses data pre-processing techniques, including Short-Time Fourier Transform (STFT) analysis, applied to EEG data for feature extraction and GA-driven hyperparameter optimization to identify an optimal neural network architecture. This architecture, consisting of Convolutional and Recurrent layers with dropout regularization, is adept at extracting temporal and spatial features from EEG signals while mitigating overfitting. Furthermore, we investigate explainable AI (XAI) strategies to get insight into the decision-making process of our GA-based optimized model. Additionally, rigorous cross-validation ensures the generalization performance of the optimized model across diverse datasets. Empirical results demonstrate the effectiveness of our approach, with the optimized CNN-LSTM hybrid model achieving an accuracy of $\mathbf{9 3. 2 8 \%}$ in classifying 24 different emotions. This study enhances our understanding of emotion recognition systems by examining the intricate interplay between EEG data analysis, CNN-LSTM networks, and Genetic Algorithm optimization. Additionally, it provides practical insights into the optimization of such systems, potentially influencing future advancements in affective computing technologies.},
  keywords={Emotion recognition;Explainable AI;Computer architecture;Brain modeling;Feature extraction;Electroencephalography;Convolutional neural networks;Optimization;Long short term memory;Genetic algorithms;CNN-LSTM;Genetic Algorithm (GA);Explainable AI;EEG;Emotions Recognition;Optimization},
  doi={10.1109/ICCCNT61001.2024.10725514},
  ISSN={2473-7674},
  month={June},}@INPROCEEDINGS{10168594,
  author={Bhat, Ashwin and Raychowdhury, Arijit},
  booktitle={2023 IEEE 5th International Conference on Artificial Intelligence Circuits and Systems (AICAS)}, 
  title={Reducing Overhead of Feature Importance Visualization via Static GradCAM Computation}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Explainable AI (XAI) methods provide insights into the operation of black-box Deep Neural Network (DNN) models. GradCAM, an XAI algorithm, provides an explanation by highlighting regions in the input feature space that were relevant to the model’s output. It involves a gradient computation step that adds a significant overhead compared to inference and hinders providing explanations to end-users. In this work, we identify the root cause of the problem to be the dynamic run-time automatic differentiation. To overcome this issue, we propose to offload the gradient computation step to compile time via analytic evaluation. We validate the idea by designing an FPGA implementation of GradCAM that schedules the entire computation graph statically. For a TinyML ResNet18 model, we achieve a reduction in the explanation generation overhead from > 2× using software frameworks on CPU/GPU systems to < 0.01× on the FPGA using our designed hardware and static scheduling.},
  keywords={Visualization;Schedules;Runtime;Processor scheduling;Computational modeling;Software algorithms;Inference algorithms;Convolutional Neural Network (CNN);Explainable AI (XAI);Feature Attribution;GradCAM;FPGA;High Level Synthesis (HLS);Domain Specific Hardware Accelerator},
  doi={10.1109/AICAS57966.2023.10168594},
  ISSN={2834-9857},
  month={June},}@INPROCEEDINGS{11038052,
  author={Lin, Yan-Cheng and Chen, Tang-Yuan and Wang, Chen-Chao and Hung, Chih-Pin and Shou, I-An and Wang, Hung-Kai},
  booktitle={2025 IEEE 75th Electronic Components and Technology Conference (ECTC)}, 
  title={GA-NN and Explainable AI for IC Packaging Warpage Optimization: A Case Study on Product Feature}, 
  year={2025},
  volume={},
  number={},
  pages={1609-1614},
  abstract={In semiconductor packaging, avoiding product warpage is difficult since it involves product specifications and related processes. In order to effectively predict warpage, traditional methods use a large amount of data to perform statistical analysis. However, in the actual field, obtaining detailed parameter data for each process requires a lot of time and manpower, which makes statistical method impractical. In other words, achieving accurate warpage prediction with limited data is crucial. Therefore, this research proposes an artificial neural network model with genetic algorithms to enhance warpage prediction. The proposed algorithm uses genetic algorithms to achieve optimize the hyperparameter of artificial neural network (ANN), which is called Genetic Algorithm Optimized Artificial Neural Network (GA-NN), the algorithm can accelerate the convergence speed and performance of the ANN. The data set was generated by the finite element method from the semiconductor packaging product feature data. Compared with other algorithms, the product data simulated by the proposed method can maintain numerical stability and achieve rapid convergence. In the proposed ANN, an interpretable AI algorithm - SHapley Additive exPlanations (SHAP) is also used to further analyze various input parameters and quantify the operation results of the machine learning model. The empirical result shows that the prediction has an error of only 3 % on Molding Functional Chip Ball Grid Array (MFCBGA) package products. Compared with traditional algorithms and other machine learning methods, the proposed algorithm performs faster and with higher prediction accuracy, while providing product feature influence and visualizes the direct relationship between product specifications and warpage. To sum up, this research achieves effective package warpage prediction, while further explains the relationship between warpage generation in the AI model, input data and prediction targets to customers. Finally, the product design cycle can be reduced and the product quality can be improved simultaneously.},
  keywords={Machine learning algorithms;Statistical analysis;Artificial neural networks;Predictive models;Packaging;Prediction algorithms;Product design;Finite element analysis;Genetic algorithms;Convergence;MFCBGA;FEM simulation;machine learning;SHapley Additive exPlanations (SHAP);Explainable},
  doi={10.1109/ECTC51687.2025.00274},
  ISSN={2377-5726},
  month={May},}@ARTICLE{9781414,
  author={Dastile, Xolani and Celik, Turgay and Vandierendonck, Hans},
  journal={IEEE Access}, 
  title={Model-Agnostic Counterfactual Explanations in Credit Scoring}, 
  year={2022},
  volume={10},
  number={},
  pages={69543-69554},
  abstract={The past decade has shown a surge in the use and application of machine learning and deep learning models across various domains. One such domain is credit scoring, where applicants are scored to assess their creditworthiness for loan applications. It is essential to ensure that no biases or discriminations are incurred during the scoring process. Most machine learning and deep learning models are prone to unintended bias and discrimination in the datasets. Therefore, it is imperative to explain each prediction from the models during the scoring process to avoid the element of model bias and discrimination. Our study proposes a novel optimization formulation that generates sparse counterfactual explanations via a custom genetic algorithm to explain the black-box model’s predictions. We evaluated the efficacy of the proposed method on publicly available credit scoring datasets by comparing the counterfactual explanations generated by the proposed method with explanations from credit scoring experts. The proposed counterfactual explanation method does not only explain rejected loan applications but also can be used to explain approved loan applications.},
  keywords={Predictive models;Deep learning;Genetic algorithms;Safety;Random forests;Prototypes;Indexes;Credit scoring;machine learning;counterfactual explanation;explainable AI;genetic algorithm},
  doi={10.1109/ACCESS.2022.3177783},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9897629,
  author={Yang, Guang and Rao, Arvind and Fernandez-Maloigne, Christine and Calhoun, Vince and Menegaz, Gloria},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)}, 
  title={Explainable AI (XAI) In Biomedical Signal and Image Processing: Promises and Challenges}, 
  year={2022},
  volume={},
  number={},
  pages={1531-1535},
  abstract={Artificial intelligence has become pervasive across disciplines and fields, and biomedical image and signal processing is no exception. The growing and widespread interest on the topic has triggered a vast research activity that is reflected in an exponential research effort. Through study of massive and diverse biomedical data, machine and deep learning models have revolutionized various tasks such as modeling, segmentation, registration, classification and synthesis, outperforming traditional techniques. However, the difficulty in translating the results into biologically/clinically interpretable information is preventing their full exploitation in the field. Explainable AI (XAI) attempts to fill this translational gap by providing means to make the models interpretable and providing explanations. Different solutions have been proposed so far and are gaining increasing interest from the community. This paper aims at providing an overview on XAI in biomedical data processing and points to an upcoming Special Issue on Deep Learning in Biomedical Image and Signal Processing of the IEEE Signal Processing Magazine that is going to appear in March 2022.},
  keywords={Deep learning;Image segmentation;Special issues and sections;Biological system modeling;Signal processing;Data models;Artificial intelligence;Explainable AI;XAI;Artificial Intelligence;Machine Learning;Deep Learning;Biomedical Data},
  doi={10.1109/ICIP46576.2022.9897629},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10371846,
  author={Sen, Anuvab and Rhik Mazumder, Arul and Sen, Udayon},
  booktitle={2023 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting}, 
  year={2023},
  volume={},
  number={},
  pages={234-239},
  abstract={Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in load forecasting. Transformer models have the potential to improve load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer-based Neural Network model integrated with different metaheuristic algorithms by their performance in load forecasting based on numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced Transformer-based Neural Network models in load forecasting accuracy and provide optimal hyperparameters for each model.},
  keywords={Measurement;Load forecasting;Computational modeling;Neural networks;Power system dynamics;Metaheuristics;Predictive models;Deep Learning;Differential Evolution;Particle Swarm Optimization;Genetic Algorithm;Meta-heuristics},
  doi={10.1109/SSCI52147.2023.10371846},
  ISSN={2472-8322},
  month={Dec},}@INPROCEEDINGS{9945207,
  author={Wu, Ching-Hsuan and Shen, Jyun-Yi and Huang, Pei-Shin and Hua, Cheng-Yen and Jiang, Yu-Chi and Kuo, Shu-Yu and Chou, Yao-Hsin},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={A Novel Explainable Nature-inspired Metaheuristic: Jaguar Algorithm with Precision Hunting Behavior}, 
  year={2022},
  volume={},
  number={},
  pages={1494-1499},
  abstract={Metaheuristics are crucial for solving complex optimization problems effectively in many fields. With the recent increased interest in explainable artificial intelligence (XAI), the issue of how metaheuristics interact with XAI has attracted much attention. Most metaheuristics have stochastic search processes, and the jaguar algorithm (JA) is a unique algorithm that has exact search paths. JA shows potential abilities both in exploration and exploitation, but still faces limitations. Therefore, this study proposes a new metaheuristic based on JA and invents precision hunting behavior (PH-JA), which inherits the concept of JA and significantly strengthens its search efficiency. PH-JA improves the solution quality by adaptively detecting the current environment’s trends during movement and precisely hunting prey. Then, PH-JA makes the best of the historical information to reduce the computational cost. PH-JA is an explainable metaheuristic and has systematic operational procedures in that each movement is performed according to the present situation rather than random elements. Therefore, the path to a solution is interpretable and can return the same result with the same input. The experimental results demonstrated the superiority of PH-JA, which is better than other classical and state-of-the-art metaheuristics in terms of solution quality and evaluation costs.},
  keywords={Costs;Systematics;Metaheuristics;Market research;Computational efficiency;Behavioral sciences;Reliability;Metaheuristics;Evolutionary Computation;Jaguar Algorithm;Explainable AI},
  doi={10.1109/SMC53654.2022.9945207},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{10798106,
  author={Kala, Ahmet and Torkul, Orhan and Yildiz, Tugba Tunacan and Selvi, Ihsan Hakan},
  journal={IEEE Access}, 
  title={Early Prediction of Student Performance in Face-to-Face Education Environments: A Hybrid Deep Learning Approach With XAI Techniques}, 
  year={2024},
  volume={12},
  number={},
  pages={191635-191649},
  abstract={A community is only as strong as its weakest link; this principle also applies to student communities in the educational field. The quality of learning achieved by students in a course is directly related to the performance of the weakest student in that course. Therefore, a high number of students failing a course and the necessity of repeating it are undesirable in terms of learning quality. This study aims to predict students’ performance early during their coursework to identify those at risk of failing, thus improving the quality of the course and determining the necessary resources to achieve this goal. To this end, we proposed a conceptual model based on a hybrid method combining Particle Swarm Optimization (PSO) and Deep Neural Networks (DNN). To evaluate the classification performance of the model, comparisons were made with classical machine learning and deep learning models. SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) methods were used to determine the contribution of different features to the model’s predictions. Additionally, to assess the generalizability and applicability of the model, the widely used xAPI-Edu-Data dataset, which covers various courses, was employed, and the accuracy results of the model were compared with early prediction studies published in the literature. As a result, it was found that our prediction model performed 6% better than the classical models and achieved better results than most of the models, except for two models in the literature with similar results. Moreover, important performance features that can be used to evaluate students earlier in the course were identified.},
  keywords={Predictive models;Education;Deep learning;Explainable AI;Data models;Long short term memory;Feature extraction;Electronic learning;Data mining;Context modeling;Early prediction of student performance;deep learning (DL);explainable artificial intelligence;local interpretable model-agnostic explanations (LIME);and shapley additive explanations (SHAP)},
  doi={10.1109/ACCESS.2024.3516816},
  ISSN={2169-3536},
  month={},}@ARTICLE{10836702,
  author={Afnan Birahim, Shaikh and Paul, Avijit and Rahman, Fahmida and Islam, Yamina and Roy, Tonmoy and Asif Hasan, Mohammad and Haque, Fariha and Chowdhury, Muhammad E. H.},
  journal={IEEE Access}, 
  title={Intrusion Detection for Wireless Sensor Network Using Particle Swarm Optimization Based Explainable Ensemble Machine Learning Approach}, 
  year={2025},
  volume={13},
  number={},
  pages={13711-13730},
  abstract={Wireless Sensor Networks (WSN) play a pivotal role in various domains, including monitoring, security, and data transmission. However, their susceptibility to intrusions poses a significant challenge. This paper proposes a novel Intrusion Detection System (IDS) leveraging Particle Swarm Optimization (PSO) and an ensemble machine learning approach combining Random Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN) models to enhance the accuracy and reliability of intrusion detection in WSNs. The system addresses key challenges such as the imbalanced nature of datasets and the evolving complexity of network attacks. By incorporating Synthetic Minority Oversampling Technique Tomek (SMOTE-Tomek) techniques to balance the dataset and employing explainable AI methods such as Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), the proposed model achieves significant improvements in detection accuracy, precision, recall, and F1 score while providing clear, interpretable results. Extensive experimentation on WSN-DS dataset demonstrates the system’s efficacy, achieving an accuracy of 99.73%, with precision, recall, and F1 score values of 99.72% each, outperforming existing approaches. This work offers a robust, scalable solution for securing WSNs, contributing to both academic research and practical applications.},
  keywords={Wireless sensor networks;Accuracy;Intrusion detection;Computational modeling;Adaptation models;Random forests;Nearest neighbor methods;Monitoring;Feature extraction;Radio frequency;Intrusion detection system;wireless sensor networks;particle swarm optimization;ensemble machine learning;explainable AI;streamlit web application},
  doi={10.1109/ACCESS.2025.3528341},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10719032,
  author={Khanna, Sakshi Taaresh and Khatri, Sunil Kumar and Sharma, Neeraj Kumar},
  booktitle={2024 IEEE Third International Conference on Power Electronics, Intelligent Control and Energy Systems (ICPEICES)}, 
  title={GACNNXAI: Employing Genetic Algorithm-Enhanced Convolutional Neural Networks and Explainable Artificial Intelligence and its Applications}, 
  year={2024},
  volume={},
  number={},
  pages={353-358},
  abstract={We have proposed a Genetic Algorithm Enhanced Convolutional Neural Network coupled with Explainable Artificial Intelligence for oral cancer detection in medical imaging. Our approach associated CNN with an explanation method to make CNN-based diagnostic decisions more understandable for a clinician. A high resolution of oral imaging is a challenge due to downsampling in several CNN networks. By optimizing the hyperparameters of CNN using Genetic Algorithms, we compensated for downsampling. Our model's hyperparameters, such as the number of layers, filter size, and learning rate, were adjusted based on performance in the validation set. Advancing further in the interpretability, we have used Layer-wise Relevance Propagation, a Local Interpretable Model-agnostic Explanation technique to indicate CNN's decision-making rationale by selecting influential regions of input images. Our proposed GA-CNN model applied to a comprehensive oral image dataset resulted in a classification accuracy of 93.7%. Our sensitivity and specificity rates were 92.5% and 94.9% improvement over other CNN models available in the literature. We have demonstrated that our GA-based optimization mechanism performs better for parameter searching in the CNN. Our proposed AI tool with an explanation for oral cancer screening will assist clinicians in the early identification of disease, establishing a complementary relationship between human health service providers and machine learning technologies for improved patient outcomes.},
  keywords={Accuracy;Data analysis;Data handling;Machine learning;Sensitivity and specificity;Power electronics;Convolutional neural networks;Optimization;Genetic algorithms;Cancer;Classification Accuracy;Convolutional Neural Networks (CNN);Explainable Artificial Intelligence (XAI);Genetic Algorithms (GA);Layer-wise Relevance Propagation (LRP);Medical Imaging;Oral Cancer Detection;Optimization},
  doi={10.1109/ICPEICES62430.2024.10719032},
  ISSN={},
  month={April},}@INPROCEEDINGS{10579033,
  author={Chan, S.},
  booktitle={2024 IEEE World AI IoT Congress (AIIoT)}, 
  title={Inference-Optimized Metaheuristic Approach for a Prospective AI Training/Inference Inversion Paradigm in Optimized Energy-Aware Computing}, 
  year={2024},
  volume={},
  number={},
  pages={370-379},
  abstract={Energy consumption at the various steps of the Machine Learning (ML) model life cycle, as a constituent application of Artificial Intelligence (AI), is infrequently reported. While the Explainable AI (XAI) or Explainable ML (XML) movement has focused upon more explainable ML models, the AI Energy Consumption (AEC) facet has not progressed as rapidly and still remains quite translucent. For example, even the AEC ratios of the key AI stages (e.g., pre-training, fine-tuning, and inferencing) have not always accompanied the releases of new ML models. This lack of data has, perhaps, contributed to the dearth of analyses on effective compute (e.g., algorithmic efficiency versus hardware efficiency) for the newer models, and in modern times, AEC may be skewing to the inferencing side. This may necessitate revised architectures, particularly amidst the findings that generalized ML models for specific tasks have a much higher AEC when contrasted against task-specific ML models. It has also been reported that, within those same models, a higher number of parameters segues to a higher AEC. Higher accuracies also beget higher AECs, and “advanced anomaly detection” necessitates tasks that have an even higher AEC. Moreover, as it is now customary to run numerous instances of a pre-trained model over various instances in an ensemble fashion, the AEC is multiplied accordingly. Yet, there are opportunities to reduce AEC at the Metaheuristic Algorithm (MA) level (e.g., at the convolutional layer), and certain versatile constructs (that scale well across the AI stages) are amenable to such optimizations; furthermore, performance metric comparisons in the literature have, traditionally, been artificially constrained to a “fixed number of allowed function calls,” and this might have led to misinterpretations of MA performance in Real World Scenario (RWS) paradigms. These misinterpretations can skew research directions, particularly for RWS Multiple Objective Large Scale Nonlinear Programming Problems (MOLSNLP) and may also lead to an underestimation of the involved AEC. This paper presents a promising RWS-oriented Particle Swarm Optimization (PSO)-based MA with concomitant Rough Order of Magnitude (ROM) AECs.},
  keywords={Energy consumption;Accuracy;Explainable AI;Computational modeling;Metaheuristics;XML;Programming;computational intelligence;evolutionary computation;nature-inspired metaheuristic;inference energy consumption;multiple objective large scale nonlinear programming},
  doi={10.1109/AIIoT61789.2024.10579033},
  ISSN={},
  month={May},}@INPROCEEDINGS{10264894,
  author={Ruz, Gonzalo A.},
  booktitle={2023 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)}, 
  title={Inference of threshold network models of the tryptophan operon in Escherichia coli}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={A recent Boolean model of the tryptophan operon in Escherichia coli has been introduced, which exhibits a desired set of fixed points modeling the operon state being on or off. Nevertheless, when updated synchronously, the model also reveals spurious limit cycles with a larger basin of attraction than the desired fixed points. This paper presents the search via evolutionary computation for threshold network models that exhibit the desired fixed points without any limit cycles. The proposed framework was applied using differential evolution and particle swarm optimization, the latter being the most efficient and effective method based on the results obtained by the simulations. Particle swarm optimization found two correct threshold networks, one of which has only weight values {-1,0,1}, making the model more interpretable.},
  keywords={Microorganisms;Evolution (biology);Computational modeling;Limit-cycles;Computational efficiency;Behavioral sciences;Particle swarm optimization;Boolean network;Threshold network;Evolutionary Computation;Tryptophan operon;Differential evolution;Particle swarm optimization},
  doi={10.1109/CIBCB56990.2023.10264894},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10981077,
  author={Elayavalli, Satvik R. and Yadav, Ipsa and Cookson, Grant T. and Sharifi, Elias and Tvrdic, Tanya and Madabhushi, Anant and Weinberg, Brent D. and Abedalthagafi, Malak and Khalighi, Sirvan},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title={Non-Invasive Molecular Classification of Gliomas Using Explainable ai and Radiomic Features from Multi-Sequence MRI}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Accurate molecular classification of brain gliomas is crucial for effective diagnosis and treatment, but existing methods mainly rely on invasive biopsies. To address this issue, we developed an explainable AI-based diagnostic system that combines radiomic features from multi-sequence MRI and clinical data to predict key molecular subtypes based on WHO 2021 criteria. Analyzing MRI and molecular data from 117 glioma patients, we extracted intensity, shape, and texture features. Our model was trained to classify various molecular subtypes separately, achieving high classification accuracy with AUC scores exceeding 0.8 for IDH mutations, CDKN2A/B deletions, and chromosomal alterations, as determined by 5-fold cross-validation. Features from T1 precontrast and FLAIR imaging of enhancing tumors and edematous regions were particularly predictive. This noninvasive method shows promise for glioma molecular classification, enhancing personalized treatment and clinical decision-making.},
  keywords={Accuracy;Explainable AI;Shape;Magnetic resonance imaging;Decision making;Biopsy;Feature extraction;Data mining;Radiomics;Tumors;Glioma classification;Radiomics;explainable AI},
  doi={10.1109/ISBI60581.2025.10981077},
  ISSN={1945-8452},
  month={April},}@INPROCEEDINGS{11043113,
  author={Wang, Shaolin and Che, Haoyang and Jiang, He and Mei, Yi and Liu, Yi and Gu, Ying},
  booktitle={2025 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Diverse Counterfactual Explanations by Differential Evolution with Ablation Strategies for Uncertain Capacitated Arc Routing Problem}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={The Uncertain Capacitated Arc Routing Problem (UCARP) presents unique challenges in real-world applications such as waste collection and winter gritting, where task demands and service costs are stochastic. Although Genetic Programming Hyper-Heuristics (GPHH) have demonstrated strong adaptability to such uncertainties by evolving dynamic routing policies, their complex decision-making processes hinder interpretability. To address this, a novel framework called Differential Evolution with Random Ablation (DERA) is introduced to generate diverse and feasible counterfactual explanations. Unlike traditional methods, DERA systematically explores multiple counterfactual scenarios by integrating random ablation into the optimisation process, thereby uncovering a broader range of plausible alternatives. Experimental results across various UCARP instances show that DERA consistently achieves high feasibility, minimal feature changes, and greater diversity in counterfactual explanations compared to baseline methods. This diversity enables a more comprehensive understanding of GPHH-evolved policies, providing actionable insights to improve decision-making transparency and robustness in dynamic environments.},
  keywords={Uncertainty;Diversity reception;Decision making;Genetic programming;Evolutionary computation;Diversity methods;Routing;Robustness;Dynamic programming;Optimization},
  doi={10.1109/CEC65147.2025.11043113},
  ISSN={},
  month={June},}@INPROCEEDINGS{10849416,
  author={Dubey, Rahul},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Explainable AI Assisted Evolutionary Search of Engineering Designs}, 
  year={2024},
  volume={},
  number={},
  pages={447-453},
  abstract={Evolutionary algorithms are widely utilized to dis-cover optimal and feasible solutions for NP-hard problems. However, the factors influencing a solution's performance, such as objective values, feasibility, or infeasibility, often lack ex-planation. This paper presents a novel Explainable AI (XAI)-assisted evolutionary search approach that utilizes evolutionary data to learn problem characteristics and guide the search process, thereby enhancing the quality of evolved solutions. The proposed method employs local and global explainable techniques efficiently and precisely to optimize parameters. The paper also introduces a framework that evolves engineering designs using the proposed XAI-assisted evolutionary approach. A practical application involving the optimization of a 2D car chassis demonstrates the effectiveness of the proposed algorithm. Experimental results show that the proposed XAI -assisted evolutionary algorithm improves the quality of solutions by between 8.15% to 39.95% over four iterations compared to traditional methods.},
  keywords={Explainable AI;NP-hard problem;Evolutionary computation;Search problems;Automobiles;Optimization;XAI;GA;machine learning;design},
  doi={10.1109/ICTAI62512.2024.00070},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10894135,
  author={Karnati, Shainy Reddy and Enukonda, Kavya Reddy and Elgonda, Vaishnavi Reddy and Hariharan, Shanmugasundaram and Kukreja, Vinay and Krishnamoorthy, Murugaperumal},
  booktitle={2024 International Conference on System, Computation, Automation and Networking (ICSCAN)}, 
  title={Thyroid Detection and Interpretation using XAI (eXplanable Artificial Intelligence)}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={We have designed this machine learning model in order to help the victims of thyroid suffering from its chronic effects. In our machine learning model we will be using various classification algorithms like KNN(K-nearest neighbours) classifiers, random forest classifier, Naive bayes classifier, random forest classifier, SVM(support vector machines) and decision trees. Among all the above classification algorithms random forest have produced more accurate results. We will be taking the data of different patients from the laboratories and convert into. csv file. We will train the model using the above algorithms and the performance of the model is calculated on the basis of Accuracy, Precision, Recall and F1-Score. In our paper we will be using explainable AI (XAI) to explain the results and output produced by our model. There are several methods and processes in the field of XAI, here we are using an open source python library called SHAP(shapely additive explanations). The main advantage of SHAP is that it will help the user to understand which feature is most influential in the model.},
  keywords={Accuracy;Explainable AI;Computational modeling;Predictive models;Prediction algorithms;Vectors;Classification algorithms;Naive Bayes methods;Random forests;Thyroid;Machine Learning algorithms;Explainable AI;Logistic Regression;Correlation Matrix;Accuracy;Precision;Recall;F1-score;XRF(Explainable Random Forest) model;SHAP(SHapely Addittive exPlainations) library;Correlation Matrix},
  doi={10.1109/ICSCAN62807.2024.10894135},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10697961,
  author={DeSimone, Hanna and Leon-Espinosa, Maikel},
  booktitle={2024 IEEE Opportunity Research Scholars Symposium (ORSS)}, 
  title={Leveraging Explainable AI in Business and Further}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This review thoroughly examines explainable artificial intelligence (XAI) and its importance in creating responsible AI. It covers AI’s background, guiding principles, the distinction between interpretability and explainability, and various explanations. We emphasize the importance of transparency in explainable systems and its application in various high-stakes fields. We will also discuss the state of regulations and give a forecast for XAI research and regulation shortly.},
  keywords={Explainable AI;Reviews;Regulation;Business},
  doi={10.1109/ORSS62274.2024.10697961},
  ISSN={},
  month={April},}@ARTICLE{10752628,
  author={Stein, Niki van and Bäck, Thomas},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics}, 
  year={2025},
  volume={29},
  number={2},
  pages={331-345},
  abstract={Large language models (LLMs), such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets. This article introduces a novel LLM evolutionary algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms. Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates, and selects algorithms based on performance metrics and feedback from runtime evaluations. This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise. We show how this framework can be used to generate novel closed box metaheuristic optimization algorithms for box-constrained, continuous optimization problems automatically. LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (covariance matrix adaptation evolution strategy and differential evolution) on the 5-D closed box optimization benchmark (BBOB). The algorithms also show competitive performance on the 10- and 20-D instances of the test functions, although they have not seen such instances during the automated generation process. The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs.},
  keywords={Benchmark testing;Evolutionary computation;Metaheuristics;Codes;Large language models;Closed box;Heuristic algorithms;Mathematical models;Vectors;Systematics;Automated code generation;evolutionary computation (EC);large language models (LLMs);metaheuristics;optimization},
  doi={10.1109/TEVC.2024.3497793},
  ISSN={1941-0026},
  month={April},}@INPROCEEDINGS{10402213,
  author={Sen, Anuvab and Sen, Udayon and Roy, Subhabrata},
  booktitle={2023 IEEE 15th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={A Comparative Analysis on Metaheuristic Algorithms Based Vision Transformer Model for Early Detection of Alzheimer's Disease}, 
  year={2023},
  volume={},
  number={},
  pages={200-205},
  abstract={A number of life threatening neuro-degenerative disorders had degraded the quality of life for the older generation in particular. Dementia is one such symptom which may lead to a severe condition called Alzheimer's disease if not detected at an early stage. It has been reported that the progression of such disease from a normal stage is due to the change in several parameters inside the human brain. In this paper, an innovative metaheuristic algorithms based ViT model has been proposed for the identification of dementia at different stage. A sizeable number of test data have been utilized for the validation of the proposed scheme. It has also been demonstrated that our model exhibits superior performance in terms of accuracy, precision, recall as well as F1-score.},
  keywords={Neurological diseases;Analytical models;Computational modeling;Metaheuristics;Transformers;Brain modeling;Alzheimer's disease;alzheimer's disease;ant colony optimization;differential evolution;genetic algorithm;mild cognitive impairment;multi-layer perception;particle swarm optimization;vision transformer},
  doi={10.1109/CICN59264.2023.10402213},
  ISSN={2472-7555},
  month={Dec},}@INPROCEEDINGS{9939601,
  author={Bhat, Ashwin and Assoa, Adou Sangbone and Raychowdhury, Arijit},
  booktitle={2022 IFIP/IEEE 30th International Conference on Very Large Scale Integration (VLSI-SoC)}, 
  title={Gradient Backpropagation based Feature Attribution to Enable Explainable-AI on the Edge}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={There has been a recent surge in the field of Explainable AI (XAI) which tackles the problem of providing insights into the behavior of black-box machine learning models. Within this field, feature attribution encompasses methods which assign relevance scores to input features and visualize them as a heatmap. Designing flexible accelerators for multiple such algorithms is challenging since the hardware mapping of these algorithms has not been studied yet. In this work, we first analyze the dataflow of gradient backpropagation based feature attribution algorithms to determine the resource overhead required over inference. The gradient computation is optimized to minimize the memory overhead. Second, we develop a High-Level Synthesis (HLS) based configurable FPGA design that is targeted for edge devices and supports three feature attribution algorithms. Tile based computation is employed to maximally use on-chip resources while adhering to the resource constraints. Representative CNNs are trained on CIFAR-10 dataset and implemented on multiple Xilinx FPGAs using 16-bit fixed-point precision demonstrating flexibility of our library. Finally, through efficient reuse of allocated hardware resources, our design methodology demonstrates a pathway to repurpose inference accelerators to support feature attribution with minimal overhead, thereby enabling real-time XAI on the edge.},
  keywords={Backpropagation;Training;Visualization;Machine learning algorithms;Very large scale integration;Inference algorithms;Libraries;Convolution Neural Network;Explainable Machine Learning;Back-propagation;Hardware Accelerator;FPGA;High-Level Synthesis (HLS)},
  doi={10.1109/VLSI-SoC54400.2022.9939601},
  ISSN={2324-8440},
  month={Oct},}@INPROCEEDINGS{11064483,
  author={Singh, Akhilesh Kumar and Tripathi, Akhilesh Kumar},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={Improving Customer Churn Prediction: A Study of Counterfactual Explanations Using Wachter's Method, Growing Spheres Method, and Genetic Algorithms}, 
  year={2025},
  volume={3},
  number={},
  pages={273-278},
  abstract={Customer churn prediction is critical for businesses to retain customers and maintain revenue. In this paper, we compare three counterfactual explanation methods: Watcher's Method, Sphere Method, and Genetic Algorithm, which are applied to customer churn prediction models. We evaluate the effectiveness of these methods in generating counterfactual instances, aiming to understand the minimal changes required to alter a customer's churn prediction. The key metrics, including Fischer Score, Imbalance Ratio, Volume, and Number of Features, are analysed to assess the performance and interpretability of the models. Our findings highlight the strengths and weaknesses of each method, providing insights into their practical applications for improving customer retention strategies.},
  keywords={Measurement;Analytical models;Solid modeling;Learning (artificial intelligence);Predictive models;Security;Churn;Genetic algorithms;Business;Churn prediction;Model interpretation;Class imbalance;Wachter's method;Growing Spheres;Genetic Algorithm;Instance position;Counterfactual Learning},
  doi={10.1109/ICCSAI64074.2025.11064483},
  ISSN={},
  month={April},}@INPROCEEDINGS{10055553,
  author={Bin Amir, Sk. Azmaeen and Hossain Khan, Abid},
  booktitle={2022 25th International Conference on Computer and Information Technology (ICCIT)}, 
  title={Calibration of a simplified thermodynamic model for VVER-1200-based nuclear power plants using evolutionary algorithms}, 
  year={2022},
  volume={},
  number={},
  pages={19-24},
  abstract={A thermal power plant's efficiency and output power are very sensitive to its surrounding weather conditions. Since a nuclear power plant (NPP) usually runs at lower thermodynamic efficiency compared to other thermal power plants, an additional decrease in output power may challenge the economic viability of the project. Thus, it is very important to establish a sufficiently accurate model than can depict the correlation between NPP output power and condenser pressure. This work attempts to calibrate a simplified thermodynamic model using two evolutionary algorithms, Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). For GA, the initial population is varied in the range of 10-1000, while the mutation and crossover rates are taken as 0.01 and 0.50, respectively. For PSO, the swarm size is varied within the range of 100-1000. Results reveal that the calibrated model has more accurate predictions compared to the original model. The model calibrated with GA is found to be slightly better performing than the one calibrated with PSO. Additionally, the calibration process is observed to be insensitive to the reference condenser pressure. Finally, it is estimated that the efficiency of the plant can go down to 33.56% at 15kPa condenser pressure compared to 37.30% at 4kPa.},
  keywords={Heating systems;Thermodynamics;Computational modeling;Sociology;Predictive models;Calibration;Particle swarm optimization;Model calibration;Evolutionary algorithm;Nuclear power;VVER-1200},
  doi={10.1109/ICCIT57492.2022.10055553},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10704309,
  author={Zhong, Rui and Zhang, Shilong and Yu, Jun and Munetomo, Masaharu},
  booktitle={2024 6th International Conference on Data-driven Optimization of Complex Systems (DOCS)}, 
  title={GeminiDE: A Novel Parameter Adaptation Scheme in Differential Evolution}, 
  year={2024},
  volume={},
  number={},
  pages={33-38},
  abstract={Parameter adaptation in differential evolution (DE) has been a well-known research topic for decades. However, the increasing complexity of expert-designed parameter adaptation schemes can be inconvenient for users. Motivated by the rapid development of large language models (LLMs), this paper proposes a novel and simple parameter adaptation scheme. Specifically, we treat the internal mechanism of parameter adaptation as a black-box problem and employ the LLM Gemini with the standard CRISPE prompt engineering framework to automatically design the parameter adaptation scheme during optimization. The proposed scheme is integrated into DE to form our proposal Gem-iniDE. To evaluate the performance of our proposed GeminiDE, we conduct comprehensive numerical experiments on IEEE-CEC2017 and CEC2022, using constant and random parameter settings in DE as competitor adaptation schemes. Experimental results and statistical analyses demonstrate that our proposed scheme significantly accelerates the optimization convergence of DE and has great potential for extending to various metaheuristic approaches. The source code of this research can be downloaded from https://github.com/RuiZhong961230/GeminiDE.},
  keywords={Statistical analysis;Source coding;Large language models;Metaheuristics;Closed box;Evolutionary computation;Prompt engineering;Proposals;Standards;Convergence;Evolutionary Computation (EC);Differential Evolution (DE);Parameter Adaptation Scheme;Large Language Model},
  doi={10.1109/DOCS63458.2024.10704309},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10500215,
  author={Adom, Isaac and Mahmoud, Mahmoud Nabil},
  booktitle={SoutheastCon 2024}, 
  title={RB-XAI: Relevance-Based Explainable AI for Traffic Detection in Autonomous Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1358-1367},
  abstract={Recent progress in artificial intelligence has brought about a revolutionary era spanning various sectors such as transportation, healthcare, finance, and cybersecurity. In the field of transportation, these advancements empower autonomous systems to precisely perceive their surroundings, make swift and secure decisions, and operate seamlessly without human intervention. In the case of Autonomous Vehicles (AVs), the perception phase, involving tasks like road surface extraction and on-road object detection, is crucial. While AVs offer benefits like improved road safety, convenience, and efficiency, they grapple with the “black box” challenge, symbolizing the opacity of their decision-making processes to humans. This absence of transparency poses acceptance, regulatory, ethical, and security issues. To tackle these challenges, our work leverages Concept Relevance Propagation (CRP), a bias-resistant Relevance-Based (RB) eXplainable Artificial Intelligence (XAI) algorithm to proffer transparent concept - level explanations to the behavior of object detection models used in AVs, for traffic perception. CRP goes beyond traditional attribution maps, by generating explanations that automatically identifies and visualizes relevant concepts within the input space. This insight sheds light on the crucial areas responsible for the behavior of object detection models used in AVs. This research aims to boost transparency, trust, and understanding in the field of AVs, fostering safer and more widely accepted autonomous systems.},
  keywords={Visualization;Explainable AI;Transportation;Object detection;Robustness;Road safety;Regulation;Autonomous Vehicles;eXplainable Artificial Intelligence;Object Detection;Concept Relevance Propagation},
  doi={10.1109/SoutheastCon52093.2024.10500215},
  ISSN={1558-058X},
  month={March},}@INPROCEEDINGS{10647779,
  author={Chowdhury, Prithwijit and Prabhushankar, Mohit and AlRegib, Ghassan and Deriche, Mohamed},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)}, 
  title={Are Objective Explanatory Evaluation Metrics Trustworthy? An Adversarial Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={3938-3944},
  abstract={Explainable AI (XAI) has revolutionized the field of deep learning by empowering users to have more trust in neural network models. The field of XAI allows users to probe the inner workings of these algorithms to elucidate their decision-making processes. The rise in popularity of XAI has led to the advent of different strategies to produce explanations, all of which only occasionally agree. Thus several objective evaluation metrics have been devised to decide which of these modules give the best explanation for specific scenarios. The goal of the paper is twofold: (i) we employ the notions of necessity and sufficiency from causal literature to come up with a novel explanatory technique called SHifted Adversaries using Pixel Elimination(SHAPE) which satisfies all the theoretical and mathematical criteria of being a valid explanation, (ii) we show that SHAPE is, infact, an adversarial explanation that fools causal metrics that are employed to measure the robustness and reliability of popular importance based visual XAI methods. Our analysis shows that SHAPE outperforms popular explanatory techniques like GradCAM and GradCAM++ in these tests and is comparable to RISE, raising questions about the sanity of these metrics and the need for human involvement for an overall better evaluation.},
  keywords={Measurement;Visualization;Shape;Shape measurement;Neural networks;Reliability theory;Predictive models;Explainable AI;Causal Metrics;Adversarial Attacks;Visual Causality;Importance Maps},
  doi={10.1109/ICIP51287.2024.10647779},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10729460,
  author={Fu, Xiaoling},
  booktitle={2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)}, 
  title={Design of an English Translation System Based on Particle Swarm Optimization and Artificial Intelligence Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={1646-1650},
  abstract={The design and establishment of an English translation system through the use of a particle swarm optimization (PSO) and artificial intelligence (AI) model, addresses the element of low accuracy in the learning of English, poor instances of English translation system, and well as thee element of long translation. Given this, the design of an English phrase corpus was created, its part of speech was determined, and the phrase antecedent and postscript probability were calculated using the quaternion cluster in the updated systems technique. The features method of extraction and technique is based on the recognition outcomes to extract the optimum contextual characteristics. Based on this, the attention mechanism and the conventional neural network that are combined to create a deep-learning neural machine translation model as seen in this study.},
  keywords={Accuracy;Heuristic algorithms;Symbols;Speech recognition;Syntactics;Feature extraction;Sensor systems;Machine translation;Particle swarm optimization;Speech processing;Intelligent Systems;Automated Systems;Particle Swarm Optimization System;Attention Mechanism;Artificial Intelligence},
  doi={10.1109/ICSECE61636.2024.10729460},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11040211,
  author={Joseph, Naveena Tresa and Kumar, S N and V, Shobhana N and Mathew, Midhun P},
  booktitle={2025 International Conference on Innovative Trends in Information Technology (ICITIIT)}, 
  title={Particle Swarm Optimization Framework for Underwater Image Enhancement in Big Data Context}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Underwater images are characterized by low-quality, poor, and non-uniform illumination due to light absorption, scattering, and suspended particles. Enhancing these images is significant in improving the image clarity and contrast, which are essential for marine biology, underwater exploration, and environmental monitoring tasks. This research proposes a Particle Swarm Optimization (PSO) framework, integrated with classical enhancement algorithms, to address these challenges in a Big Data environment. The PSO algorithm optimizes contrast and clarity while preserving fine image details, ensuring high-quality enhancements suitable for downstream analytics. The enhancement pipeline incorporates color cast neutralization for superior color channels and dynamic adjustments of gain factors (J and K) for inferior color channels. The histogram is separated into two regions at the average point of mean and median values of the image histogram. Histogram stretching is applied over the entire dynamic range and is combined to create a new histogram of the enhanced image. Mean equalization-based PSO is applied and finally, the edges and details are enhanced by unsharp masking. Compared to traditional methods, experimental results demonstrate remarkable visual and quantitative improvements. The algorithm was tested on Heron Island Coral Reef Public Database (HICRD) and satisfactory results were obtained. The proposed PSO-based enhancement technique increases the interpretability and reliability of an underwater image, validated in terms of performance metrics.},
  keywords={Histograms;Visualization;Image color analysis;Heuristic algorithms;Image edge detection;Marine vegetation;Big Data;Visual databases;Particle swarm optimization;Image enhancement;Particle Swarm Optimization (PSO);Underwater Image Enhancement;mean equalization},
  doi={10.1109/ICITIIT64777.2025.11040211},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10667115,
  author={Damia, AmirHossein and Parvizimosaed, Mohammadreza and Bakhshai, Alireza and Salehi, Mojtaba},
  booktitle={2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title={Optimized Test Data Generation for Path Testing Using Improved Combined Fitness Function with Modified Particle Swarm Optimization Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={412-416},
  abstract={Software testing is essential for assuring the reliability and excellence of software systems. Nevertheless, already used optimization techniques, such Particle Swarm Optimization (PSO), sometimes get stuck in local optima during testing. This study suggests innovative improvements to the PSO algorithm to address and overcome this constraint. Initially, we propose a method in which every particle keeps track of a collection of superior particles and chooses a global best (gbest) at random. This approach helps to explore a wider range of solutions and reduces the likelihood of being stuck in local minima. Furthermore, we use an enhanced crowding method to specifically tackle the discrepancy between the exploration and exploitation stages. This approach prioritizes extensive exploration and exploitation during the early phases of the search, progressively shifting towards a strategy that focuses more on exploitation as the algorithm advances. We present a thorough explanation of these changes, specifically highlighting the modifications made to the pbest section and the use of a novel fitness function that enhances the search process in the given space. The method that we offer has the potential to improve software testing methods by optimizing PSO-based techniques, leading to better performance and efficiency. The experimental findings have shown that our method outperforms numerous existing evolutionary or meta-heuristic algorithms in terms of test data generation speed and achieves superior coverage with fewer evaluations. The algorithms being compared are the Adaptive Genetic Algorithm (AGA), Dandelion Optimizer (DO), Chaotic Flower-Fruit Fly Optimization Algorithm (CFFFOA), Imperialist Competitive Algorithm (ICA), Chaos Adaptive Particle Swarm Optimization Algorithm (CAPSO), Particle Swarm Optimization Algorithm with Empirical Balance Strategy (PSOEBS), and Teaching Learning-Based Optimization (TLBO).},
  keywords={Software testing;Chaos;Software algorithms;Metaheuristics;Education;Flowering plants;Software systems;Software Test;Test Data Generation;Particle Swarm Optimization Algorithm;exploration;exploitation;fitness function},
  doi={10.1109/CCECE59415.2024.10667115},
  ISSN={2576-7046},
  month={Aug},}@INPROCEEDINGS{10552551,
  author={Liu, Yi and Bi, Ying and Cui, Yu and Zhu, Chengyuan and Cheng, Wen and Sheng, Mingkai and Zhang, Yiding},
  booktitle={2024 7th International Symposium on Autonomous Systems (ISAS)}, 
  title={Adapting explainable Decision Sets to Continuous-based Real-World Problems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={For machine learning (ML) models to be trusted in decision-making, eXplainable AI (XAI) becomes essential. A promising route to XAI is the development of interpretable ML models, e.g. Decision Sets (DS), which apply a swarm of rules for intelligent problem resolution. Although there has been progress in creating explainable DS using Evolutionary Computation (EC), these approaches often struggle with complex problems involving continuous features. This difficulty arises because the stochastic nature of these approaches cannot adapt to the complexity of problems based on continuous features. In response, we propose a groundbreaking method that formulates DS by breaking down the problem into sub-tasks, each addressed by a specific rule, through a novel interpretable framework. By leveraging a divide & conquer tactic and utilizing the inherent distribution of data to direct the EC search, this method assembles a rule-based intelligent model. Through testing on a wide array of datasets, our approach has proven to generate explainable DSs that maintain high prediction accuracy when dealing with complex issues involving continuous features.},
  keywords={Adaptation models;Accuracy;Navigation;Explainable AI;Computational modeling;Stochastic processes;Evolutionary computation;eXplainable Artificial Intelligence;Decision Set;Evolutionary Computation},
  doi={10.1109/ISAS61044.2024.10552551},
  ISSN={},
  month={May},}@ARTICLE{10241995,
  author={Alabdulhafith, Maali and Saleh, Hager and Elmannai, Hela and Ali, Zainab Hassan and El-Sappagh, Shaker and Hu, Jong-Wan and El-Rashidy, Nora},
  journal={IEEE Access}, 
  title={A Clinical Decision Support System for Edge/Cloud ICU Readmission Model Based on Particle Swarm Optimization, Ensemble Machine Learning, and Explainable Artificial Intelligence}, 
  year={2023},
  volume={11},
  number={},
  pages={100604-100621},
  abstract={ICU readmission is usually associated with an increased number of hospital death. Predicting readmission helps to reduce such risks by avoiding early discharge, providing appropriate intervention, and planning for patient placement after ICU discharge. Unfortunately, ICU scores such as the simplified acute physiology score (SAPS) and Acute Physiology and Chronic Health (APACHE) could help predict mortality or evaluate illness severity. Still, it is ineffective in predicting ICU readmission. This study introduces a clinical monitoring fog-computing-based system for remote prognosis and monitoring of intensive care patients. This proposed monitoring system uses the advantages of machine learning (ML) approaches for generating a real-time alert signal to doctors for supplying e-healthcare, accelerating decision-making, and monitoring and controlling health systems. The proposed system includes three main layers. First, the data acquisition layer, in which we collect the vital signs and lab tests of the patient’s health conditions in real-time. Then, the fog computing layer processes. The results are then sent to the cloud layer, which offers sizable storage space for patient healthcare. Demographic data, lab tests, and vital signs are aggregated from the MIMIC III dataset for 10,465 patients. Feature selection methods: Genetic algorithm (GA) and practical swarm optimization (PSO) are used to choose the optimal feature subset from detests. Moreover, Different traditional ML models, ensemble learning models, and the proposed stacking models are applied to full features and selected features to predict readmission after 30 days of ICU discharge. The proposed stacking models recorded the highest performance compared to other models. The proposed stacking ensemble model with selected features by POS achieved promising results (accuracy = 98.42, precision = 98.42, recall = 98.42, and F1-Score = 98.42), compared to full features and selected features. We also, provide model explanations to ensure efficiency, effectiveness, and trust in the developed model through local and global explanations.},
  keywords={Predictive models;Hospitals;Monitoring;Machine learning;Edge computing;MIMICs;Real-time systems;Ensemble learning;Stacking;Artificial intelligence;Medical information systems;Particle swarm optimization;Admittance measurement;Machine learning;ensemble learning;stacking ensemble learning;ICU readmission rate;explainable artificial intelligence},
  doi={10.1109/ACCESS.2023.3312343},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11043021,
  author={Shinohara, Yamato and Xu, Jinglue and Li, Tianshui and Iba, Hitoshi},
  booktitle={2025 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Large Language Models as Particle Swarm Optimizers}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Recently, several approaches have gained attention by integrating large language models (LLMs) into evolutionary algorithms. Building on this trend, we introduce Language Model Particle Swarm Optimization (LMPSO), a novel method that incorporates an LLM into the swarm intelligence framework of Particle Swarm Optimization (PSO). In LMPSO, the velocity of each particle is defined as a part of the prompt that generates the next candidate solution, leveraging an LLM to produce solutions while respecting the PSO paradigm. This integration enables an LLM-driven search process that adheres to the foundational principles of PSO. We evaluate LMPSO on the Traveling Salesman Problem (TSP) and on a heuristic improvement task for TSP, where solutions are represented as program code. Heuristic improvement aims to enhance existing heuristics by searching for new ones; it is challenging for standard PSO due to the solution representation in the form of program code. Experimental results demonstrate that LMPSO can generate high-quality solutions for small TSP instances and improve existing TSP heuristics while following the PSO search framework. By incorporating LLMs into PSO, LMPSO expands the applicability of swarm intelligence and highlights the potential of LLMs for addressing complex optimization challenges.},
  keywords={Codes;Large language models;Buildings;Traveling salesman problems;Market research;Particle swarm optimization;Optimization;Standards;particle swarm optimization;large language model;combinatorial optimization;heuristic improvement},
  doi={10.1109/CEC65147.2025.11043021},
  ISSN={},
  month={June},}@ARTICLE{10714354,
  author={Mahmud, Tanjim and Datta, Nippon and Chakma, Rishita and Kanti Das, Utpol and Tarek Aziz, Mohammad and Islam, Musaddikul and Hasnat Muhammed Salimullah, Abul and Shahadat Hossain, Mohammad and Andersson, Karl},
  journal={IEEE Access}, 
  title={An Approach for Crop Prediction in Agriculture: Integrating Genetic Algorithms and Machine Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={173583-173598},
  abstract={Objectives: The agricultural sector in many South Asian countries, including Bangladesh and India, plays a pivotal role in the economy, with a significant portion of the population dependent on it for livelihood. However, farmers often encounter challenges such as unpredictable weather conditions, soil variability, and natural disasters like floods and erosion, leading to substantial crop losses and financial strain. Despite government subsidies, many farmers struggle to sustain their livelihoods, resulting in a decline in interest in agriculture. Our focus lies on predicting the classification of various crops, including rice, jute, maize, and others, based on a combination of soil and weather features. Soil features, including Nitrogen, Phosphorus, Potassium, and pH levels, along with weather variables such as Temperature, Humidity, and Rainfall, are utilized as inputs for the predictive model. Methods: In this study, we address the critical issue of crop prediction by leveraging advanced machine-learning techniques and integrating genetic algorithms into the predictive model. Our proposed approach employs a hybrid methodology, where a Genetic Algorithm is utilized to optimize the hyperparameters of the model, enhancing its performance and robustness. Specifically, we employ a Random Forest classifier, a powerful ensemble learning technique, to classify the class labels associated with 22 different types of crops. Findings: The model’s accuracy is evaluated extensively, demonstrating a remarkable accuracy rate of 99.3%. Additionally, we utilized Local Interpretable Model-agnostic Explaination(LIME) and SHapley Additive exPlanations(SHAP) Explainable AI (XAI) methods to interpret and validate the model’s predictions. Novelty: The study presents a unique method for crop prediction that combines machine learning (ML) with genetic algorithms (GAs). The goal of this integration is to improve crop forecast models’ interpretability and accuracy. Due to the nature of local approximation LIME may yield contradictory answers. On the other hand, for sophisticated models and extensive datasets, SHAP can be computationally costly. By improving feature selection and model parameters, the integration of GAs with ML models overcomes these drawbacks and produces predictions that are more reliable and accurate. The high accuracy achieved by our system underscores its potential to mitigate crop losses and enhance agricultural productivity, thereby contributing to the sustainability and prosperity of the agricultural sector in any country.},
  keywords={Crops;Accuracy;Soil measurement;Predictive models;Meteorology;Random forests;Agriculture;Rain;Prediction algorithms;Genetic algorithms;Explainable AI;Machine learning;Agriculture;crop prediction;genetic algorithm;fitness function;random forest;explainable AI (XAI)},
  doi={10.1109/ACCESS.2024.3478739},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10722564,
  author={Ecik, Emre and John, Werner and Withöft, Julian and Brüning, Ralf and Götze, Jürgen},
  booktitle={2024 International Symposium on Electromagnetic Compatibility – EMC Europe}, 
  title={A Statistically Evaluated Decision Tree Approach for SI-Compliant PCB Design}, 
  year={2024},
  volume={},
  number={},
  pages={140-145},
  abstract={Constantly decreasing switching times in electronic systems lead to EMC problems such as signal integrity (SI)-noise and make it difficult for the PCB designer to find an adequate solution within the available parameter space. AI algorithms are very promising to find SI-compliant design parameters. Methods such as genetic algorithm (GA) or Bayesian optimization (BO) have been used to determine an optimal parameter set and support circuit design. However, the algorithms mentioned can be rather time-consuming for a large dimension of the design space, especially during the training process. Furthermore, there is generally no explanation for the results of the AI approaches available. The designer has to accept the proposed solutions without insight. In this contribution, a combined approach is presented to ensure SI-conformity. Signal waveforms describing the SI behaviour are generated for two different kind of signal networks (point-to-point and daisy chain) and are analyzed with a decision tree (DT) approach for SI-compliance. In order to obtain SI-compliant design parameters, the occurrence of the parameters is analyzed using probability mass functions. Thereby an optimum parameter set is provided as part of the available design space. The proposed method represents a simple approach due to the understandable and hierarchical structure of the decision tree (physically explainable AI).},
  keywords={Training;Support vector machines;Explainable AI;Computational modeling;Artificial neural networks;Electromagnetic compatibility;Bayes methods;Decision trees;Artificial intelligence;Genetic algorithms;Signal integrity;anomaly detection;decision tree;DDR3 memory signals;waveform analysis;(joint) probability mass function;explainable AI},
  doi={10.1109/EMCEurope59828.2024.10722564},
  ISSN={2325-0364},
  month={Sep.},}@INPROCEEDINGS{11042961,
  author={Mohamad Anfar, Mohamad Rimas and Chen, Qi and Zhang, Mengjie},
  booktitle={2025 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Node Importance-Based Multi-Objective Genetic Programming for Enhanced Model Interpretability in Symbolic Regression}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Interpretability is a critical requirement in high-stakes real-world applications. Genetic Programming (GP) is one of the most interpretable machine learning algorithms currently available. While certain datasets necessitate complex GP models to capture underlying patterns, others may not require such complexity. However, GP lacks a mechanism to dynamically assess the required model complexity during evolution. As a result, GP often evolves overly complex models that are challenging to analyze, thereby reducing interpretability. This paper introduces a novel algorithm designed to evolve compact multi-objective GP models without significantly compromising regression error, thereby enhancing interpretability. The proposed method employs NSGA-II to simultaneously minimize regression error and maximize a newly introduced per-node importance metric. This metric quantifies the contribution of each node in terms of error reduction. By leveraging this metric, the algorithm discards large GP models containing many low-importance nodes, effectively avoiding unnecessarily complex solutions. Experimental results on ten regression datasets demonstrate that the proposed method consistently evolves smaller GP models with better interpretability while maintaining competitive predictive performance, particularly on high-dimensional datasets.},
  keywords={Measurement;Analytical models;Machine learning algorithms;Computational modeling;Heuristic algorithms;Genetic programming;Evolutionary computation;Predictive models;Prediction algorithms;Complexity theory;Genetic Programming;Interpretability;NSGA-II;Per-node Importance;Regression},
  doi={10.1109/CEC65147.2025.11042961},
  ISSN={},
  month={June},}@INPROCEEDINGS{10563676,
  author={Kumar, Gaurav and Sharma, Kuldeep and Sharma, Kamal and Dwivedi, Shashi Prakash and Shrivastava, Anurag and Bisht, Yashwant Singh},
  booktitle={2024 4th International Conference on Innovative Practices in Technology and Management (ICIPTM)}, 
  title={Optimizing Power Distribution Grids Using Particle Swarm Optimization Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper is the first to work on metaheuristic algorithm which combines PSO, GA, ACO and DE with deep learning to optimize power distribution networks. In particular, the interpretable methods including guided backscattering and GradCAM++. Valid tests were conducted on a realistic model of the electrical distribution network, which included various scenarios with loads and different conditions in the networks. The findings revealed that DE and PSO yielded superior results in terms of convergence rate, solution quality as well as reliability compared to other algorithms. Thanks to the oriented backpropagation and GradCAM++ techniques, one can get valuable information concerning how different network configurations affect optimization effects providing more transparency in solving a decision-making problem. With the contribution of DE in the comparative analysis with related works, this paper provides a broad view on optimization algorithms of electricity distribution network and enlarges its search scope. This study provides a profound understanding of the behaviour and performance of algorithms under distinct network conditions to informed professionals about tailored optimisation schemes. Introduction of interpretation techniques allows one to identify the opportunities for further enhancing transparency throughout hybrid approaches and optimization processes.},
  keywords={Backpropagation;Metaheuristics;Decision making;Distribution networks;Writing;Transformers;Particle swarm optimization;Metaheuristic Algorithms;Power Distribution Grids;Differential Evolution;Particle Swarm Optimization;Deep Learning Interpretability;Result Values},
  doi={10.1109/ICIPTM59628.2024.10563676},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10797224,
  author={Arpaia, Pasquale and Ammendola, Lidia and Cropano, Maria and De Luca, Matteo and Calce, Anna Della and Gargiulo, Ludovica and Lus, Giacomo and Maffei, Luigi and Malangone, Daniela and Moccaldi, Nicola and Raimo, Simona and Signoriello, Elisabetta and De Blasiis, Paolo},
  booktitle={2024 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)}, 
  title={Identification of EEG Features of Transcranial Electrical Stimulation (tES) Based on eXplainable Artificial Intelligence (XAI)}, 
  year={2024},
  volume={},
  number={},
  pages={1153-1158},
  abstract={Electroencephalographic (EEG) features of transcranial Electrical Stimulation (tES) effects in Multiple Sclerosis (MS) patients were identified. Machine Learning and eXplainable Artificial Intelligence (XAI) algorithms were the used methods for the EEG feature selection. Current tES-based treatments lack of adaptivity to their effects on individuals. Real-time modification of electrical stimulation parameters based on the trend of specific EEG features may represent a new perspective in terms of personalized medicine for MS patients. This preliminary study aimed to identify EEG features reflecting the effect of tES treatment. Ongoing analyses are exploring the correlation between the identified EEG features and the expected clinical outcomes. Five MS patients underwent non-pharmacological treatment combining Theory of Mind (ToM) training with tES or sham treatment. Pre-and post-treatment variation in EEG features were assessed both in Eyes Opened (EO) and Eyes Closed (EC) conditions. In particular, absolute and relative power across six frequency bands, and Posterior Dominant Rhythm (PDR) amplitude and frequency were explored. The Sequential Feature Selection (SFS) algorithm in combination with Support Vector Machine (SVM) classifier identified (i) difference of absolute powers in high beta band in T3 channel, (ii) difference of absolute powers in gamma band in T3 channel, and (iii) difference of absolute powers in gamma band in O2 channel in EO condition, and (i) difference of absolute powers in gamma band in T3 channel, (ii) difference of absolute powers in gamma band in C4 channel, and (iii) difference of PDR amplitude in O2 channel in EC condition as the most discriminating tES from sham treatment (67.5 % accuracy in EO and 83.33 % in EC conditions, respectively). The SHapley Additive exPlanations (SHAP) algorithm highlighted PDR amplitudes in O2 and O1 as most informative features.},
  keywords={Support vector machines;Training;Accuracy;Machine learning algorithms;Explainable AI;Precision medicine;Electrical stimulation;Feature extraction;Electroencephalography;Classification algorithms;Multiple Sclerosis;EEG device;tES;Theory of Mind;eXplainable AI;Machine Learning},
  doi={10.1109/MetroXRAINE62247.2024.10797224},
  ISSN={},
  month={Oct},}@ARTICLE{11043155,
  author={Kabuye, Henry and Issac, Biju and Yumlembam, Rahul and Neera, Jeyamohan},
  journal={IEEE Access}, 
  title={Explainable and Uncertainty Aware AI-Based Ransomware Detection}, 
  year={2025},
  volume={13},
  number={},
  pages={106573-106589},
  abstract={Ransomware poses a serious and evolving threat, demanding detection methods that can adapt to new attack vectors while maintaining transparency and reliability. This study proposes a comprehensive framework that integrates data augmentation, explainable artificial intelligence, and uncertainty quantification to address key challenges in ransomware detection. By leveraging synthetic data generation techniques, the approach mitigates class imbalance and captures varied ransomware behaviours. Simultaneously, explainable AI methods shed light on model decisions, enhancing interpretability and building trust among cybersecurity professionals. An uncertainty-aware component flags ambiguous predictions, allowing for targeted manual reviews and minimising incorrect classifications. Experiments on multiple ransomware datasets show the framework’s ability to maintain high detection rates, even under adversarial conditions. By combining RanSAP and RDset datasets, the framework achieves marked performance improvements. When SMOTE was applied, Random Forest reached an F1-score of 0.9963, while a CNN with Monte Carlo Dropout attained 0.9906. Further incorporating CT-GAN boosted the CNN’s F1-score to 0.9978, underscoring the robustness of our approach. The results suggest that combining robust data augmentation, interpretability, and uncertainty handling offers a practical avenue for deploying reliable ransomware detection systems in real-world environments.},
  keywords={Ransomware;Uncertainty;Adaptation models;Data augmentation;Feature extraction;Monte Carlo methods;Explainable AI;Training;Robustness;Estimation;Data augmentation;explainable AI;machine learning;ransomware;uncertainty},
  doi={10.1109/ACCESS.2025.3581424},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10705229,
  author={Yayavaram, Siddharth and Yayavaram, Arnav and Christopher, Jabez and Arunachalam, Vasan},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={Interpretable Feature Optimization for Sadness Recognition in Speech Emotion Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The paper introduces a new interpretable intelligent system aimed at optimizing feature selection for Speech Emotion Recognition (SER), specifically targeting the emotion of ‘Sadness' across multiple datasets. The system employs Genetic Algorithm (GA) to improve classification performance for ‘Sadness' by maximizing the class-wise F1-score. The framework's efficacy is validated across four prominent speech emotion recognition datasets (Ravdess, Savee, Emodb, Tess), and selected features are analyzed using Shapley values to ascertain their significance in achieving high F1-scores. Experiments use a 10-fold cross-validation with a 90:10 split for training and testing data, while hyperparameters are fine-tuned across different algorithms. The framework's explainability analysis is depicted through beeswarm plots to aggregate these results. Additionally, the study analyzes the frequency distribution of selected feature types and how they manifest. The study's focus on explainability provides insights into the significance of selected features in recognizing sadness in speech, thereby paving the way for enhanced human-computer interaction systems.},
  keywords={Training;Emotion recognition;Speech recognition;Speech enhancement;Feature extraction;Vectors;Intelligent systems;Optimization;Genetic algorithms;Testing;Emotion recognition;Sadness speech Detection;Explainable AI;Metaheuristic Algorithms;Support Vector Machine;Shapley values},
  doi={10.1109/IS61756.2024.10705229},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{11017085,
  author={Batool, Iqra and Gad, Gad and Fouda, Mostafa M. and Ibrahem, Mohamed I. and Fadlullah, Zubair Md},
  booktitle={2025 7th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA)}, 
  title={EXAD: Framework for Designing Explanations in Adaptive User Interfaces}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Adaptive user interfaces that personalize their behavior based on user patterns are becoming increasingly common, yet they often fail to communicate their adaptations to users effectively. This paper introduces EXAD, a framework for designing explanations in adaptive interfaces that considers adaptation type, explanation timing, and user characteristics. Through a mixed-methods study with 42 participants, we investigated how explanation approaches affect trust, understanding, and efficiency across content, layout, and interaction adaptations. Our findings demonstrate that explanation effectiveness varies significantly by adaptation type: Proactive explanations build the highest trust for layout adaptations, while progressive explanations offer the best balance between transparency and efficiency. We extend the EXAD framework to B5G (Beyond Fifth Generation) network systems, enabling context-aware explanations that enhance trust and transparency. Our results show that carefully designed explanation strategies improve user understanding and operational efficiency, and can be translatable in dynamic network environments such as resource allocation based on user patterns in B5G networks. This research bridges theoretical work in explainable AI with practical implementation challenges in adaptive interfaces.},
  keywords={Human computer interaction;Adaptive systems;Explainable AI;Layout;User experience;Timing;Resource management;Robots;Optimization;Network systems;Explainable AI;Adaptive User Interfaces;Human-Computer Interaction;User Trust;Transparency;Explanation Design;5G networks;resource allocation},
  doi={10.1109/ICHORA65333.2025.11017085},
  ISSN={2996-4393},
  month={May},}@INPROCEEDINGS{10796806,
  author={Zhang, Rong},
  booktitle={2024 First International Conference on Software, Systems and Information Technology (SSITCON)}, 
  title={Bidirectional Encoder Representation from Transformers and Particle Swarm Optimization based Evaluation of Oral Function in Preschool Teachers}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Preschool teachers play a vital role in shaping young students’ basic skills and knowledge. Oral communication has aspects like clarity, tone, voice, etc. If early identification of the oral function doesn’t happen, young children might face challenges in communication like pronunciation, clarity, and tone. To overcome these issues this paper and develop a model for automatic evaluation system for oral function of preschool teachers proposed a Bidirectional Encoder Representation from Transformers (BERT) with Particle Swarm Optimization (PSO) for feature selection, efficiently achieving better accuracy. Initially, the process begins with data collection using classroom vocal interaction and engagement in an inclusive preschool named cohort that contains data in audio format. Then, the audio data is preprocessed using noise reduction, and further by incorporating the Natural Language Preprocessing (NLP) technique namely Hidden Markov Model (HMM) the audio data is converted into text data. Then, the converted text data performs feature selection using a Bidirectional Encoder Representation from Transformers (BERT) which selects the relevant features like pitch, frequency, voice, tone, etc. from the converted text. Later, Particle Swarm Optimization (PSO) is used as an optimization technique that optimizes the selected features and gives optimal solutions. The proposed model acquired greater results with an accuracy of 90%.},
  keywords={Accuracy;Noise reduction;Natural languages;Hidden Markov models;Bidirectional control;Transformers;Feature extraction;Encoding;Particle swarm optimization;Optimization;bidirectional encoder representation from transformers;kalman filter;oral function;particle swarm optimization;preschool teachers},
  doi={10.1109/SSITCON62437.2024.10796806},
  ISSN={},
  month={Oct},}@ARTICLE{10024365,
  author={Wang, Shaolin and Mei, Yi and Zhang, Mengjie},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Explaining Genetic Programming-Evolved Routing Policies for Uncertain Capacitated Arc Routing Problems}, 
  year={2024},
  volume={28},
  number={4},
  pages={918-932},
  abstract={Genetic programming (GP) has been successfully used to evolve routing policies that can make real-time routing decisions for uncertain arc routing problems. Although the evolved routing policies are highly effective, they are typically very large and complex, and hard to be understood and trusted by real users. Existing studies have attempted to improve the interpretability by developing new GP approaches to evolve both effective and interpretable (e.g., with smaller program size) routing policies. However, they still have limitations due to the tradeoff between effectiveness and interpretability. To address this issue, we propose a new post-hoc explanation approach to explaining the effective but complex routing policies evolved by GP. The new approach includes a local ranking explanation and a global explanation module. The local ranking explanation uses particle swarm optimization to learn an interpretable linear model that accurately explains the local behavior of the routing policy for each decision situation. Then, the global explanation module uses a clustering technique to summarize the local explanations into a global explanation. The experimental results and case studies on the benchmark datasets show that the proposed method can obtain accurate and understandable explanations of the routing policies evolved for uncertain arc routing problems. Our explanation approach is not restricted to uncertain arc routing, but has a great potential to be generalized to other optimization and machine learning problems, such as learning classifier systems and reinforcement learning.},
  keywords={Task analysis;Routing;Costs;Computational modeling;Vehicle dynamics;Training;Random variables;Capacitated arc routing;genetic programming (GP);hyper-heuristic;interpretability},
  doi={10.1109/TEVC.2023.3238741},
  ISSN={1941-0026},
  month={Aug},}@ARTICLE{10979286,
  author={Cravens, Ben and Lensen, Andrew and Maddigan, Paula and Xue, Bing},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Genetic Programming for Explainable Manifold Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Manifold learning techniques play a pivotal role in machine learning by revealing lower-dimensional embeddings within high-dimensional data, thereby enhancing the efficiency, interpretability, and scalability of data analysis. Despite their utility, current manifold learning methods often lack explicit functional mappings, which are critical for ensuring explainability in regulated and high-stakes applications. This paper introduces Genetic Programming for Explainable Manifold Learning (GP-EMaL), a novel integration of Genetic Programming (GP) and Explainable Artificial Intelligence (XAI). GP-EMaL leverages the inherently interpretable, tree-based structures of GP to generate explicit, functional mappings while directly addressing complexity challenges through innovative penalties for tree size, symmetry, and operator selection. By enabling customisable complexity metrics, GP-EMaL adapts to diverse application needs, achieving high manifold quality and significantly improved explainability. Comprehensive experiments demonstrate that GP-EMaL matches or exceeds the performance of existing approaches, producing simpler and more interpretable models. This work advances the state of explainable manifold learning, paving the way for its adoption in domains such as healthcare, environmental modelling, and financial analysis.},
  keywords={Complexity theory;Manifold learning;Measurement;Optimization;Genetic programming;Computational modeling;Feature extraction;High dimensional data;Evolutionary computation;Syntactics;Manifold learning;genetic programming;dimensionality reduction;explainable artificial intelligence},
  doi={10.1109/TETCI.2025.3561666},
  ISSN={2471-285X},
  month={},}@ARTICLE{10684201,
  author={Shikha, Shikha and Sethia, Divyashikha and Indu, S.},
  journal={IEEE Access}, 
  title={Optimization of Wearable Biosensor Data for Stress Classification Using Machine Learning and Explainable AI}, 
  year={2024},
  volume={12},
  number={},
  pages={169310-169327},
  abstract={This work utilizes wearable devices for real-time stress detection and investigates the effectiveness of meditation audio in reducing stress levels after academic exposure. Physiological data, including Interbeat Interval (IBI)-derived Heart Rate Variability (HRV), Blood Volume Pulse (BVP), and electrodermal activity (EDA), are collected during the Montreal Imaging Stress Task (MIST). The stress classification methodology employs an integrated approach using Genetic Algorithm and Mutual Information to reduce feature set redundancy. It further uses Bayesian optimization to fine-tune machine learning hyperparameters. The results indicate that the combination of EDA, BVP, and HRV achieves the highest classification accuracy of 98.28% and 97.02% using the Gradient Boosting (GB) algorithm for 2-level and 3-level stress classification. In contrast, EDA and HRV alone achieve a comparable accuracy of 97.07% and 95.23% for 2-level and 3-level stress classification, respectively. Furthermore, the SHAP Explainable AI (XAI) analysis confirms that HRV and EDA are the most significant features for stress classification. The study also finds evidence that listening to meditation audio reduces stress levels. These findings highlight the potential of wearable technology combined with machine learning for real-time stress monitoring and management in academic environments.},
  keywords={Stress;Human factors;Biomedical monitoring;Accuracy;Support vector machines;Heart rate variability;Anxiety disorders;Explainable AI;Feature extraction;Machine learning;Wearable devices;Academic environment;explainable AI;feature selection;machine learning;mental stress;wearable device},
  doi={10.1109/ACCESS.2024.3463742},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10371844,
  author={Lezama, Fernando and Almeida, Jose and Soares, Joao and Vale, Zita},
  booktitle={2023 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Explainergy: Towards Explainability of Metaheuristic Performance in the Energy Field}, 
  year={2023},
  volume={},
  number={},
  pages={783-788},
  abstract={We propose the concept of “explainergy”, a new way of including explainability in the metaheuristic performance of algorithms solving problems in the energy domain. To this end, we open the discussion around eXplainable computational Intelligence (XCI), focusing on using metaheuristic optimization for complex energy-related problems. It is well known that computational intelligence applied to optimization cannot guarantee optimality theoretically and also faces issues related to premature convergence, tuning parameters, and variability of the results. These aspects slow the adoption of such methods by energy industry practitioners. Our proposal considers incorporating ideas already applied to the artificial intelligence paradigm, namely those related to eXplainable AI, to motivate current research in this field and provide solutions from metaheuristics with explainability character-istics. Through a case study solving a bidding problem in local electricity markets, we shed light on some ideas that might be advantageous to understanding the metaheuristic performance for energy experts unfamiliar with approximate algorithms. If an XCI framework is successfully developed, it can increase metaheuristic adoption, reliability, and broader success.},
  keywords={Industries;Metaheuristics;Linear programming;Real-time systems;Reliability;Proposals;Object recognition;artificial intelligence;explainergy;explainable decision support systems;explainable computational intelligence;metaheuristics},
  doi={10.1109/SSCI52147.2023.10371844},
  ISSN={2472-8322},
  month={Dec},}@INPROCEEDINGS{10721659,
  author={Alzubaidi, Laith H. and Kumar Pareek, Piyush and Reddy, R. Archana and Abed, Z. and Das, Abhijeet},
  booktitle={2024 International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS)}, 
  title={Predicting Natural Hazards and Disaster Risks using Improved Particle Swarm Optimization based Support Vector Machine}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Predicting natural hazards before its occurrence is crucial to reduce the impact it causes economically to the cities/areas. Therefore, advanced Machine Learning (ML) models can be used to manage disasters by performing an early detection of hazards susceptible locations. The geographical data on natural hazards is collected pre-processed to eliminate the null values. The data is applied with the normalization technique and performed data scaling to transform all the variables to the same range. To obtain complex hierarchical features, Residual Network (ResNet) is used and the extracted features are selected using Attention Based Isolated Forest (ABIF) to solve the standard linear or quadratic optimization problem. To obtain the optimal solution in predicting the natural hazards the parameters of Support Vector Machine (SVM) are optimised using Improved Particle Swarm Optimization (IPSO). This ensures the model’s generalization and predictive ability and to handle high dimension data. The results showed that the proposed IPSO-SVM achieved better prediction accuracy with 98.59%, and precision of 97.74% when compared to the conventional ML methods such as Natural Gradient Boosting Machines (NGBoost) and AdaBag- Nearest Shrunken Centroids (AB-NSC).},
  keywords={Support vector machines;Accuracy;Disasters;Biological system modeling;Transforms;Predictive models;Feature extraction;Hazards;Particle swarm optimization;Optimization;attention based isolated forest;improved particle swarm optimization;natural hazards prediction;residual network;support vector machine},
  doi={10.1109/IACIS61494.2024.10721659},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10913096,
  author={Yi, Jingyuan and Yu, Peiyang and Huang, Tianyi and Xu, Zeqiu},
  booktitle={2024 6th International Conference on Frontier Technologies of Information and Computer (ICFTIC)}, 
  title={Optimization of Transformer Heart Disease Prediction Model Based on Particle Swarm Optimization Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={1109-1113},
  abstract={Aiming at the latest particle swarm optimization algorithm, this paper proposes an improved Trans- former model to improve the accuracy of heart disease prediction and provide a new algorithm idea based on particle swarm optimization (PSO). We first use three mainstream machine learning classification algorithms - decision tree, random forest and XGBoost, and then output the confusion matrix of these three models. The results showed that the random forest model had the best performance in predicting the classification of heart disease, with an accuracy of 92.2%. Then, we apply the Transformer model based on PSO algorithm to the same dataset for the classification experiment. The results show that the classification accuracy of the model is as high as 96.5%, 4.3% higher than that of random forest, which verifies the effectiveness of PSO in optimizing Transformer model. The above research shows that PSO significantly improves Transformer performance in heart disease prediction. Improving the ability to predict heart disease is a global priority with benefits for all humankind. Accurate prediction can enhance public health, optimize medical resources, and reduce healthcare costs, leading to a healthier society. This advancement paves the way for more efficient health management and supports the foundation of a healthier, more resilient global community.},
  keywords={Heart;Accuracy;Machine learning algorithms;Predictive models;Transformers;Prediction algorithms;Classification algorithms;Particle swarm optimization;Random forests;Diseases;Particle swarm optimization;Transformer;Prediction of heart disease},
  doi={10.1109/ICFTIC64248.2024.10913096},
  ISSN={},
  month={Dec},}@ARTICLE{10577273,
  author={Khan, Nadia and Nauman, Muhammad and Almadhor, Ahmad S. and Akhtar, Nadeem and Alghuried, Abdullah and Alhudhaif, Adi},
  journal={IEEE Access}, 
  title={Guaranteeing Correctness in Black-Box Machine Learning: A Fusion of Explainable AI and Formal Methods for Healthcare Decision-Making}, 
  year={2024},
  volume={12},
  number={},
  pages={90299-90316},
  abstract={In recent years, Explainable Artificial Intelligence (XAI) has attracted considerable attention from the research community, primarily focusing on elucidating the opaque decision-making processes inherent in complex black-box machine learning systems such as deep neural networks. This spike in interest originates from the widespread adoption of black-box models, particularly in critical domains like healthcare and fraud detection, highlighting the pressing need to understand and validate their decision-making mechanisms rigorously. In addition, prominent XAI techniques, including LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (Shapley Additive exPlanations), rely on heuristics and cannot guarantee the correctness of the explanations provided. This article systematically addresses this critical issue associated with machine learning and deep learning models, underscoring XAI’s pivotal role in promoting model transparency to enhance decision-making quality. Furthermore, this study advocates integrating Formal Methods to provide correctness guarantees for black-box internal decision-making. The proposed methodology unfolds in three pivotal stages: firstly, training black-box models using neural networks to generate synthetic datasets; secondly, employing LIME and SHAP techniques to interpret the models and visualize their internal decision-making processes; and finally, training decision trees on the synthetic datasets to implement Formal Methods for ensuring the correctness of the black-box model’s decision-making. To validate this proposed approach, experimentation was conducted on four widely recognized medical datasets, including the Wisconsin Breast Cancer and Thyroid Cancer (TC) datasets, which are available in the UCI Machine Learning Repository. Specifically, this research represents a significant contribution by pioneering a novel approach that seamlessly integrates XAI and Formal Methods, thereby furnishing correctness guarantees for internal decision-making processes within the healthcare domain.},
  keywords={Decision making;Closed box;Mathematical models;Explainable AI;Neurons;Predictive models;Brain modeling;Machine learning;Cancer detection;Prognostics and health management;Black-box machine learning;neural networks;interpretable machine learning;cancer prognosis;decision-making;formal methods;formal verification;colored petri nets},
  doi={10.1109/ACCESS.2024.3420415},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10431568,
  author={Nguyen, Elisa and Nauta, Meike and Englebienne, Gwenn and Seifert, Christin},
  booktitle={2023 IEEE 5th International Conference on Cognitive Machine Intelligence (CogMI)}, 
  title={Feature Attribution Explanations for Spiking Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={59-68},
  abstract={Third-generation artificial neural networks, Spiking Neural Networks (SNNs), can be efficiently implemented on hardware. Their implementation on neuromorphic chips opens a broad range of applications, such as machine learning-based autonomous control and intelligent biomedical devices. In critical applications, however, insight into the reasoning of SNNs is important, thus SNNs need to be equipped with the ability to explain how decisions are reached. We present Temporal Spike Attribution (TSA), a local explanation method for SNNs. To compute the explanation, we aggregate all information available in model-internal variables: spike times and model weights. We evaluate TSA on artificial and real-world time series data and measure explanation quality w.r.t. multiple quantitative criteria. We find that TSA correctly identifies a small subset of input features relevant to the decision (i.e., is output-complete and compact) and generates similar explanations for similar inputs (i.e., is continuous). Further, our experiments show that incorporating the notion of absent spikes improves explanation quality. Our work can serve as a starting point for explainable SNNs, with future implementations on hardware yielding not only predictions but also explanations in a broad range of application scenarios. Source code is available at https://github.com/ElisaNguyen/tsa-explanations.},
  keywords={Statistical analysis;Computational modeling;Source coding;Time series analysis;Predictive models;Hardware;Time measurement;Explainability;feature attribution;spiking neural network},
  doi={10.1109/CogMI58952.2023.00018},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9960432,
  author={Parginos, Konstantinos and Bessa, Ricardo and Camal, Simon and Kariniotakis, Georges},
  booktitle={2022 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)}, 
  title={Interpretable data-driven solar power plant trading strategies}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Standard practices of decision-making in energy systems are dynamic, non-linear, complex, and chaotic processes in nature. Trading the power produced by solar photovoltaic (PV) plants in electricity markets is an important decision-making problem which receives increasing attention in the past few decades. The main objective of this paper is to build an interpretable data-driven decision aid model for the case study of a solar power plant with the objective to minimize imbalance costs and thus maximise the revenue, using Symbolic Regression (SR) through Genetic Programming. The use of SR in the experiments and analysis developed in this paper show numerous advantages. SR evolves linear combinations of non-linear functions of the input variables. Three penalty metrics are introduced to enhance the interpretability of the final solutions. SR shows robust results, especially in the case study.},
  keywords={Photovoltaic systems;Measurement;Analytical models;Costs;Decision making;Genetic programming;Wind farms;Artificial Intelligence;Renewables;Interpretability;Trading;Solar;Symbolic Regression;Genetic Programming},
  doi={10.1109/ISGT-Europe54678.2022.9960432},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10903452,
  author={Biswas, Sanad and Grundlingh, Nina and Boardman, Jonathan and White, Joseph and Le, Linh},
  booktitle={2024 International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Target Permutation for Feature Significance and Applications in Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1115-1120},
  abstract={Statistical techniques like generalized linear models have always been the indispensable tools for understanding relationships in data. However, over the last two decades, growth in data size and complexity has fueled the rise of powerful “black box” machine learning models (i.e., Deep Learning). Such models lack transparent mechanisms to evaluate the importance and contributions of individual features. This may be unacceptable ethically and/or legally. Moreover, the inability to precisely assess contributions from features complicates model validation and understanding. In this paper, we propose a permutation test for feature importance in differentiable models with a focus on neural networks. Unlike existing permutation-based methods, ours shuffles the target instead of the inputs. This change allows for simultaneous testing of all features and does not require the assumption of independence among inputs that is prevalent in current work in this area. Through extensive experiments, we empirically demonstrate that this permutation test can reveal highly nonlinear associations, is robust to multicollinearity among features, and can be used to filter unnecessary inputs, while preserving or improving models' predictive performance in both classification and regression.},
  keywords={Training;Deep learning;Ethics;Explainable AI;Computational modeling;Neural networks;Redundancy;Noise;Predictive models;Data models;Feature Importance;Nonlinear Association;Permutation Test;Target Permutation;Explainable AI},
  doi={10.1109/ICMLA61862.2024.00170},
  ISSN={1946-0759},
  month={Dec},}@ARTICLE{10244162,
  author={Singh, Vishakha and Sharma, Ritesh and Singh, Sanjay Kumar},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Designing New Blood-Brain Barrier Penetrating Molecules Using Novel Hybridized Gravitational Search Algorithm and Explainable AI}, 
  year={2024},
  volume={5},
  number={5},
  pages={2127-2138},
  abstract={Artificial intelligence (AI) has emerged as a powerful tool in computational biology, where it is being used to analyze large datasets to detect difficult biological patterns. This has enabled the design of new drug molecules. In this article, a novel method called hybridized gravitational search algorithm (HyGSA) has been proposed to design novel blood-brain barrier penetrating peptides (B3P2s) with desirable characteristics that enable them to cross the blood-brain barrier (BBB) and deliver neurological drugs directly to the brain. The HyGSA has two important modules in the form of an explainable machine learning classifier (with an accuracy, f1-score, and area under the ROC curve (AUROC) of 84%, 84%, and 91%, respectively) and an explainable deep learning-based B3P2 classifier (with an accuracy, f1-score, and AUROC of 89%, 91%, and 95%, respectively). The former was used to determine the crucial hand-engineered features, and the latter was designed to determine the critical amino acids that play an important role in the BBB penetrability of a peptide. Moreover, the population of particles was sampled at the beginning of each iteration to ensure a good mix of particles with low, average, and high fitness. This was achieved using a novel method that takes inspiration from the subset-sum problem and uses the mean and variance of the distribution of particles. For the pilot study, some B3P2s were discovered and optimized from a set of cell-penetrating peptides. Lastly, a free online tool has been deployed at https://b3p2design.anvil.app to help the scientific community discover and optimize B3P2s in protein sequences.},
  keywords={Peptides;Drugs;Search problems;Optimization;Explainable AI;Computational biology;Amino acids;Explainable artificial intelligence (AI);gravitational search algorithm (GSA);multiobjective optimization;nondominated fronts;subset-sum problem},
  doi={10.1109/TAI.2023.3313130},
  ISSN={2691-4581},
  month={May},}@ARTICLE{10449717,
  author={Alexander, Zoe and Chau, Duen Horng and Saldaña, Christopher},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={An Interrogative Survey of Explainable AI in Manufacturing}, 
  year={2024},
  volume={20},
  number={5},
  pages={7069-7081},
  abstract={Artificial intelligence (AI) is a driving force behind Industry 4.0 in manufacturing. Specifically, machine learning has been applied to all parts of the manufacturing process: from product design optimization to anomaly detection for quality control. Explainable AI (XAI) and interpretable AI (IAI) methods have been developed to provide transparency into how models make decisions. This survey presents a thorough review of who, what, when, where, why, and how both IAI and XAI methods have been used in manufacturing. Due to the multidisciplinary nature of manufacturing, this work provides the results from a systematic literature review that surveyed papers from highly rated venues in multiple manufacturing and AI-related fields to give the reader a holistic view of the space. This survey is intended to help both individuals from academia and industry quickly understand the applications, areas of research, and future work involved with creating explainable industrial solutions.},
  keywords={Manufacturing;Biological system modeling;Surveys;Artificial intelligence;Predictive models;Data models;Industries;Artificial intelligence (AI);deep learning (DL);explainable artificial intelligence (XAI);human–computer interaction (HCI);industry 4.0;interpretable artificial intelligence (IAI);machine learning (ML);manufacturing},
  doi={10.1109/TII.2024.3361489},
  ISSN={1941-0050},
  month={May},}@ARTICLE{10976715,
  author={Hameed, Saad and Qolomany, Basheer and Belhaouari, Samir Brahim and Abdallah, Mohamed and Qadir, Junaid and Al-Fuqaha, Ala},
  journal={IEEE Open Journal of the Computer Society}, 
  title={Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models}, 
  year={2025},
  volume={6},
  number={},
  pages={574-585},
  abstract={Determining the ideal architecture for deep learning models, such as the number of layers and neurons, is a difficult and resource-intensive process that frequently relies on human tuning or computationally costly optimization approaches. While Particle Swarm Optimization (PSO) and Large Language Models (LLMs) have been individually applied in optimization and deep learning, their combined use for enhancing convergence in numerical optimization tasks remains underexplored. Our work addresses this gap by integrating LLMs into PSO to reduce model evaluations and improve convergence for deep learning hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the difficulties of efficiency and convergence by using LLMs (particularly ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster achievement of target objectives. Our method speeds up search space exploration by substituting underperforming particle placements with best suggestions offered by LLMs. Comprehensive experiments across three scenarios—(1) optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM) networks for time series regression, and (3) using Convolutional Neural Networks (CNNs) for material classification—show that the method significantly improves convergence rates and lowers computational costs. Depending on the application, computational complexity is lowered by 20% to 60% compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by 60% for both regression and classification tasks, all while preserving accuracy and error rates. This groundbreaking methodology offers a very efficient and effective solution for optimizing deep learning models, leading to substantial computational performance improvements across a wide range of applications.},
  keywords={Computational modeling;Tuning;Neurons;Deep learning;Convergence;Large language models;Computational efficiency;Accuracy;Predictive models;Particle swarm optimization;Deep learning optimization;PSO;LLM;machine learning;hyper-parameter optimization},
  doi={10.1109/OJCS.2025.3564493},
  ISSN={2644-1268},
  month={},}@ARTICLE{9804787,
  author={Giuste, Felipe and Shi, Wenqi and Zhu, Yuanda and Naren, Tarun and Isgut, Monica and Sha, Ying and Tong, Li and Gupte, Mitali and Wang, May D.},
  journal={IEEE Reviews in Biomedical Engineering}, 
  title={Explainable Artificial Intelligence Methods in Combating Pandemics: A Systematic Review}, 
  year={2023},
  volume={16},
  number={},
  pages={5-21},
  abstract={Despite the myriad peer-reviewed papers demonstrating novel Artificial Intelligence (AI)-based solutions to COVID-19 challenges during the pandemic, few have made a significant clinical impact, especially in diagnosis and disease precision staging. One major cause for such low impact is the lack of model transparency, significantly limiting the AI adoption in real clinical practice. To solve this problem, AI models need to be explained to users. Thus, we have conducted a comprehensive study of Explainable Artificial Intelligence (XAI) using PRISMA technology. Our findings suggest that XAI can improve model performance, instill trust in the users, and assist users in decision-making. In this systematic review, we introduce common XAI techniques and their utility with specific examples of their application. We discuss the evaluation of XAI results because it is an important step for maximizing the value of AI-based clinical decision support systems. Additionally, we present the traditional, modern, and advanced XAI models to demonstrate the evolution of novel techniques. Finally, we provide a best practice guideline that developers can refer to during the model experimentation. We also offer potential solutions with specific examples for common challenges in AI model experimentation. This comprehensive review, hopefully, can promote AI adoption in biomedicine and healthcare.},
  keywords={Artificial intelligence;COVID-19;Biological system modeling;Data models;Pandemics;Training;Systematics;COVID-19;electronic health records;expla- inable artificial intelligence;explanation evaluation;explanation generation;explanation representation;medical imaging},
  doi={10.1109/RBME.2022.3185953},
  ISSN={1941-1189},
  month={},}@ARTICLE{10640075,
  author={Goran, Radic and Jovanovic, Luka and Bacanin, Nebojsa and Stanković, Miloš S. and Simic, Vladimir and Antonijevic, Milos and Zivkovic, Miodrag},
  journal={IEEE Access}, 
  title={Identifying and Understanding Student Dropouts Using Metaheuristic Optimized Classifiers and Explainable Artificial Intelligence Techniques}, 
  year={2024},
  volume={12},
  number={},
  pages={122377-122400},
  abstract={This study addresses the pressing issue of student dropout in higher education institutions and explores the potential of artificial intelligence (AI) to mitigate this challenge. Student dropout is a complex phenomenon influenced by diverse factors, including internal and external, student characteristics and skills. To enhance retention strategies, it is crucial to identify the nuanced reasons behind dropout decisions, which often go unnoticed by university staff. Therefore, this study investigates the integration of metaheuristic optimization techniques with Adaptive Boosting (AdaBoost) and eXtreme Gradient Boosting (XGBoost) machine learning (ML) models for student dropout identification. By leveraging these well-known ML techniques, the goal is to enhance the accuracy and reliability of dropout predictions in terms of standard classification metrics. Further, by harnessing the exploration and exploitation capabilities of metaheuristics, the study aims to fine-tune both models, thereby increasing their accuracy and robustness in identifying at-risk students. Additionally, to address limitations of existing metaheuristics, a modified version of recently proposed Sinh Cosh Optimizer (SCHO) was developed, that manages to generate well-performing XGBoost and AdaBoost models for students dropout prediction. The study demonstrates that both tuned models can effectively identify at-risk students, providing valuable insights for targeted educational support initiatives. Three experimental evaluations, two with binary and one with multi-class student dropout classification, are conducted on real-world datasets along with rigid comparative analysis and statistical validation with other cutting-edge metaheuristics. According to experimental outcomes, proposed methodology outscores significantly other approaches in terms of performance. Finally, a comprehensive analysis of influential factors was performed using SHapley Additive exPlanations (SHAP) and Shapley Additive Global importancE (SAGE) explainable AI techniques on the best generated models to identify the factors that most significantly influence dropout decisions. This work contributes to advancing AI applications in higher education, providing insights for policymakers and institutions to design targeted interventions for student retention, ultimately enhancing the overall success and effectiveness of higher education systems.},
  keywords={Education;Metaheuristics;Predictive models;Artificial intelligence;Prediction algorithms;Boosting;Higher education dropout;explainable artificial intelligence;metaheuristics;XGBoost;AdaBoost;optimization},
  doi={10.1109/ACCESS.2024.3446653},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10389065,
  author={Bhat, Ashwin and Raychowdhury, Arijit},
  booktitle={2023 IEEE Biomedical Circuits and Systems Conference (BioCAS)}, 
  title={Explainable ECG Beat Classification On The Edge for Smart, Trustworthy and Low-Power Wearables}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Wearable ECG tracking devices perform diagnosis at the edge utilizing end-to-end Deep Neural Networks (DNN). However, the black-box nature of these models is not conducive to safety-critical medical applications. While Explainable AI (XAI) methods have been utilized to understand the working of these models, their real-time performance has not been explored. In this work, we first design a tiny Convolutional Neural Network (CNN) for automatic heartbeat classification. Second, we use feature attribution to analyze the learning behaviour of the small model. The hardware performance of these compute-intensive algorithms is analyzed on an edge-TPU like systolic array architecture. Finally, we utilize the relevance scores to identify important heartbeat samples and perform selective data transmission to boost the overall energy efficiency of the system.},
  keywords={Performance evaluation;Heart beat;Computational modeling;Wearable computers;Electrocardiography;Real-time systems;Hardware;ECG;Arrhythmia Classification;Convolutional Neural Network (CNN);Explainable AI (XAI);Smart Wearables},
  doi={10.1109/BioCAS58349.2023.10389065},
  ISSN={2766-4465},
  month={Oct},}@INPROCEEDINGS{11077526,
  author={Olusegun, Ruth and Yang, Bo},
  booktitle={2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)}, 
  title={Explainable Tabular Transformer Model for Enhancing Security in Cryptocurrencies}, 
  year={2025},
  volume={},
  number={},
  pages={504-513},
  abstract={The application of Blockchain technology is increasingly becoming attractive to many users and organizations due to its immutability properties. However, its platforms, especially cryptocurrencies, remain vulnerable to cyberattacks. Blockchain relies on its inherent security features, consensus mechanism, and cryptographic hash function. However, these alone are inadequate to address the network's evolving fraud activities. AI-based fraud detection systems have increasingly been utilized to enhance security, but they often struggle with handling complex feature dynamics in blockchain data. This results in intense model training, computational overhead, adaptability, and a lack of clarity. This study presents efficient pre-trained tabular transformers that analyze and make predictions in less than a few seconds. A hybrid feature selection framework was introduced to reduce the data feature space without sacrificing key elements. Four deep learning models were developed alongside the proposed system, including Long ShortTerm Memory (LSTM), Convolutional Neural Networks (CNN), Convolutional Neural Networks, and Long Short-Term Memory (CLSTM) and Multilayer Perceptron (MLP) for performance evaluation. Our proposed model achieves 99.96% and 98.54% accuracies on Ethereum and Bitcoin transactions, respectively, outperforming conventional neural networks and SOTA models. This study introduced a scalable AI solution that can be deployed within the blockchain environment for real-time analysis. This work also addresses efficiency and ethical concerns impacting the reliance on AI in cryptocurrencies.},
  keywords={Deep learning;Explainable AI;Computational modeling;Bitcoin;Transformers;Feature extraction;Blockchains;Fraud;Convolutional neural networks;Long short term memory;Bitcoin;Ethereum;Pre-trained transformer;KernelShap;Bayesian Inference;Explainable AI (XAI);Anomaly Detection},
  doi={10.1109/AIRC64931.2025.11077526},
  ISSN={},
  month={May},}@ARTICLE{10443319,
  author={Hong, Jun and Zhan, Zhi-Hui and He, Langchong and Xu, Zongben and Zhang, Jun},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Protein Structure Prediction Using a New Optimization-Based Evolutionary and Explainable Artificial Intelligence Approach}, 
  year={2025},
  volume={29},
  number={3},
  pages={646-660},
  abstract={Protein structure prediction (PSP) is an important scientific problem because it helps humans to understand how proteins perform their biological functions. This article models the PSP problem as a multiobjective optimization problem with three fast and accurate knowledge-based energy functions. This way, using evolutionary computation (EC)-based artificial intelligence (AI) approach to solve this multiobjective PSP problem to find the optimal structure is explainable. Considering that the multiple populations for multiple objectives (MPMO) framework shows efficient performance in solving lots of multiobjective benchmarks and real-world problems, this article proposes a new AI approach named improved MPMO-based differential evolution (IMPMO-DE) to solve the multiobjective PSP problem. To our best knowledge, this is the first time that MPMO is applied to PSP, with three novel strategies. First, an adaptive archive-based mutation strategy is proposed to better balance the exploration and exploitation abilities by adaptively using different archive-based mutation operators in different evolutionary stages. Second, a mixed individual transfer strategy is proposed to share search information among the multiple populations to accelerate the convergence speed. Third, an evolvable archive update strategy is proposed to generate more promising solutions through evolving the archived solutions. IMPMO-DE is tested on 28 representative proteins and all the available template-free modeling proteins up to 404 residues in the famous critical assessment of protein structure prediction (CASP14) competition. Experimental results show that IMPMO-DE performs better than the compared state-of-the-art EC-based PSP methods and ranks above average compared with all the CASP14 competitors. More importantly, IMPMO-DE is a new efficient AI approach that opens a promising optimization-based evolutionary and explainable way for efficient PSP rather than deep learning approaches like AlphaFold2, especially for newly discovered proteins without similar known protein structures.},
  keywords={Proteins;Statistics;Sociology;Optimization;Evolutionary computation;Artificial intelligence;Prediction algorithms;Artificial intelligence (AI);differential evolution (DE);evolutionary computation (EC);multiobjective evolutionary algorithm (MOEA);multiple populations for multiple objectives (MPMO);protein structure prediction (PSP)},
  doi={10.1109/TEVC.2024.3365814},
  ISSN={1941-0026},
  month={June},}@ARTICLE{10787121,
  author={Raouf, Hussien Abdel and Fouda, Mostafa M. and Ibrahem, Mohamed I.},
  journal={IEEE Open Journal of the Computer Society}, 
  title={Revolutionizing User Authentication Exploiting Explainable AI and CTGAN-Based Keystroke Dynamics}, 
  year={2025},
  volume={6},
  number={},
  pages={97-108},
  abstract={Due to the reliability and efficiency of keystroke dynamics, enterprises have adopted it widely in multi-factor authentication systems, effectively strengthening user authentication and thereby boosting the security of online and offline services. The existing works that detect imposter users suffer from performance and robustness degradation. Therefore, this article introduces a novel methodology to enhance user authentication and identify imposter users who attempt to have unauthorized access. We first use quantile transformation (QT) to mitigate outliers in the user's typing behavior that affects the authentication process and then employ conditional tabular generative adversarial networks (CTGAN) for data augmentation to learn the users' typing patterns better. Next, five accurate transfer learning models (VGG19, EfficientNetB0, Resnet50, MobileNetV2, and DenseNet121) are utilized for extracting effective features within the typing patterns, so our methodology can detect imposter users accurately and hence make precise decisions to enhance the user authentication process. Finally, we ensure transparency and trust in our user authentication methodology by incorporating explainable artificial intelligence (XAI), utilizing local interpretable model-agnostic explanations (LIME). Extensive experiments using a publicly available keystroke dynamics benchmark dataset from Carnegie Mellon University (CMU) showcase superior security performance and robustness using the proposed methodology compared to the state-of-the-art approaches.},
  keywords={Authentication;Accuracy;Feature extraction;Support vector machines;Keystroke dynamics;Hidden Markov models;Biometrics;Passwords;Transfer learning;Robustness;Conditional tabular generative adversarial networks (CTGAN);explainable artificial intelligence (XAI);keystroke dynamics;user authentication},
  doi={10.1109/OJCS.2024.3513895},
  ISSN={2644-1268},
  month={},}@ARTICLE{10299625,
  author={AlMohimeed, Abdulaziz and Saad, Redhwan M. A. and Mostafa, Sherif and El-Rashidy, Nora Mahmoud and Farrag, Sarah and Gaballah, Abdelkareem and Elaziz, Mohamed Abd and El-Sappagh, Shaker and Saleh, Hager},
  journal={IEEE Access}, 
  title={Explainable Artificial Intelligence of Multi-Level Stacking Ensemble for Detection of Alzheimer’s Disease Based on Particle Swarm Optimization and the Sub-Scores of Cognitive Biomarkers}, 
  year={2023},
  volume={11},
  number={},
  pages={123173-123193},
  abstract={Alzheimer’s disease (AD) is a progressive neurological disorder characterized by memory loss and cognitive decline, affecting millions worldwide. Early detection is crucial for effective treatment, as it can slow disease progression and improve quality of life. Machine learning has shown promise in AD detection using various medical modalities. In this paper, we propose a novel multi-level stacking model that combines heterogeneous models and modalities to predict different classes of AD. The modalities include cognitive sub-scores (e.g., clinical dementia rating – sum of boxes, Alzheimer’s disease assessment scale) from the Alzheimer’s Disease Neuroimaging Initiative dataset. In the proposed approach, in level 1, we used six base models (Random Forest (RF), Decision Tree (DT), Support Vector Machine (SVM), Logistic Regression (LR), K-nearest Neighbors (KNN), and Native Bayes (NB)to train each modality (ADAS, CDR, and FQA). Then, we build stacking training that combines the outputs of each base model for the training set and staking testing that combines the outcomes of each model for the testing set. In level 2, three stacking models are produced for each modality that trains and evaluates based on the output of 6 base models based on (RF, LR, DT, SVM, KNN, and NB) are combined in training stacking for the training set and testing stacking for the testing set. Stacking training is used to train meta-learners (RF), and stacking testing is used to evaluate meta-learners (RF). Finally, in level 3, the output prediction of the stacking model from each modality (ADAS, CDR, and FQA) in the training and testing datasets is merged to build a new dataset, which is staking training and stacking testing. Training stacking is used to train the meta-learner, and the testing set is used to evaluate the meta-learner and produce the final prediction. Our research also aims to provide model explanations, ensuring efficiency, effectiveness, and trust through explainable artificial intelligence (XAI). Feature selection optimization based on Particle Swarm Optimization is used to select the most appropriate sub-scores. The proposed model shows significant potential for improving early disease diagnosis. The results demonstrate that the multi-modality approach outperforms single-modality approaches. Moreover, the proposed multi-level stacking models achieve the highest performance with selected features compared to regular ML classifiers and stacking models using full multi-modalities, achieving accuracy, precision, recall, and F1-scores of 92.08%, 92.07%, 92.08%, and 92.01% for two classes, and 90.03%, 90.19%, 90.03%, and 90.05% for three classes, respectively.},
  keywords={Stacking;Support vector machines;Magnetic resonance imaging;Diseases;Alzheimer's disease;Brain modeling;Data models;Machine learning;multi-level stacking machine learning;ensemble learning;sub scores of cognitive;Alzheimer’s disease;explainable artificial intelligence;particle swarm optimization},
  doi={10.1109/ACCESS.2023.3328331},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10945037,
  author={Zuin, Gianlucca and Veloso, Adriano},
  booktitle={2024 IEEE 23rd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)}, 
  title={Navigating Time’s Possibilities: Plausible Counterfactual Explanations for Multivariate Time-Series Forecast through Genetic Algorithms}, 
  year={2024},
  volume={},
  number={},
  pages={2575-2582},
  abstract={Counterfactual learning has become promising for understanding and modeling causality in complex and dynamic systems. This paper presents a novel method for counterfactual learning in the context of multivariate time series analysis and forecast. The primary objective is to uncover hidden causal relationships and identify potential interventions to achieve desired outcomes. The proposed methodology integrates genetic algorithms and rigorous causality tests to infer and validate counterfactual dependencies within temporal sequences. More specifically, we employ Granger causality to enhance the reliability of identified causal relationships, rigorously assessing their statistical significance. Then, genetic algorithms, in conjunction with quantile regression, are used to exploit these intricate causal relationships to project future scenarios. The synergy between genetic algorithms and causality tests ensures a thorough exploration of the temporal dynamics present in the data, revealing hidden dependencies and enabling the projection of outcomes under hypothetical interventions. We evaluate the performance of our algorithm on real-world data, showcasing its ability to handle complex causal relationships, revealing meaningful counterfactual insights, and allowing for the prediction of outcomes under hypothetical interventions.},
  keywords={Navigation;Heuristic algorithms;Scalability;Time series analysis;Refining;Cause effect analysis;Production;Security;Reliability;Genetic algorithms;Counterfactuals;Time-Series Forecast;Genetic Algorithms},
  doi={10.1109/TrustCom63139.2024.00359},
  ISSN={2324-9013},
  month={Dec},}@INPROCEEDINGS{10883411,
  author={V, Thanikachalam and B, Nandhitha and M, Atchaya and R, Raghav Kaushik and S, Srivarshinee},
  booktitle={2025 6th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI)}, 
  title={A Hybrid Quantum Particle Swarm and Genetic Algorithm Framework for Diabetes Prediction Through Feature Selection}, 
  year={2025},
  volume={},
  number={},
  pages={1588-1594},
  abstract={The increasing prevalence of diabetes has necessitated the development of predictive models that can aid in early diagnosis and effective management of the condition. In this study, a novel hybrid optimization framework that combines Quantum Particle Swarm Optimization (QPSO) with Genetic Algorithms (GA) to enhance feature selection processes specifically tailored for diabetes prediction is presented. The approach utilizes a multi-objective fitness function that evaluates the trade-offs between model accuracy, computational efficiency, and the dimensionality of the feature set. By allowing for the simultaneous consideration of multiple criteria, this method aims to identify the most relevant features from the diabetes dataset while minimizing overfitting and maximizing interpretability. Through extensive experimentation on a publicly available diabetes dataset, this hybrid approach outperforms traditional feature selection methods. Results indicate that the selected feature subsets not only improve the accuracy of the Logistic Regression model but also maintain a balance between feature count and computational time. This study highlights the potential of advanced optimization techniques in medical diagnostics, emphasizing their role in developing efficient, interpretable, and accurate predictive models. The implications of this research extend beyond diabetes prediction, providing a framework that can be adapted for various medical applications where feature selection is critical.},
  keywords={Accuracy;Medical services;Machine learning;Predictive models;Feature extraction;Diabetes;Particle swarm optimization;Optimization;Genetic algorithms;Overfitting;Feature Selection;Quantum Particle Swarm Optimization;Genetic Algorithms;Diabetes Prediction;Machine Learning},
  doi={10.1109/ICMCSI64620.2025.10883411},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10150408,
  author={Nishimura, Yasutaka and Takeda, Naoto and Legaspi, Roberto and Ikeda, Kazushi and Plötz, Thomas and Chernova, Sonia},
  booktitle={2023 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, 
  title={Extraction of Important Temporal Order for eXplainable AI on Time-series data}, 
  year={2023},
  volume={},
  number={},
  pages={659-664},
  abstract={We propose a new eXplainable AI method on time-series data in which multiple events are arranged in temporal order, to extract important temporal order between events for opaque model decision. Toward this, our proposed method analyzes the model behavior when inputting perturbated data generated by changing the order of the events. We evaluate our method via an exemplary classification task on a simulated dataset to confirm how accurate our method is in extracting important temporal order of events. In addition, we evaluate our method on a smart home sensor dataset to demonstrate what kind of important order is actually extracted.},
  keywords={Pervasive computing;Analytical models;Conferences;Smart homes;Activity recognition;Data models;Behavioral sciences;eXplainable AI;time-series data;human activity recognition},
  doi={10.1109/PerComWorkshops56833.2023.10150408},
  ISSN={2766-8576},
  month={March},}@INPROCEEDINGS{11064370,
  author={Wang, Zelong and Yin, Chunya and Han, Yitong and Ding, Wenjie},
  booktitle={2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)}, 
  title={Fault Diagnosis Method for Adaptive DC-DC Circuits Based PSO-LSTM}, 
  year={2025},
  volume={},
  number={},
  pages={1216-1219},
  abstract={DC-DC circuits are critical for ensuring the reliable stability of switching power supply systems. However, soft faults caused by diverse operating environments significantly degrade the performance of these systems. To address this issue, this study proposes an adaptive algorithm that integrates particle swarm optimization (PSO) with long short-term memory (LSTM) neural networks for circuit fault diagnosis. Specifically, the hyperparameters of the LSTM network are automatically optimized using the PSO algorithm to accommodate different fault diagnosis tasks and data characteristics. Subsequently, the LSTM network learns temporal features from the data and captures their dependencies. Finally, an self-attention mechanism assigns weights to different time steps to enhance feature importance and outputs the diagnostic results. The proposed method employs multiple evaluation metrics, and the experimental results demonstrate that it performs on par with or surpasses many classical algorithms.},
  keywords={Fault diagnosis;Power supplies;Heuristic algorithms;Circuits;Adaptive algorithms;Prediction algorithms;Circuit stability;Particle swarm optimization;Long short term memory;Switching circuits;fault diagnosis;DC-DC circuits;LSTM;PSO;Self-attention mechanism},
  doi={10.1109/NNICE64954.2025.11064370},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10431573,
  author={Nanavati, Praharsh and Prasad, Ranjitha},
  booktitle={2023 IEEE 5th International Conference on Cognitive Machine Intelligence (CogMI)}, 
  title={CLIMAX: An Exploration of Classifier-Based Contrastive Explanations}, 
  year={2023},
  volume={},
  number={},
  pages={49-58},
  abstract={Explainable AI is an evolving area that deals with understanding the decision making of machine learning models so that these models are more transparent, accountable, and understandable for humans. In particular, post-hoc model-agnostic interpretable AI techniques explain the decisions of a black-box ML model for a single instance locally, without the knowledge of the intrinsic nature of the ML model. Despite their simplicity and capability in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. Moreover, in the context of black-box classifiers, existing approaches justify the predicted class, but these methods do not ensure that the explanation scores strongly differ as compared to those of another class. In this work we propose a novel post-hoc model agnostic XAI technique that provides contrastive explanations justifying the classification of a black box classifier along with a reasoning as to why another class was not predicted. Our method, which we refer to as CLIMAX which is short for Contrastive Label-aware Influence-based Model Agnostic XAI, is based on local classifiers. In order to ensure model fidelity of the explainer, we require the perturbations to be such that it leads to a class-balanced surrogate dataset. Towards this, we employ a label-aware surrogate data generation method based on random oversampling and Gaussian Mixture Model sampling. Further, we propose influence subsampling in order to retaining effective samples and hence ensure sample complexity. We show that we achieve better consistency as compared to baselines such as LIME, BayLIME, and SLIME. We also depict results on textual and image based datasets, where we generate contrastive explanations for any black-box classification model where one is able to only query the class probabilities for an instance of interest.},
  keywords={Perturbation methods;Closed box;Mixture models;Predictive models;Reliability;Machine intelligence;Gaussian mixture model;Contrastive Explanations;Influence Functions;Gaussian Mixture Models},
  doi={10.1109/CogMI58952.2023.00017},
  ISSN={},
  month={Nov},}@ARTICLE{9966927,
  author={Wang, Bin and Pei, Wenbin and Xue, Bing and Zhang, Mengjie},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A Multiobjective Genetic Algorithm to Evolving Local Interpretable Model-Agnostic Explanations for Deep Neural Networks in Image Classification}, 
  year={2024},
  volume={28},
  number={4},
  pages={903-917},
  abstract={Deep convolutional neural networks have become a dominant solution for numerous image classification tasks. However, a main criticism is the poor explainability due to the black-box characteristic, which hurdles the extensive usage of deep convolutional neural networks. To address this issue, this article proposes a new evolutionary multiobjective-based method, which aims to explain the behaviors of deep convolutional neural networks by evolving local explanations on specific images. To the best of our knowledge, this is the first evolutionary multiobjective method to evolve local explanations. The proposed method is model agnostic, i.e., it is applicable to explain any deep convolutional neural networks. ImageNet is used to examine the effectiveness of the proposed method. Three well-known deep convolutional neural networks—VGGNet, ResNet, and MobileNet, are chosen to demonstrate the model-agnostic characteristic. Based on the experimental results, it can be observed that the local explanations are understandable to end users, who need to check the sensibility of the evolved explanations to decide whether to trust the predictions made by the deep convolutional neural networks. Furthermore, the local explanations evolved by the proposed method improves the confidence of deep convolutional neural networks making the predictions. Finally, the pareto front and convergence analyses indicate that the proposed method can form a good set of nondominated solutions.},
  keywords={Feature extraction;Predictive models;Convolutional neural networks;Closed box;Image color analysis;Image classification;Image segmentation;Evolutionary deep learning;explainable machine learning;image classification;local explanations},
  doi={10.1109/TEVC.2022.3225591},
  ISSN={1941-0026},
  month={Aug},}@ARTICLE{10499858,
  author={Lee, Chan Gyu and Jun, Sungbum},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Feature Extraction With Genetic Programming for Root Cause Identification in Manufacturing With Interpretable Machine Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={For fault detection (FD) in manufacturing, various machine learning (ML) models have been widely applied to minimise human intervention and improve detection performance. Even though ML models such as neural networks (NN) have been shown to identify faults effectively, root cause identification (RCI) is becoming more difficult due to their black-box structures and the trade-off between accuracy and interpretability. In order to improve performance while maintaining interpretability, we propose a new framework named FERMAT (Feature Extraction for finding Root causes for Manufacturing Applications with Tree-based algorithms), which enhances the performance of height-limited decision trees (C4.5) through dimensionally-aware genetic programming for feature extraction. Especially in FERMAT, only interpretable features are extracted to prevent decision trees from delivering uninterpretable expressions to practitioners. In the present study, FERMAT’s applicability to RCI was verified with both manufacturing and non-manufacturing datasets with different imbalance ratios. The experimental results showed that FERMAT outperformed the other single-tree-based models by extracting good features and delivered performance comparable to the black-box models.},
  keywords={Feature extraction;Manufacturing;Closed box;Fault diagnosis;Data models;Predictive models;Decision trees;decision tree;dimensional awareness;feature extraction;genetic programming;interpretability;machine learning},
  doi={10.1109/TEVC.2024.3388725},
  ISSN={1941-0026},
  month={},}@ARTICLE{10767756,
  author={Wu, Xingyu and Wu, Sheng-Hao and Wu, Jibin and Feng, Liang and Tan, Kay Chen},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap}, 
  year={2025},
  volume={29},
  number={2},
  pages={534-554},
  abstract={Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride toward artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM’s further enhancement under closed box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this article provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: 1) LLM-enhanced EA and 2) EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this article provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.},
  keywords={Optimization;Closed box;Reviews;Evolutionary computation;Codes;Search problems;Collaboration;Surveys;Software engineering;Prompt engineering;Algorithm generation;evolutionary algorithm (EA);large language model (LLM);neural architecture search (NAS);optimization problem;prompt engineering},
  doi={10.1109/TEVC.2024.3506731},
  ISSN={1941-0026},
  month={April},}@ARTICLE{10927650,
  author={Ma, Shuai and Leng, Jiewu and Chen, Zhuyun and Du, Yixian and Zhang, Xiaoji and Liu, Qiang},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Intrinsically and Post-Hoc Interpretable Kolmogorov–Arnold Network and Genetic Algorithm for Laser Deep Penetration Welding Parameters Optimization}, 
  year={2025},
  volume={74},
  number={},
  pages={1-14},
  abstract={Process parameters are critical in determining the quality and performance of laser deep penetration welding (LDPW) by influencing key factors like breakpoint tensile strength (BTS). Optimizing these parameters is essential for ensuring consistent weld quality and improving overall efficiency. As the dimensionality of process parameters and sample sizes increase, the relationship between process parameters and BTS becomes increasingly complex, uncertain, and nonlinear. While machine learning (ML) techniques struggle to handle this complexity, deep learning (DL) methods, although effective at capturing nonlinear patterns, are constrained by their lack of interpretability, limiting their wider application in industrial environments. To address these challenges, an intrinsically and post-hoc interpretable approach combining the Kolmogorov-Arnold network (KAN) and genetic algorithm (GA) is proposed. An LDPW experimental platform is first established, employing an orthogonal design (L81) with five factors at three levels to gather datasets. A lightweight and interpretable KAN with a single hidden layer is then developed to establish the nonlinear relationship between process parameters and BTS. During optimization, the trained KAN serves as a surrogate model to define decision variables, constraints, and objective functions, with interpretability enhanced through graphical and symbolic representations that clarify how process parameters impact the BTS. Furthermore, the post-hoc Shapley additive explanation (SHAP) method is employed to provide global and local insight into the overall and sample-specific influence on BTS. Validation of the optimization results confirms that the proposed method outperforms other state-of-the-art approaches.},
  keywords={Welding;Computational modeling;Predictive models;Genetic algorithms;Optimization;Mathematical models;Training;Laser beams;Additives;Laser modes;Genetic algorithm (GA);Kolmogorov–Arnold Network (KAN);laser deep penetration welding (LDPW);process parameters optimization (PPO);Shapley additive explanation (SHAP)},
  doi={10.1109/TIM.2025.3551494},
  ISSN={1557-9662},
  month={},}@ARTICLE{9845191,
  author={Wang, Shaolin and Mei, Yi and Zhang, Mengjie},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A Multi-Objective Genetic Programming Algorithm With α Dominance and Archive for Uncertain Capacitated Arc Routing Problem}, 
  year={2023},
  volume={27},
  number={6},
  pages={1633-1647},
  abstract={The uncertain capacitated arc routing problem (UCARP) is an important combinatorial optimization problem with many applications in the real world. Genetic programming hyper-heuristic has been successfully used to automatically evolve routing policies, which can make real-time routing decisions for UCARPs. It is desired to evolve routing policies that are both effective and small/simple to be easily understood. The effectiveness and size are two potentially conflicting objectives. A further challenge is the objective selection bias issue, i.e., it is much more likely to obtain small but ineffective routing policies than the effective ones that are typically large. In this article, we propose a new multiobjective genetic programming algorithm to evolve effective and small routing policies. The new algorithm employs the α dominance strategy with a newly proposed α adaptation scheme to address the objective selection bias issue. In addition, it contains a new archive strategy to prevent the loss of promising individuals due to the rotation of training instances. The experimental results showed that the newly proposed algorithm can evolve significantly better routing policies than the current state-of-the-art algorithms for UCARP in terms of both effectiveness and size. We have also analyzed the evolved routing policies to show better interpretability.},
  keywords={Routing;Task analysis;Costs;Training;Optimization;Genetic programming;Statistics;Capacitated arc routing;genetic programming;hyper-heuristic;multiobjective},
  doi={10.1109/TEVC.2022.3195165},
  ISSN={1941-0026},
  month={Dec},}@INPROCEEDINGS{10598144,
  author={Saqib, Mohd and Fung, Benjamin C.M. and Charland, Philippe and Walenstein, Andrew},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={GAGE: Genetic Algorithm-Based Graph Explainer for Malware Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={2258-2270},
  abstract={Malware analysts often prefer reverse engineering using Call Graphs, Control Flow Graphs (CFGs), and Data Flow Graphs (DFGs), which involves the utilization of black-box Deep Learning (DL) models. The proposed research introduces a structured pipeline for reverse engineering-based analysis, offering promising results compared to state-of-the-art methods and providing high-level interpretability for malicious code blocks in subgraphs. We propose the Canonical Executable Graph (CEG) as a new representation of Portable Executable (PE) files, uniquely incorporating syntactical and semantic information into its node embeddings. At the same time, edge features capture structural aspects of PE files. This is the first work to present a PE file representation encompassing syntactical, semantic, and structural characteristics, whereas previous efforts typically focused solely on syntactic or structural properties. Furthermore, recognizing the limitations of existing graph explanation methods within Explainable Artificial Intelligence (XAI) for malware analysis, primarily due to the specificity of malicious files, we introduce Genetic Algorithm-based Graph Explainer (GAGE). GAGE operates on the CEG, striving to identify a precise subgraph relevant to predicted malware families. Through experiments and comparisons, our proposed pipeline exhibits substantial improvements in model robustness scores and discriminative power compared to the previous benchmarks. Furthermore, we have successfully used GAGE in practical applications on real-world data, producing meaningful insights and interpretability. This research offers a robust solution to enhance cybersecurity by delivering a transparent and accurate understanding of malware behaviour. Moreover, the proposed algorithm is specialized in handling graph-based data, effectively dissecting complex content and isolating influential nodes.},
  keywords={Explainable AI;Taxonomy;Semantics;Reverse engineering;Pipelines;Syntactics;Malware;malware analysis;explainable AI;interpretability;graph;genetic algorithm},
  doi={10.1109/ICDE60146.2024.00179},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10927485,
  author={Nguyen, Duc An and Nguyen, Khanh T. P. and Medjaher, Kamal},
  booktitle={2024 8th International Conference on System Reliability and Safety (ICSRS)}, 
  title={Enhancing PHM with XAI: Review and Dataset for System Health Indicator Construction}, 
  year={2024},
  volume={},
  number={},
  pages={511-517},
  abstract={The integration of Explainable Artificial Intelligence (XAI) into Prognostics and Health Management (PHM) holds the promise of transforming maintenance strategies across various industrial sectors by enhancing the interpretability and accuracy of predictive models. Despite progress, integrating XAI into PHM encounters hurdles, such as the absence of consistent methodologies and a unified framework, complicating the integration of these AI techniques into PHM systems. Additionally, the development of interpretable and robust health indicators (HIs) for systems with multiple components is still largely unexplored. It is hindered by the lack of benchmark data that fully represents system complexities, including component interdependencies and the variability of operational conditions. Therefore, this paper aims at addressing these challenges by offering two main contributions: a focused review of XAI in PHM and the introduction of a novel run-to-fail (RTF) dataset from the Tennessee Eastman Process (TEP). This dataset is a leap forward, enabling the detailed monitoring of component degradation and the exploration of component interactions under various environmental conditions. It allows enhancing the development of explainable HIs and prognostics algorithms at the system level to better reflect the challenges faced in industrial settings.},
  keywords={Degradation;Explainable AI;Reviews;Benchmark testing;Propulsion;Predictive models;Complexity theory;Safety;Prognostics and health management;Usability;Explainable AI;Prognostics and Health Management;Health Indicator;Predictive Maintenance;Tennessee Eastman process},
  doi={10.1109/ICSRS63046.2024.10927485},
  ISSN={},
  month={Nov},}@ARTICLE{10670520,
  author={Zhang, Zherui and Yang, Fan and Cheng, Ran and Ma, Yuxin},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={ParetoTracker: Understanding Population Dynamics in Multi-Objective Evolutionary Algorithms Through Visual Analytics}, 
  year={2025},
  volume={31},
  number={1},
  pages={820-830},
  abstract={Multi-objective evolutionary algorithms (MOEAs) have emerged as powerful tools for solving complex optimization problems characterized by multiple, often conflicting, objectives. While advancements have been made in computational efficiency as well as diversity and convergence of solutions, a critical challenge persists: the internal evolutionary mechanisms are opaque to human users. Drawing upon the successes of explainable AI in explaining complex algorithms and models, we argue that the need to understand the underlying evolutionary operators and population dynamics within MOEAs aligns well with a visual analytics paradigm. This paper introduces ParetoTracker, a visual analytics framework designed to support the comprehension and inspection of population dynamics in the evolutionary processes of MOEAs. Informed by preliminary literature review and expert interviews, the framework establishes a multi-level analysis scheme, which caters to user engagement and exploration ranging from examining overall trends in performance metrics to conducting fine-grained inspections of evolutionary operations. In contrast to conventional practices that require manual plotting of solutions for each generation, ParetoTracker facilitates the examination of temporal trends and dynamics across consecutive generations in an integrated visual interface. The effectiveness of the framework is demonstrated through case studies and expert interviews focused on widely adopted benchmark optimization problems.},
  keywords={Optimization;Heuristic algorithms;Vectors;Evolutionary computation;Visual analytics;Interviews;Inspection;Visual analytics;multi-objective evolutionary algorithms;evolutionary computation},
  doi={10.1109/TVCG.2024.3456142},
  ISSN={1941-0506},
  month={Jan},}@INPROCEEDINGS{10756371,
  author={Mokhtar, N.F.M. and Amir, M.I.S.},
  booktitle={2024 Sixth International Conference on Intelligent Computing in Data Sciences (ICDS)}, 
  title={Mathematical Modelling of NHU Cell Dynamics and Proliferation Using Interpretable Machine Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper introduces a novel approach to predicting the number of cells in the Normal Human Urothelial (NHU) environment in untreated monolayer cultures over 48 hours. We employ symbolic regression to develop a mathematical model that delineates the relationship between cell growth over time, migration speed, and angular velocity. This data-driven technique scrutinises the cell growth curve, a concept well-known among biologists. Our methodology utilises advanced computer vision techniques to monitor time-lapse videos, extracting detailed features such as cell growth, mean migration speed, and angular velocity. A significant innovation in our analysis is the application of Genetic Programming for symbolic regression, which enhances our ability to accurately predict cellular behaviours under control conditions. This approach allows the evolved network to precisely identify features crucial for understanding the responses of cells in untreated scenarios, providing unbiased, efficient insights into cell dynamics. This method supports the development of mathematical models for computational simulations, enhancing our understanding of cellular behaviours under normal conditions.},
  keywords={Computational modeling;Genetic programming;Machine learning;Predictive models;Feature extraction;Mathematical models;Data models;Angular velocity;Dynamic programming;Videos;Genetic programming;symbolic regression;NHU cells;evolutionary algorithms;mathematical modeling},
  doi={10.1109/ICDS62089.2024.10756371},
  ISSN={},
  month={Oct},}@ARTICLE{10979287,
  author={Wang, Bin and Pei, Wenbin and Xue, Bing and Zhang, Mengjie},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Explaining Deep Convolutional Neural Networks for Image Classification by Evolving Local Interpretable Model-Agnostic Explanations}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Deep convolutional neural networks (CNNs) have proven their effectiveness and are widely acknowledged as the dominant method for image classification. However, their lack of explainability remains a significant drawback, particularly in real-world applications where users need to understand the rationale behind predictions to determine their trustworthiness. Local Interpretable Model-agnostic Explanations (LIME) is a popular method for explaining deep CNN predictions, but it suffers from two major limitations: (1) a computationally expensive sampling process to generate perturbed images, and (2) the need to pre-define the number of interpretable features (superpixels), which often requires manual fine-tuning. To address these limitations, we propose a novel Genetic Algorithm (GA)-based method, called E-LIME, which automatically evolves local explanations for deep CNNs. The proposed method eliminates the need for the computationally expensive sampling process used in LIME and allows for the automatic selection of superpixels without pre-defining their number. Specifically, E-LIME introduces a flexible encoding strategy to represent superpixels as binary vectors and a new fitness function that evaluates the selected superpixels based on the probability of the deep CNN making a specific prediction. By optimising the fitness value, E-LIME selects the most informative superpixels while removing noisy features, resulting in more efficient and accurate local explanations. In the experiments, ResNet is used as an example model to be explained, and the ImageNet dataset is selected as the benchmark dataset. DenseNet and MobileNet are further explained to demonstrate the model-agnostic characteristics of the proposed method. The evolved local explanations on four randomly selected images from ImageNet show that the proposed method successfully captures meaningful interpretable features, improving the probabilities/confidences of the deep CNN models in making predictions. Moreover, the proposed method obtains local explanations within one minute, which is more than ten times faster than LIME. The proposed E-LIME method not only overcomes the limitations of LIME but also provides a more efficient and flexible approach to explaining deep CNN predictions, making it highly suitable for real-world applications where interpretability and computational efficiency are critical.},
  keywords={Genetic algorithms;Convolutional neural networks;Computational modeling;Image color analysis;Vectors;Predictive models;Image classification;Contracts;Accuracy;Optimization;Explainable machine learning;local explanations;model-agnostic explanations;evolutionary deep learning;image classification},
  doi={10.1109/TETCI.2025.3558419},
  ISSN={2471-285X},
  month={},}@INPROCEEDINGS{10236793,
  author={Wang, Liang and Zhu, Lei},
  booktitle={2023 International Conference on Networking, Informatics and Computing (ICNETIC)}, 
  title={Building Contour Extraction and Digital Modeling Based on Improved Particle Swarm Optimization and Remote Sensing Data}, 
  year={2023},
  volume={},
  number={},
  pages={739-743},
  abstract={Digitalization and modeling of urban buildings have always been an extremely important content in the construction of digital cities. Automatic extraction by computer can speed up the acquisition and utilization of ground information, and this way can save a lot of manpower and material resources compared with traditional manual digitization. In order to solve the problem that the traditional active contour model can't distinguish between the ground objects with similar reflectivity and the initial contour near the building, this article uses particle swarm optimization (PSO) algorithm and its improved model to solve the problem of three-dimensional space path planning, so as to realize building contour extraction and digital modeling. The improved model can automatically extract the building contour, and has the advantages of being insensitive to noise, not needing the initial contour near the building and implicitly changing the topological structure, and has achieved good results. The algorithm uses fewer iterations when it converges to the global optimal solution, and the generated path quality is higher, which effectively improves the computational efficiency and reliability of PSO algorithm when it is applied to three-dimensional space path planning.},
  keywords={Solid modeling;Image segmentation;Computational modeling;Level set;Buildings;Urban areas;Path planning;Particle swarm optimization;Remote sensing data;Building contour extraction;Digital modeling},
  doi={10.1109/ICNETIC59568.2023.00157},
  ISSN={},
  month={May},}@ARTICLE{10745214,
  author={Zheng, Chengcai and Li, Xisheng and You, Jia and Liu, Yin and Bai, Yanru and Hu, Tao},
  journal={IEEE Sensors Journal}, 
  title={Ellipse Fitting Method Based on Pendulum Genetic Algorithm Particle Swarm Optimization for Well Diameter Measurement by Laser Distance Sensor}, 
  year={2025},
  volume={25},
  number={1},
  pages={587-600},
  abstract={The disadvantages of ellipse fitting method based on algebraic method include its large sampling range, unclear physical explanation of the parameters of the equation, uncontrollable accuracy, and susceptibility to outliers. Aiming at these problems, this article presents a method called pendulum algorithm to calculate the shortest distance from a point to a curve that meets a given accuracy requirement. Based on geometric principles, the pendulum algorithm splits the curve and search interval and calculates the shortest distance iteratively. Since the traditional particle swarm optimization (PSO) algorithm may fall into local optimal value, this article incorporates the idea of genetic algorithm (GA), presents the GA PSO (GAPSO) algorithm, and increases the randomness and diversity of particles, which avoids falling into local optimal value. The calculation results of the pendulum algorithm are used as the fitness function of the GAPSO, and the pendulum GAPSO (P-GAPSO) hybrid algorithm is proposed to realize the ellipse fitting to the data points. The P-GAPSO hybrid algorithm was tested through simulated and real experiments, and in the real experiment, it is applied to the simulated well diameter measurement process by laser distance sensor in different environments. Simulation and experiments validate the effectiveness, feasibility, and superiority of the pendulum algorithm and P-GAPSO hybrid algorithm.},
  keywords={Fitting;Mathematical models;Accuracy;Sensors;Particle swarm optimization;Genetic algorithms;Computational modeling;Measurement uncertainty;Particle measurements;Oscillators;Ellipse fitting;improved particle swarm optimization (PSO);laser distance sensor;pendulum algorithm;well diameter measurement},
  doi={10.1109/JSEN.2024.3488185},
  ISSN={1558-1748},
  month={Jan},}@INPROCEEDINGS{11073849,
  author={Asmita, R. S. and Swetha, M. and Thillai, M. Muthu and Sujibala, T. and Kumar, K. and Vigneshwaran, B.},
  booktitle={2025 7th International Conference on Inventive Material Science and Applications (ICIMA)}, 
  title={Machine Learning Models for DGA Detection in Power Transformers: A Comparative Study of SVM and Random Forest Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={522-526},
  abstract={Power transformer condition monitoring is crucial to guarantee a consistent and dependable power supply. It lowers the chance of unplanned malfunctions and expensive outages by assisting in the early detection of faults. In this respect, two partially optimized neural network models were compared: the first, Support Vector Machine (SVM) optimized using Particle Swarm Optimization (PSO), and the second, Random Forest (RF) optimized using Random Search based on Dissolved Gas Analysis (DGA) data. PD fault detection was performed using the two aforementioned models. The results indicate that the PSO-optimized SVM achieved an accuracy of 96%, outperforming the Random Search-optimized RF, which attained 90% accuracy. Feature importance analysis revealed that hydrogen (H2) and acetylene (C2H2) are significant for fault detection, while methane (CH4) negatively impacts classification performance. Confusion matrix analysis showed that SVM is more effective in distinguishing fault classes compared to RF, although misclassification remains a challenge. The results underscore the need for more robust predictive maintenance strategies to enhance the early detection and diagnosis of transformer faults, ultimately improving the reliability and operation of the systems.},
  keywords={Support vector machines;Radio frequency;Accuracy;Fault detection;Prediction algorithms;Feature extraction;Classification algorithms;Power transformers;Particle swarm optimization;Random forests;Power Transformer;Particle Swarm Optimization;Random Forest;Support Vector Machine;Dissolved Gas Analysis},
  doi={10.1109/ICIMA64861.2025.11073849},
  ISSN={},
  month={May},}@INPROCEEDINGS{10878687,
  author={Jin, Hui},
  booktitle={2024 11th International Forum on Electrical Engineering and Automation (IFEEA)}, 
  title={An Improved PSO-Optimized RBF Forecasting Algorithm and Its Application}, 
  year={2024},
  volume={},
  number={},
  pages={1394-1397},
  abstract={In order to enhance the forecasting capability of the RBF neural network for time series data, a Particle Swarm Optimization (PSO) algorithm is introduced to optimize the neural network parameters. Due to the tendency of the PSO algorithm to fall into local optima when dealing with complex high-dimensional nonlinear problems, this paper proposes an improved adaptive adjustment of the inertia weight IP-R algorithm and applies this algorithm to trajectory prediction in a carbon accumulation environment. Comparative experiments show that the IP-R algorithm reduces the RMSE in trajectory prediction for three types of moving targets by 0.14, 0.15, and 0.16, respectively.},
  keywords={Electrical engineering;Pedestrians;Runtime;Neural networks;Time series analysis;Prediction algorithms;Trajectory;Forecasting;Particle swarm optimization;Carbon;RBF;PSO;carbon accumulation environment;trajectory prediction},
  doi={10.1109/IFEEA64237.2024.10878687},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10633673,
  author={Bramson, Aaron and Mita, Masayoshi},
  booktitle={2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Explicable Machine Learning Models Using Rich Geospatial Data}, 
  year={2024},
  volume={},
  number={},
  pages={2381-2386},
  abstract={Machine learning includes a variety of analysis tools with improved predictive performance compared to traditional statistical approaches. However, that performance comes at a cost of transparency and explainability because much of the models' details are not accessible or interpretable by humans. This problem is compounded by the ability of AI to make use of any correlated data to improve predictions, and practitioners' willingness to sacrifice insight for accuracy. However, there is growing push for “explainable AI”, especially in applications where people's lives, safety, or well-being are affected. Here we explore an intermediate solution suited to policy decision making: leverage the power of tree-based machine learning methods (such as LightGBM) but restrict oneself to variables with plausible causal influence on the target feature. We explore this “explicable AI” approach with an application to housing price prediction and show that using rich geospatial data can replace implicitly spatial variables (such as coordinates and neighborhood names) and achieve similar or better accuracy. And because the explanatory features are naturally interpretable, feature importance analyses provide genuine insights into contributing factors in a way that is similar to hedonic pricing models.},
  keywords={Accuracy;Explainable AI;Computational modeling;Decision making;Pricing;Predictive models;Software;machine learning;geospatial data;explainable AI;evidence-based decision-making;price prediction},
  doi={10.1109/COMPSAC61105.2024.00382},
  ISSN={2836-3795},
  month={July},}@ARTICLE{10217067,
  author={Kuo, Shu-Yu and Jiang, Yu-Chi and Hua, Cheng-Yen and Chou, Yao-Hsin and Kuo, Sy-Yen},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Evo-Panel: Dynamic Visualization Tool for Optimization Process}, 
  year={2023},
  volume={7},
  number={6},
  pages={1717-1732},
  abstract={Evolutionary algorithms (EAs) are efficient computational intelligence (CI) techniques for solving complex optimization problems in different areas. Explainable artificial intelligence (XAI) has recently emerged as a popular research topic as people are increasingly attempting to know the manner in which AI algorithms attain results. In this context, understanding and analyzing the optimization processes of EAs to effectively enhance their performance are crucial and challenging tasks. Most traditional visualization methods focus on dimensionality reduction, visualization of the fitness landscape and the optimization process for a particular algorithm. This study represents the first attempt at establishing a comprehensive dynamic visualization tool, Evo-Panel, that can directly and flexibly illustrate the detailed procedures of different optimization algorithms in solving numerical benchmark functions. The objective is to help users identify the influence of the design formulas, operations, and parameters on the capabilities and performances of the algorithms. Using Evo-Panel, users can trace each movement, observe the process in an intuitive manner, and interpret the processes related to EAs, which are important to XAI. Moreover, Evo-Panel can be used to perform a comparative analysis to illustrate the differences between algorithms. Results of case studies demonstrate that the use of Evo-Panel can allow professors to explain optimization algorithms in a visually interesting manner and facilitate the students' understanding of EAs. The tool can promote CI-related education from teaching and learning perspectives. Furthermore, researchers can use it to obtain information for analysis, facilitate debugging, verify design ideas, gain insights into the process, and enhance the design of EAs.},
  keywords={Optimization;Visualization;Metaheuristics;Benchmark testing;Heuristic algorithms;Genetic algorithms;Evolutionary computation;Computational intelligence;Interactive systems;Optimization methods;Artificial intelligence;Computational intelligence;evolutionary computation;interactive systems;metaheuristics;optimization;visualization tools;XAI},
  doi={10.1109/TETCI.2023.3299298},
  ISSN={2471-285X},
  month={Dec},}
