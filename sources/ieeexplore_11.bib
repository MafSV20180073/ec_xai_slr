@INPROCEEDINGS{10137196,
  author={Li, Wantong and Luo, Yandong and Yu, Shimeng},
  booktitle={2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)}, 
  title={RAWAtten: Reconfigurable Accelerator for Window Attention in Hierarchical Vision Transformers}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={After the success of the transformer networks on natural language processing (NLP), the application of transformers to computer vision has followed suit to deliver unprecedented performance gains on vision tasks including image recognition and object detection. The multi-head self-attention (MSA) is the key component in transformers, allowing the models to learn the amount of attention paid to each input position. In particular, hierarchical vision transformers (HVTs) utilize window-based MSA to capture the benefits of the attention mechanism at various scales for further accuracy enhancements. Despite its strong modeling capability, MSA involves complex operations that make transformers prohibitively costly for hardware deployment. Existing hardware accelerators have mainly focused on the MSA workloads in NLP applications, but HVTs involve different parameter dimensions, input sizes, and data reuse opportunities. Therefore, we design the RAWAtten architecture to target the window-based MSA workloads in HVT models. Each w-core in RAWAtten contains near-memory compute engines for linear layers, MAC arrays for intermediate matrix multiplications, and a lightweight reconfigurable softmax. The w-cores can be combined at runtime to perform hierarchical processing to accommodate varying model parameters. Compared to the baseline GPU, RAWAtten at 40nm provides 2.4x average speedup for running the window-MSA workloads in Swin transformer models while consuming only a fraction of GPU power. In addition, RAWAtten achieves 2x area efficiency compared to prior ASIC accelerator for window-MSA.},
  keywords={Runtime;Power demand;Computational modeling;Graphics processing units;Computer architecture;Performance gain;Transformers;vision transformer;multi-head self-attention;domain-specific accelerator;reconfigurable architecture;near-memory compute},
  doi={10.23919/DATE56975.2023.10137196},
  ISSN={1558-1101},
  month={April},}@INPROCEEDINGS{10692515,
  author={Zhan, Kaichao and Zhang, Xingquan and Liu, Yupei and Yin, Zhiming and Li, Yonghua},
  booktitle={2024 IEEE 7th International Conference on Big Data and Artificial Intelligence (BDAI)}, 
  title={Integrating Mechanistic Knowledge with Deep Learning Models for Wellbore Integrity Risk Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={302-307},
  abstract={As global oil resources gradually deplete, the development of deepwater oil and gas resources becomes particularly important. However, the exploitation of deepwater oil and gas resources is characterized by high technology, high investment, and high risk, especially in terms of production safety. This study aims to accurately predict wellbore annular pressure by analyzing the time series of wellbore production data using advanced Transformer models and a multi-scale mechanism. By incorporating a multi-scale mechanism, the model can pay attention to both short-term subtle changes and long-term trend changes. Experimental results show that compared to traditional LSTM models and Transformer, our Multi-Scale Transformer model demonstrates advantages in prediction accuracy and the ability to handle long sequence data. This will contribute to enhancing the safety and economic efficiency of deepwater oil and gas development.},
  keywords={Accuracy;Biological system modeling;Oils;Production;Predictive models;Oil insulation;Transformers;Data models;Safety;Genetic algorithms;Wellbore Integrity;Risk Assessment;LSTM-CNN-Attention;Genetic Algorithm;Knowledge Graph},
  doi={10.1109/BDAI62182.2024.10692515},
  ISSN={},
  month={July},}@ARTICLE{10623239,
  author={Zeng, Zhao and Gao, Ying},
  journal={IEEE Access}, 
  title={Cost Control Management of Construction Projects Based on Fuzzy Logic and Auction Theory}, 
  year={2024},
  volume={12},
  number={},
  pages={130292-130304},
  abstract={In this study, an innovative method has been proposed for resource allocation among contractors in large construction projects. This method is designed based on a combination of machine learning techniques, fuzzy theory, and auction modeling. Resource allocation in the context of large construction projects, where multiple contractors work simultaneously, can pose a complex problem. Developing an efficient method to address this issue can contribute to improving project performance in terms of cost and construction delays. We have presented a three-stage method for resource allocation in large construction projects. In the first stage, machine learning techniques are utilized to develop two distinct neural network models for predicting costs and delays for each contractor. These models utilize the Genetic Algorithm (GA) to optimize their parameters. In the second stage, a fuzzy model is used, which takes inputs from the neural network models and other contractor-specific features. This model prioritizes the needs of the contractors. Finally, an auction model is employed to fairly distribute the limited project resources between the contractors in need. The implementation results indicate that the proposed method for allocating resources among contractors in large construction projects have achieved MAE and RMSE values of 18.88 and 22.98, respectively, demonstrating a significant performance improvement compared to other proposed methods.},
  keywords={Costs;Delays;Biological system modeling;Resource management;Machine learning;Fuzzy logic;Predictive models;Cost control management;construction projects;fuzzy logic;auction theory},
  doi={10.1109/ACCESS.2024.3438291},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10825046,
  author={Khairul Islam, Muhammed Ifte and Mohammed Saifuddin, Khaled and Hossain, Tanvir and Akbas, Esra},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={DyGCL: Dynamic Graph Contrastive Learning For Event Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={559-568},
  abstract={Predicting events, ranging from political unrest to disease outbreaks and criminal activities, stands as a pivotal task in proactively addressing emerging challenges. Despite the richness of textual data as a source for event detection, it is challenging to extract contextual information from documents due to their complex structure and the dynamic evolution of events. In response to this challenge, dynamic Graph Neural Networks (GNNs) have emerged as a promising tool for capturing the intricate patterns embedded within textual data graphs. Nevertheless, many models in this domain primarily rely on local node-level representations, overlooking the essential global graph-level context. However, both node-level and graph-level representations are critical for effective event prediction. Node-level representations provide insight into the local structure, while graph-level representations offer an understanding of the global structure and the evaluation of temporal graphs. To address these challenges, in this paper, we propose a Dynamic Graph Contrastive Learning (DyGCL) method for event prediction. Our model DyGCL first employs a local view encoder to effectively capture the local dynamic structure of input graphs as the evolving node representations. Then, it performs a global view encoder to perceive the hierarchical dynamic graph representation of the input graphs. Finally, the graph representations from both encoders, optimized via contrastive learning, are combined with an attention mechanism and utilized to predict future events. Our extensive experiments demonstrate that our proposed method outperforms the state-of-the-art methods for event prediction on six real-world datasets.},
  keywords={Attention mechanisms;Event detection;Contrastive learning;Big Data;Graph neural networks;Distance measurement;Data mining;Diseases;Context modeling;Event Prediction;Dynamic Graph Neural Networks;Dynamic Graph;Graph Pooling},
  doi={10.1109/BigData62323.2024.10825046},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{9915813,
  author={Liu, Limin and Chen, Zhen and Wang, Yongsheng and Liu, Guangwen},
  booktitle={2022 IEEE International Conference on Sensing, Diagnostics, Prognostics, and Control ( SDPC)}, 
  title={Predicting Gasoline RON Loss by Machine Learning}, 
  year={2022},
  volume={},
  number={},
  pages={261-267},
  abstract={In order to improve gasoline quality and reduce the environmental pollution caused by gasoline combustion, we aim to study the nonlinear relationship between octane loss and various gasoline components in gasoline refining process, further optimize the gasoline refining process, and reduce the content of sulfur and olefin in gasoline products while minimizing the octane loss. Firstly, we used the nonlinear data reduction algorithm t-SNE to preprocess the data of each component in gasoline and clarify each characteristic parameter; then we constructed the Multiple Nonlinear Regression (MNR) model to analyze the correlation and influence of each characteristic data on octane value; finally, we selected NSGA-II multi-objective genetic algorithm. The NSGA-II multi-objective genetic algorithm was selected for in-depth optimization of the model. The multiple linear regression model was also constructed for comparison experiments, and the advantages and disadvantages of the predicted results of the two models were analyzed, as well as the feasibility of extension. In the experiments with industrial data samples published in Sinopec Gaoqiao Petrochemical real-time database (Honeywell PHD) and LIMS experimental database, it was found that the multivariate nonlinear regression model has more superiority in achieving the purpose of the study, the value of harmful content in gasoline was reduced by 32.7%, and the predicted data of each component of gasoline was more in line with environmental protection requirements. Therefore, the multivariate nonlinear model has the highest reasonableness in predicting the octane loss values.},
  keywords={Analytical models;Databases;Refining;Predictive models;Feature extraction;Data models;Sensors;Environmental protection;Octane loss;Unsupervised feature extraction;Machine learning;Multiple nonlinear regression},
  doi={10.1109/SDPC55702.2022.9915813},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10617339,
  author={Akilandeswari, A. and G, Arasuraja and Yamsani, Nagendar and Radhika, S. and Legapriyadharshini, N. and Padmakala, S.},
  booktitle={2024 International Conference on Advancements in Power, Communication and Intelligent Systems (APCI)}, 
  title={Enhancing Fetal Health Monitoring through TPOT and Optuna in Machine Learning-Driven Prenatal Care}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study delves into the application of advanced machine learning techniques for the classification of fetal health, a critical domain in prenatal care. Utilizing a dataset based on cardiotocograms (CTGs), which record key fetal indicators like heart rate and uterine contractions, we compare two distinct machine learning approaches: a Random Forest Classifier optimized with the hyper parameter tuning tool Optuna, and a genetic programming-based model developed using TPOT (Tree-based Pipeline Optimization Tool).The Random Forest Classifier, configured with specific hyper parameters, delivered an accuracy of 94.13% and an impressive AUC of 0.9826. In contrast, the TPOT-optimized model, a Gradient Boosting Classifier with finely tuned parameters, achieved a higher accuracy of 96.01% and an internal CV score of approximately 95.24%. This comparison underscores the strengths and potential applications of these advanced methodologies in predicting and ensuring fetal health.},
  keywords={Analytical models;Accuracy;Refining;Machine learning;Predictive models;Boosting;Medical diagnosis;Fetal Health Monitoring;Machine Learning in Prenatal Care;Cardiotocogram Analysis;Hyper parameter Optimization;Automated Machine Learning (AutoML);Predictive Modeling in Healthcare},
  doi={10.1109/APCI61480.2024.10617339},
  ISSN={},
  month={June},}@INPROCEEDINGS{10020828,
  author={Drakopoulos, Georgios and Mylonas, Phivos},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={A Genetic Algorithm For Boolean Semiring Matrix Factorization With Applications To Graph Mining}, 
  year={2022},
  volume={},
  number={},
  pages={3864-3870},
  abstract={Matrix factorization is paramount in large scale graph mining as well as a versatile paradigm for dimensionality reduction. In particular, factoring a graph adjacency matrix may well reveal, depending on the specific f actor properties, higher order structure. The latter describes global graph properties better compared to first order connectivity patterns such as vertex degrees. The Boolean semiring factorization of an adjacency matrix yields a product of two smaller and sparser matrices where the former contains disjoint fundamental vertex subsets and the latter combinations thereof. Therefore, the first factor represents community structure and the second has the cross connections between them. In this way graph partitioning and dimensionality reduction are simultaneously achieved. Because of the nature of the Boolean semiring, most common linear algebraic solvers cannot be applied. Moreover, the exact factorization is NP hard. To address these limitations, a genetic algorithm has been developed with evolutionary operations tailored to heuristically compute said factorization which offers interpretability and a high parallelism potential. Besides graph mining the major applications of the Boolean semiring factorization include role mining in enterprise database and operating system realms, curriculum design, and graph flows under inflexible uniqueness constraints. The results obtained by applying the proposed genetic algorithm to synthetic graph benchmarks are very encouraging.},
  keywords={Dimensionality reduction;Databases;Operating systems;Big Data;Parallel processing;Benchmark testing;Data mining;Boolean semiring;dimensionality reduction;matrix factorization;genetic algorithm;graph mining;role mining},
  doi={10.1109/BigData55660.2022.10020828},
  ISSN={},
  month={Dec},}@ARTICLE{10360107,
  author={Wang, Meng and Bian, Zhihao and Yan, Yu and Hussain, Mujahid and Wang, Guobin and Song, Cancan and Lan, Yubin},
  journal={IEEE Access}, 
  title={Research on Flow Decision-Making Model of Plant Protection UAV Based on Feature Selection}, 
  year={2024},
  volume={12},
  number={},
  pages={13699-13710},
  abstract={The field environment is complex and variable, and multiple factors constrain the effectiveness of UAV applications, and a single flow applications may result in over- or under-use of pesticides in plots with different requirements. Therefore, it is crucial to study a decision-making model of flow rate for plant protection UAVs under multi-factor interaction. In this paper, based on a large amount of experimental data, combined with Pearson correlation analysis and random forest variable importance score ranking, screening the features obtained from the experiment increases the correlation between input and output, making the output results more reliable. The model evaluation results showed that the GA-BP neural network model has a correlation coefficient of 0.99 between the true value, predicted value, and a coefficient of determination of 0.98, which is better than the general regression model. A validation test was conducted to test the effectiveness of the model for new data. The final result yields an error value within ±20% for the GA-BP model to predict the flow rate. At the same time, the BP neural network fluctuated more for some of the predicted values, which caused a 50% error in fitting results. It proves the feasibility of the BP neural network optimized based on feature screening and genetic algorithm in plant protection UAV flow rate decision-making, which can provide a reference basis and scientific guidance for precise variable spraying operation of plant protection UAVs.},
  keywords={Spraying;Neural networks;Predictive models;Data models;Biological system modeling;Correlation;Autonomous aerial vehicles;Plant protection drone;BP neural network;genetic algorithm;variable spraying;decision model;spraying flow rate},
  doi={10.1109/ACCESS.2023.3342923},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11010584,
  author={Durdabak, Keremalp and Zincir-Heywood, Nur and Heywood, Malcolm and Jou, Stephan and Pospelova, Maria and Koduvely, Hari M. and Narayanan, Asad},
  booktitle={2025 IEEE Symposium on Computational Intelligence in Security, Defence and Biometrics (CISDB)}, 
  title={Exploring the Effect of Dimensionality Reduction Techniques on Filtration Attacks}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper investigates the effectiveness of dimensionality reduction techniques combined with stratified sampling on the performance of detection models for (in/ex) filtration attacks. Utilizing a Random Forest model, we explore the impact of reduced feature sets derived by dimensionality reduction methods, namely PCA, ICA, AE, and GP. The evaluations are performed on three cybersecurity datasets across various granularity levels. Results indicate that while models trained on reduced feature sets perform comparably to those trained on full feature sets, they offer significant advantages such as reduced computational demands and enhanced explainability, suggesting potential for operational efficiencies in cybersecurity.},
  keywords={Dimensionality reduction;Biometrics;Filtration;Computational modeling;Computational efficiency;Computer security;Computational intelligence;Principal component analysis;Dimensionality Reduction;Computational Intelligence;Attack Detection;Cybersecurity},
  doi={10.1109/CISDB64969.2025.11010584},
  ISSN={},
  month={March},}@INPROCEEDINGS{10631144,
  author={Zhai, Qingchen and Zhang, Zhiwei and Xiao, Ruozhou},
  booktitle={2024 IEEE 35th International Conference on Application-specific Systems, Architectures and Processors (ASAP)}, 
  title={LLM Based End-to-end Branch Predictor Optimization Generator}, 
  year={2024},
  volume={},
  number={},
  pages={214-216},
  abstract={The branch predictor is one of the crucial components influencing the performance of modern processors. This study aims to explore the optimization space for configurable parameters within the branch predictor and, based on the parameter results, generate RTL code end-to-end through a large language model, thereby enhancing the efficiency of processor microarchitecture design. In the research process, we take the TAGE branch predictor as an example, explore it towards the real-time control domain function library, and construct a DSP processor branch predictor for an eight-stage pipelined dual-issue architecture based on LLM.},
  keywords={Program processors;Codes;Microarchitecture;Large language models;Systems architecture;Process control;Real-time systems},
  doi={10.1109/ASAP61560.2024.00050},
  ISSN={2160-052X},
  month={July},}@ARTICLE{10224774,
  author={Xiong, Jing and Hong, Tianqi and Zhao, Dongbo and Zhang, Yu},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={MATNilm: Multi-Appliance-Task Non-Intrusive Load Monitoring With Limited Labeled Data}, 
  year={2024},
  volume={20},
  number={3},
  pages={3177-3187},
  abstract={Nonintrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data that are hard to collect. In this article, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a 2-D attention mechanism in order to capture spatio-temporal correlations among all appliances. With only one-day training data and limited appliance operation profiles, the proposed SA algorithm can achieve comparable test performance to the case of training with the full dataset. Finally, simulation results show that our proposed approach features a significantly improved performance over many baseline models. The relative errors can be reduced by more than 50% on average.},
  keywords={Training;Home appliances;Task analysis;Power demand;Hidden Markov models;Training data;Load monitoring;Attention mechanism;data augmentation (DA);multitask learning;nonintrusive load monitoring (NILM)},
  doi={10.1109/TII.2023.3301026},
  ISSN={1941-0050},
  month={March},}@ARTICLE{9369860,
  author={Ferdaus, Md Meftahul and Chakrabortty, Ripon K. and Ryan, Michael J.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Multiobjective Automated Type-2 Parsimonious Learning Machine to Forecast Time-Varying Stock Indices Online}, 
  year={2022},
  volume={52},
  number={5},
  pages={2874-2887},
  abstract={Real-time forecasting of the financial time-series data is challenging for many machine learning (ML) algorithms. First, many ML models operate offline, where they need a batch of data, which may not be available during training. Besides, due to a fixed architecture of the majority of the offline-based ML models, they suffer to deal with the uncertain nature of financial time-series data. In contrast, online learning mode evolving-structured ML models could be promising for financial time-series forecasting. For real-time deployment of such models, low memory demand is a must. Besides, the model’s explainability plays a crucial role in forecasting financial time-series. Considering all the requirements, a rule-based autonomous neuro-fuzzy learning algorithm called the parsimonious learning machine (PALM) is proposed here to forecast time-varying stock indices. To provide efficient automation of the proposed algorithm by maintaining the model explainability in terms of limited number linguistic IF-THEN rules, two popular multiobjective evolutionary algorithms (MEAs), such as a real-coded genetic algorithm (GA) and a self-adaptive differential evolution (DE) algorithm are utilized here. In addition, fuzzy type-2 variants of PALMs’ are considered here due to better uncertainty handling capacity than their type-1 counterparts. To evaluate the proposed algorithm’s performance, the closing stock price of fifteen (15) different stock market indices are predicted here. From the results, it is observed that the MEA-based PALMs are performing better than the state-of-the-art benchmark online ML models and providing a rule-based explainable model to the end-user.},
  keywords={Predictive models;Prediction algorithms;Data models;Fuzzy logic;Machine learning algorithms;Fuzzy sets;Forecasting;Automated;learning algorithm;meta-heuristics;multiobjective;network complexity},
  doi={10.1109/TSMC.2021.3061389},
  ISSN={2168-2232},
  month={May},}@INPROCEEDINGS{10446661,
  author={Yang, Gaobin and He, Maokui and Niu, Shutong and Wang, Ruoyu and Yue, Yanyan and Qian, Shuangqing and Wu, Shilong and Du, Jun and Lee, Chin-Hui},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding with Sequence-to-Sequence Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={11626-11630},
  abstract={We propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates the strengths of memory-aware multi-speaker embedding (MA-MSE) and sequence-to-sequence (Seq2Seq) architecture, leading to improvement in both efficiency and performance. Next, we further decrease the memory occupation of decoding by incorporating input features fusion and then employ a multi-head attention mechanism to capture features at different levels. NSD-MS2S achieved a macro diarization error rate (DER) of 15.9% on the CHiME-7 EVAL set, which signifies a relative improvement of 49% over the official baseline system, and is the key technique for us to achieve the best performance for the main track of CHiME-7 DASR Challenge. Additionally, we introduce a deep interactive module (DIM) in MA-MSE module to better retrieve a cleaner and more discriminative multi-speaker embedding, enabling the current model to outperform the system we used in the CHiME-7 DASR Challenge. Our code is available at https://github.com/liyunlongaaa/NSD-MS2S.},
  keywords={Codes;Error analysis;Memory management;Graphics processing units;Oral communication;Signal processing;Acoustics;CHiME challenge;speaker diarization;memory-aware speaker embedding;sequence-to-sequence architecture},
  doi={10.1109/ICASSP48485.2024.10446661},
  ISSN={2379-190X},
  month={April},}@ARTICLE{9354353,
  author={Mandalari, Anna Maria and Lutu, Andra and Custura, Ana and Khatouni, Ali Safari and Alay, Özgü and Bagnulo, Marcelo and Bajpai, Vaibhav and Brunstrom, Anna and Ott, Jörg and Trevisan, Martino and Mellia, Marco and Fairhurst, Gorry},
  journal={IEEE Transactions on Mobile Computing}, 
  title={Measuring Roaming in Europe: Infrastructure and Implications on Users’ QoE}, 
  year={2022},
  volume={21},
  number={10},
  pages={3687-3699},
  abstract={“Roam like Home” is the initiative of the European Commission (EC) to end the levy of extra charges when roaming within the European region. As a result, people can use data services more freely across Europe. However, the implications of roaming solutions on network performance have not been carefully examined yet. This paper provides an in-depth characterization of the implications of international data roaming within Europe. We build a unique roaming measurement platform using 16 different mobile networks deployed in six countries across Europe. Using this platform, we measure different aspects of international roaming in 4G networks in Europe, including mobile network configuration, performance characteristics, and quality of experience. We find that operators adopt a common approach to implement roaming called Home-routed roaming (HR). This results in additional latency penalties of 60 ms or more, depending on geographical distance. This leads to worse browsing performance, with an increase in the metrics related to Quality of Experience (QoE) of users (Page Load time and Speed Index) in the order of 15-20 percent. We further analyze in isolation the impact of latency on QoE metrics and find that the penalty imposed by HR leads to a degradation on QoE metrics up to 150 percent in case of intercontinental roaming.},
  keywords={Home automation;Europe;Quality of experience;Servers;IP networks;Mobile nodes;Mobile computing;Roaming;mobile networks;measurements},
  doi={10.1109/TMC.2021.3058787},
  ISSN={1558-0660},
  month={Oct},}@INPROCEEDINGS{10406106,
  author={Byun, Woohong and Mukhopadhyay, Saibal},
  booktitle={2023 IEEE 66th International Midwest Symposium on Circuits and Systems (MWSCAS)}, 
  title={Hessian-Based Parameter Quantization Method for BERT}, 
  year={2023},
  volume={},
  number={},
  pages={516-520},
  abstract={Transformer-based language models have shown outstanding performance in various NLP tasks, but using them on edge devices is very challenging due to their notorious memory usage. To address this issue, this paper proposes a novel parameter quantization method for BERT that quantizes important parameters with higher precision bit width and unimportant parameters with lower precision bit width using a Hessian-based sensitivity metric. The experimental results show that our method achieves 19.6X compression of the model parameters in BERT with a 0.8% accuracy drop on MNLI compared to the BERT base model, generally outperforming other existing layer-wise quantization methods.},
  keywords={Performance evaluation;Quantization (signal);Sensitivity;Circuits and systems;Memory management;Transformers;Task analysis;BERT;quantization;Transformer;NLP;DNN},
  doi={10.1109/MWSCAS57524.2023.10406106},
  ISSN={1558-3899},
  month={Aug},}@INPROCEEDINGS{10045427,
  author={Yao, Tianqing and Chen, David},
  booktitle={2022 8th Annual International Conference on Network and Information Systems for Computers (ICNISC)}, 
  title={The Graph Convolutional Networks Framework for Predicting Pandemic Impact on Stock Prices}, 
  year={2022},
  volume={},
  number={},
  pages={426-430},
  abstract={Covid-19 has dealt an unprecedented hit to the global economy and all industries, with varying degrees of decline from retail to real estate. This volatility is most evident in stock prices. Previous stock price forecasting methods typically used historical data for each stock as a separate input into the system. This paper proposes an attention-based parallel graph convolutional network framework, which consists of two parallel GCNs. The first GCN takes stock features as input, and the second GCN takes other industry features as input, and sets an attention model to reflect the pairwise interactions between networks. Experimental results on selected stock data show that the model outperforms both the LSTM model and the GCN model in accuracy and F1 score.},
  keywords={Industries;COVID-19;Measurement;Pandemics;Computational modeling;Prediction algorithms;Market research;GCN;parallel GCNs;stock prices;forecasting methods},
  doi={10.1109/ICNISC57059.2022.00090},
  ISSN={},
  month={Sep.},}@ARTICLE{10562353,
  author={Wang, Jia-Cheng and Chen, Yao-Jia and Zou, Quan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={GRACE: Unveiling Gene Regulatory Networks With Causal Mechanistic Graph Neural Networks in Single-Cell RNA-Sequencing Data}, 
  year={2025},
  volume={36},
  number={5},
  pages={9005-9017},
  abstract={Reconstructing gene regulatory networks (GRNs) using single-cell RNA sequencing (scRNA-seq) data holds great promise for unraveling cellular fate development and heterogeneity. While numerous machine-learning methods have been proposed to infer GRNs from scRNA-seq gene expression data, many of them operate solely in a statistical or black box manner, limiting their capacity for making causal inferences between genes. In this study, we introduce GRN inference with Accuracy and Causal Explanation (GRACE), a novel graph-based causal autoencoder framework that combines a structural causal model (SCM) with graph neural networks (GNNs) to enable GRN inference and gene causal reasoning from scRNA-seq data. By explicitly modeling causal relationships between genes, GRACE facilitates the learning of regulatory context and gene embeddings. With the learned gene signals, our model successfully decoding the causal structures and alleviates the accurate determination of multiple attributes of gene regulation that is important to determine the regulatory levels. Through extensive evaluations on seven benchmarks, we demonstrate that GRACE outperforms 14 state-of-the-art GRN inference methods, with the incorporation of causal mechanisms significantly enhancing the accuracy of GRN and gene causality inference. Furthermore, the application to human peripheral blood mononuclear cell (PBMC) samples reveals cell type-specific regulators in monocyte phagocytosis and immune regulation, validated through network analysis and functional enrichment analysis.},
  keywords={Gene expression;Regulation;Cause effect analysis;Graph neural networks;Accuracy;Iron;Vectors;Causality inference;gene regulatory networks (GRNs);graph neural networks (GNNs);single-cell RNA sequencing (RNA-seq)},
  doi={10.1109/TNNLS.2024.3412753},
  ISSN={2162-2388},
  month={May},}@ARTICLE{10313024,
  author={Wan, Liangtian and Fu, Zhengqiang and Ling, Yi and Sun, Yuchen and Li, Xiaona and Sun, Lu and Xia, Feng and Yan, Xiaoran and Aggarwal, Charu C.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Z-Laplacian Matrix Factorization: Network Embedding With Interpretable Graph Signals}, 
  year={2024},
  volume={36},
  number={8},
  pages={4331-4345},
  abstract={Network embedding aims to represent nodes with low dimensional vectors while preserving structural information. It has been recently shown that many popular network embedding methods can be transformed into matrix factorization problems. In this paper, we propose the unifying framework “Z-NetMF,” which generalizes random walk samplers to Z-Laplacian graph filters, leading to embedding algorithms with interpretable parameters. In particular, by controlling biases in the time domain, we propose the Z-NetMF-t algorithm, making it possible to scale contributions of random walks of different length. Inspired by node2vec, we design the Z-NetMF-g algorithm, capturing the random walk biases in the graph domain. Moreover, we evaluate the effect of the bias parameters based on node classification and link prediction tasks. The results show that our algorithms, especially the combined model Z-NetMF-gt with biases in both domains, outperform the state-of-art methods while providing interpretable insights at the same time. Finally, we discuss future directions of the Z-NetMF framework.},
  keywords={Matrix decomposition;Signal processing algorithms;Task analysis;Information filters;Electronic mail;Sun;Sparse matrices;Biased random walk;graph laplacian;link prediction;matrix factorization;network embedding;node classification},
  doi={10.1109/TKDE.2023.3331027},
  ISSN={1558-2191},
  month={Aug},}@INPROCEEDINGS{9870349,
  author={Yatsu, Naoya and Shiraishi, Hiroki and Sato, Hiroyuki and Takadama, Keiki},
  booktitle={2022 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={XCSR with VAE using Gaussian Distribution Matching: From Point to Area Matching in Latent Space for Less-overlapped Rule Generation in Observation Space}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper focuses on the matching mechanism of Learning Classifier System (LCS) in a continuous space and proposes a novel matching mechanism based on Gaussian distribution. This mechanism can match the “area” instead of the “point (one value)” in the continuous space unlike the conventional LCS such as XCSR (XCS with Continuous-Valued Inputs). Such an area matching contributes to generating the rules (called classifiers) with less-overlapped with other rules. Concretely, the proposed area matching mechanism employed in XCSR using VAE can generate appropriate classifiers for latent variables with high-dimensional inputs by VAE and create a human-interpretable observation space of human-interpretable classifiers. Since the latent variable in VAE is followed by Gaus-sian distribution, the following three matching mechanisms are compared: (i) the (single) point matching that selects the classifier which condition covers the mean of Gaussian distribution M; (ii) the multiple points matching that selects the classifier which condition covers the data sampled from Gaussian distribution (M, u); and (iii) the area matching that selects the classifier which condition roughly covers a certain area of Gaussian distribution (M, o). Through the intensive experiments on the high dimension maze problem, the following implications have been revealed: (1) the point matching in XCSR with VAE generates the ambiguous classifiers which conditions are overlapped with the other classifiers with the different action; (2) the sampling multiple points matching in XCSR with VAE has a potential of generating the less-overlapped classifiers by improving the data set through sampling. (3) the proposed area matching can generate the less-overlapped classifiers with the same learning steps, which corresponds to the time of the point matching.},
  keywords={Image color analysis;Evolutionary computation;Gaussian distribution;variational autoencoder;data mining},
  doi={10.1109/CEC55065.2022.9870349},
  ISSN={},
  month={July},}@INPROCEEDINGS{10898558,
  author={Zhang, Xiangyue},
  booktitle={2024 IEEE 2nd International Conference on Electrical, Automation and Computer Engineering (ICEACE)}, 
  title={MobileNetV2-Transformer Hybrid Architecture for Effective Video Frame Embeddings Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={171-175},
  abstract={In the realm of video processing and analysis, accurate prediction of future frames is crucial in applications like video compression, anomaly detection and augmented reality. This paper introduces a novel approach that leverages the powerful feature extraction capabilities of MobileNetV2 combined with the temporal sequence modeling strengths of Transformer models to predict the embeddings of following video frames. More specifically, the paper first employs the MobileNetV2 architecture to extract high-dimensional embeddings from individual frames, capitalizing on its efficiency and robustness in handling image data. These embeddings serve as a compact representation of the visual content of each frame. Then, a Transformer model is utilized to analyze the sequential nature of those embeddings, aiming to predict the embeddings of following frames. The Transformer’s multi-head self-attention mechanism allows for an enhanced understanding of the temporal dynamics of the video, facilitating more accurate predictions compared with traditional methods. The paper later evaluates the method on test video datasets. The results indicate that the model needs further development such as incorporating a greater variety and quantity of experimental data. The method involved in the paper opens new avenues for real-time video analysis applications with the combination of Convolutional Neural Networks (CNNs) and Transformer models. The implications of this research extend to improve the performance of video-based systems across various domains, possibly indicating the potential of integrated architectures in advancing video analytic techniques.},
  keywords={Analytical models;Accuracy;Computational modeling;Video sequences;Computer architecture;Predictive models;Streaming media;Transformers;Data models;Robustness;MobileNetV2;multi-head transformer;embeddings;video prediction analysis},
  doi={10.1109/ICEACE63551.2024.10898558},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10437555,
  author={Xie, Zaipeng and Fang, Wenhao and Yu, Bingzhe and Ding, Yang and Pan, Yanling and Song, WenZhan},
  booktitle={GLOBECOM 2023 - 2023 IEEE Global Communications Conference}, 
  title={AMTL-Loc: Efficient WiFi Indoor Localization with Reduced Fingerprint Collection}, 
  year={2023},
  volume={},
  number={},
  pages={2475-2480},
  abstract={Collecting Wi-Fi fingerprints is essential for Wi-Fi-based indoor localization techniques. However, this process can be time-consuming and labor-intensive due to the spatial and tempo-ral variations of Wi-Fi signals caused by environmental factors, interference, and fading. Moreover, the variability of signals emit-ted by different access points can hinder localization accuracy, especially in complex indoor environments. To overcome these challenges, we propose the Attention Mechanism-based Transfer Learning Indoor Localization (AMTL-Loc) framework, which transfers a pre-trained model from a source space to a target space and adapts it using minimal data by extracting redundant information from Wi-Fi fingerprints. Our experimental evalu-ations show that the AMTL-Loc framework can significantly reduce the fingerprint collection workload in diverse indoor environments while maintaining high localization accuracy compared to existing state-of-the-art indoor localization methods. Therefore, our framework offers a promising solution to enhance the efficiency and accuracy of Wi-Fi-based indoor localization techniques.},
  keywords={Location awareness;Transfer learning;Interference;Fingerprint recognition;Data collection;Indoor environment;Wireless fidelity;Wi-Fi fingerprint collection;indoor localization;transfer learning-based method;attention mechanism;spatio-temporal feature},
  doi={10.1109/GLOBECOM54140.2023.10437555},
  ISSN={2576-6813},
  month={Dec},}@INPROCEEDINGS{10991311,
  author={Kumar Reddy, Sana Pavan and Mounika, M. and Harikiran, Jonnadula and Chandana, Bolem Sai},
  booktitle={2024 2nd International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)}, 
  title={Design of an Improved Model for Pothole Detection Using Multiple Scale CNNs and Deep Neural Decision Forest Ensemble Process}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Badly maintained roads and potholes greatly contribute to vehicle damage, accidents, and high maintenance costs. Traditional approaches for pothole detection considered only single-modal data, making their performance in diversified road conditions be defective. Weak detection reliability typically gives them high false positive rates, and they also usually experience problems in variations of pothole sizes, road surface characteristics, and sensor noise. These problems are further addressed in this work through the design of a novel multimodal deep learning framework for pothole detection, based on feature-level fusion of Multiple Scale Convolutional Neural Networks. It fuses visual data acquired by cameras, vibration data captured by accelerometers, and spatial data captured by GPS, making it robust in detection across different environments. MSCNN extracts features at different resolutions to ensure that it captures both small and large potholes, while feature-level fusion further improves the ability of the network to fuse different sensor information cohesively. Such an approach is called multimodal, and this increases robustness in the detection system in highly complex road conditions. Finally, an ensemble learning strategy by using a deep neural decision forest is also adopted to further the model both in accuracy and interpretability. Deep binary decision tree ensembles for abstract feature extraction power from CNNs and RNNs and interpretable decision boundaries. This means that ensemble approaches increase the accuracy and robustness of overall predictions by capitalizing on the strengths of each of its individual model component. Finally, optimization of the hyperparameters is conducted with a hybrid solution of Genetic Algorithm and Gradient-Based Refinement (GA-GR). The proposed system will improve the efficiency, resilience, and detection accuracy compared with other systems, and has a potential positive impact on road-keeping-traffic operations in reducing maintenance costs and improving road safety.},
  keywords={Accuracy;Costs;Fuses;Forestry;Feature extraction;Robustness;Maintenance;Ensemble learning;Convolutional neural networks;Genetic algorithms;Pothole Detection;Multimodal Processing;Deep Neural Decision Forest;Multiple Scale CNN;Genetic Algorithm;Ensemble process},
  doi={10.1109/SCOPES64467.2024.10991311},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10392937,
  author={Mohan, G Bharathi and Kumar, R Prasanna and R, Elakkiya and Pendem, Mukhtesh Venkata Sri Sai},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={Fine-Tuned BERT Based Multilingual Model for Named Entity Recognition in Native Indian Languages}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Accurate recognition of named entities is essential in real-world applications such as information extraction, document classification, sentiment analysis, and machine translation.The fine-tuned BERT model demonstrates exceptional performance in multilingual NER tasks by leveraging language-specific opti-mization, cross-lingual transfer, and context understanding. In contrast, other models applied to multilingual data may exhibit limitations. The research utilizes the WikiAnn dataset, including languages like Tamil, Malayalam, Bengali, and Marathi, to train the fine-tuned BERT model.The system achieved a precision of 95.3% and a recall of 94.8% for the Bengali language, outperforming the generic model used for Indian languages.It demonstrates superior accuracy and effectiveness in deciphering Bengali text.This research provides a robust solution for accurate named entity recognition across multiple languages, benefiting real-world applications.},
  keywords={Measurement;Sentiment analysis;Text analysis;Text recognition;Information retrieval;Transformers;Data models;Named Entity Recognition;Bidirectional Encoder Representations from Transformers;Native Indian languages;WikiAnn dataset;Natural Language Processing;Machine Learning},
  doi={10.1109/EASCT59475.2023.10392937},
  ISSN={},
  month={Oct},}@ARTICLE{10261449,
  author={Olague, Gustavo and Pineda, Roberto and Ibarra-Vazquez, Gerardo and Olague, Matthieu and Martinez, Axel and Bakshi, Sambit and Vargas, Jonathan and Reducindo, Isnardo},
  journal={IEEE Transactions on Emerging Topics in Computing}, 
  title={Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning}, 
  year={2023},
  volume={11},
  number={4},
  pages={1018-1030},
  abstract={Machine learning is at the center of mainstream technology and outperforms classical approaches to handcrafted feature design. Aside from its learning process for artificial feature extraction, it has an end-to-end paradigm from input to output, reaching outstandingly accurate results. However, security concerns about its robustness to malicious and imperceptible perturbations have drawn attention since its prediction can be changed entirely. Salient object detection is a research area where deep convolutional neural networks have proven effective but whose trustworthiness represents a significant issue requiring analysis and solutions to hackers’ attacks. Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations. We test this evolutionary computation methodology against several adversarial attacks and noise perturbations using standard databases and a real-world problem of a shorebird called the Snowy Plover portraying a visual attention task. We compare our methodology with five different deep learning approaches, proving that they do not match the symbolic paradigm regarding robustness. All neural networks suffer significant performance losses, while brain programming stands its ground and remains unaffected. Also, by studying the Snowy Plover, we remark on the importance of security in surveillance activities regarding wildlife protection and conservation.},
  keywords={Robustness;Computational modeling;Perturbation methods;Task analysis;Mathematical models;Visualization;Security;Adversarial examples;visual attention;brain programming;adversarial patch;wildlife conservation},
  doi={10.1109/TETC.2023.3316549},
  ISSN={2168-6750},
  month={Oct},}@ARTICLE{9774367,
  author={Ding, Peng and Jia, Minping and Zhuang, Jichao and Ding, Yifei and Cao, Yudong and Zhao, Xiaoli and Lee, Chi-Guhn},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Multiobjective Evolution Enhanced Collaborative Health Monitoring and Prognostics: A Case Study of Bearing Life Test With Three-Axis Acceleration Signals}, 
  year={2022},
  volume={71},
  number={},
  pages={1-12},
  abstract={Machine learning has brought data-driven methods for equipment health monitoring and prognostics to a new level. In fact, the weak cross-domain generalizability and “Black-box” nature remain unsolved. Hence, we propose a novel framework, multiobjective evolution enhanced collaborative health monitoring and prognostics (MEeCHMPs), in which multiple relevant objectives are optimized via evolutionary approaches to construct health indicators for monitoring and prognosis of equipment health conditions. Specifically, we quantify prior degradation knowledge and domain discrepancy into an unsupervised multicriteria decision-making problem and combine various degradation features so that the Pareto dominance should be maintained. Furthermore, the explainable prognostics model is developed to solve a supervised multicriteria decision-making problem leading to symbolic life expressions, which are accurate and interpretable. Finally, MEeCHMP is applied to bearing life testes involving three-axis acceleration signals to demonstrate its performance in a real case.},
  keywords={Degradation;Monitoring;Machinery;Optimization;Prognostics and health management;Decision making;Predictive models;Health indicator (HI);information fusion;interpretable degradation prognostics;multiobjective optimization;rotating machinery;three-axis acceleration signals},
  doi={10.1109/TIM.2022.3175033},
  ISSN={1557-9662},
  month={},}@ARTICLE{10753026,
  author={Charoenkwan, Phasit and Schaduangrat, Nalini and Moni, Mohammad Ali and Shoombuatong, Watshara},
  journal={IEEE Transactions on Computational Biology and Bioinformatics}, 
  title={iMRSA-Fuse: A Fast and Accurate Computational Approach for Predicting Anti-MRSA Peptides by Fusing Multi-View Information}, 
  year={2025},
  volume={22},
  number={1},
  pages={2-12},
  abstract={Methicillin-resistant S. aureus (MRSA) has prominently emerged among the recognized causes of community-acquired and hospital infections. We proposed a novel computational approach, iMRSA-Fuse, based on a multi-view feature fusion strategy for fast and accurate anti-MRSA peptide identification. In iMRSA-Fuse, we explored and integrated 12 different sequence-based feature descriptors from multiple perspectives, in conjunction with 12 popular machine learning (ML) algorithms, to construct multi-view features that were able to fully capture the useful information of anti-MRSA peptides. Additionally, we applied our customized genetic algorithm to determine a set of multi-view features to enhance its discriminative ability. Based on a series of comparative results, our multi-view features exhibited the most discriminative ability compared to several conventional feature descriptors. Moreover, concerning the independent test dataset, iMRSA-Fuse achieved the best balanced accuracy (BACC) and Matthew's correlation coefficient (MCC) of 0.997 and 0.981, respectively with an increase of 3.93 and 7.78%, respectively. Finally, to facilitate the large-scale identification of candidate anti-MRSA peptides, a user-friendly web server of the iMRSA-Fuse model is constructed and is freely accessible at https://pmlabqsar.pythonanywhere.com/iMRSA-Fuse. We anticipate that this new computational approach will be effectively applied to screen and prioritize candidate peptides that might exhibit the great anti-MRSA activities.},
  keywords={Peptides;Amino acids;Encoding;Bioinformatics;Biological system modeling;Feature extraction;Computational modeling;Accuracy;Random forests;Immune system;Anti-MRSA peptide;sequence analysis;bioinformatics;machine learning;feature representation;multi-view feature},
  doi={10.1109/TCBBIO.2024.3496503},
  ISSN={2998-4165},
  month={Jan},}@ARTICLE{10737068,
  author={Yu, Aihua and Xiao, Qingjie},
  journal={IEEE Access}, 
  title={A Water Quality Prediction Model Based on Long Short-Term Memory Networks and Optimization Algorithms}, 
  year={2024},
  volume={12},
  number={},
  pages={175607-175615},
  abstract={Currently, the water environment is complicated, and the water quality factors are non-linear and non-stationary, so the accuracy of traditional water quality prediction model is restricted. In order to improve the model performance and the prediction accuracy of the water quality factors (PH, dissolved oxygen, ammonia nitrogen and total phosphorus), a prediction model based on AWPSO-LSTMAT is proposed in this paper. The Zhou River in Haihe River Basin will be the major research objective of this study, therefore, the monitoring data of four types of water quality factors collected from the Xitunqiao water quality monitoring section is considered as the data set. In comparison with the previous prediction models such as SVR, LSTM, CNN-LSTM and CNN-GRU, obviously, the prediction effect of AWPSO-LSTMAT is significantly improved by means of modifying and optimizing the original algorithm. The mean absolute error (MAE) in predicting PH, dissolved oxygen (DO), ammonia nitrogen (AN) and total phosphorus (TP) are 0.039, 0.34, 0.119 and 0.019, respectively. Moreover, the designed prediction model can own higher prediction accuracy and stronger generalization than other existed models. According to above-mentioned illustration, this study can provide reliable technical support for the early warning of water quality, and has a good application value for the water environment treatment in the Zhou River.},
  keywords={Rivers;Water quality;Logic gates;Predictive models;Long short term memory;Feature extraction;Accuracy;Vectors;Tuning;Prediction algorithms;Water quality prediction;water quality factors;long short-term memory network;attention mechanism;adaptive weight particle swarm optimization},
  doi={10.1109/ACCESS.2024.3487348},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10392700,
  author={Kumar, Ujjwal and Ameria, Dhairya and Gupta, Pragya},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={MetaFRS- Federated Learning based Cold Start Recommendation System using Meta-Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The field of recommender systems has seen significant advancements recently, but several challenges remain. In this paper, we address two challenges in recommendation systems. Firstly, conventional recommendation systems require uploading private data to a central server for training, which inevitably impacts user privacy. To tackle this issue, we use Graph Federated Learning (GFL), a novel paradigm for distributed learning that ensures privacy preservation by enabling training on distributed data and eliminating direct data sharing. However, distributed recommender systems have a performance gap compared to non-distributed ones due to incomplete user-item interaction graphs. As a result, these systems struggle to utilize indirect user-item interactions effectively. Secondly, the cold start scenario, where a recommender system lacks sufficient data to make accurate recommendations for new users or items. Therefore, we propose MetaFRS - Federated Learning based Cold Start Recommendation System using Meta-Learning to overcome these limitations. Our system incorporates a graph neural network that uses attention mechanisms and an aggregation layer to summarize various orders of indirect user-item and user-user interactions. Meta-learning algorithm is employed to address the issue of sparse interactions in cold start scenarios and incomplete user-item graphs in a distributed setup.},
  keywords={Metalearning;Training;Data privacy;Privacy;Computer aided instruction;Federated learning;Distance learning;Federated Learning;meta-learning;cold-start recommendations;Graph Neural Networks},
  doi={10.1109/EASCT59475.2023.10392700},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10391863,
  author={Wang, Haodong and Ge, Xiaonan and Nie, Guangpeng and Xu, Gui},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={Application Method for Highway Traffic Flow Prediction and Speed Optimization Based on BP Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  abstract={This study focuses on addressing traffic congestion and enhancing Intelligent Transportation Systems (ITS). Anomalies in data are managed through polynomial fitting, and daily and hourly traffic flow on highways is analyzed. It is observed that traffic is heaviest on weekends compared to weekdays. Probability statistics are then utilized to tackle potential errors caused by vehicles transitioning between segments, calculating leakage rates based on toll booth passage probabilities, with passenger vehicles showing the highest leakage rate. A probability model is developed to determine toll lane numbers and emergency systems at each site. Finally, a BP neural network prediction module optimizes speed limits for various segments over the next 24 hours in real-time. In conclusion, the BP neural network model excels in traffic flow prediction, significantly reducing errors compared to other models. This research provides substantial support for Intelligent Transportation Systems and related fields, vital for mitigating congestion and improving road networks.},
  keywords={Computational modeling;Roads;Neural networks;Fitting;Predictive models;Probability;Real-time systems;traffic flow model;pearson correlation coefficient;matlab software;spss software;probability model;bp neural network},
  doi={10.1109/EASCT59475.2023.10391863},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10933000,
  author={Kolavasi, Poojitha and Latha Kakarla, Naga Madhavi},
  booktitle={2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)}, 
  title={A Robust Machine Learning Approach to Heart Disease Detection with Optimized Features}, 
  year={2025},
  volume={},
  number={},
  pages={1140-1147},
  abstract={Heart disease classification remains a complex task due to various challenges, including data imbalance, feature redundancy, and the need for enhanced model interpretability. Several recent techniques, such as deep learning-based approaches, ensemble learning, and hybrid feature selection, have been explored to improve classification accuracy. However, limitations such as overfitting, high computational complexity, and difficulty in selecting clinically relevant features persist. To address these challenges, this study proposes a reliable and robust framework for heart disease classification using machine learning. Methodologically, the approach is anchored on pre-processing the data which includes outlier handling using the Z-score method, such that a reduced dataset with 396 records was obtained; this number was reduced to 389 records for improvement in data consistency and compactness. Standardization is achieved with MaxAbsScaler in such a way that it scales all the features into the range [0, 1] and maintains sparsity while ensuring compatibility with machine learning models. A hybrid feature selection technique GA-LASSO utilizes Lasso regression combined with a Genetic algorithm (GA) so that this subset of features is optimized through regularization and evolutionary search. The most important features were selected: cp, oldpeak, and chol, and thus a fixed low dimension with improved performance was warranted. In the model building stage, an ensemble framework of Support Vector Machine (SVM) and K-Nearest Neighbors (KNN) classifiers combined with the Voting Classifier is used. It has produced excellent balancing of the strengths of two classifiers, while the predictive performance can be delivered commendably. The evaluation metrics show accuracy at 93.59%, precision at 70.59%, recall at 100%, and an F1 score of 82.76%. A high capability for discrimination was there in this model through the ROC AUC score of 0.9874. These results stress the efficiency of proposed methodology in achieving accuracy and reliability towards heart diseases classification.},
  keywords={Heart;Support vector machines;Accuracy;Standardization;Nearest neighbor methods;Feature extraction;Reliability;Stress;Diseases;Genetic algorithms;Heart disease classification;SVM (Support Vector Machine);KNN (K-Nearest Neighbors;Genetic Algorithm;Lasso Regression;ROC AUC Score},
  doi={10.1109/ICSADL65848.2025.10933000},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10392996,
  author={Cui, Qiang and Yang, Xianyan and Jia, Yuyan},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={Design and Implementation of Mobile Intelligent Translation System Based on Android Platform}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={As the most important communication tool between people, language is an ever-increasing demand for people to quickly switch between different languages.This study aims to develop a mobile intelligent translation system based on the Android platform, allowing users to perform multilingual translation anytime and anywhere, and to provide high-quality and accurate translation results.This paper mainly studies the mobile intelligent translation system based on the Android platform. This paper designs and implements a mobile intelligent translation system based on the Android platform, including image acquisition, preprocessing, image text recognition and translation results processing functions. Through experiments, it can be known that the online translation speed of the mobile intelligent translation system constructed in this paper is 31%, the pronunciation success rate is 99%, and the translation success rate is 98.7%, which has a certain gap with the more mature translation systems on the market.},
  keywords={Text recognition;Operating systems;Image processing;Semantics;Speech recognition;Switches;Software;android platform;mobile translation;intelligent translation;translation system},
  doi={10.1109/EASCT59475.2023.10392996},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11076866,
  author={L, Sumanth and B N, Srujan and Acharya, Shravan and Kudari, Shravankumar},
  booktitle={2025 Global Conference in Emerging Technology (GINOTECH)}, 
  title={Artificial Intelligence for Circuit Design and Optimization: Enhancing Efficiency and Performance}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The rapid advancements in artificial intelligence (AI) and machine learning (ML) are transforming the field of electronics design automation (EDA), enabling faster, more efficient, and highly optimized circuit designs. Traditional circuit design methods often require extensive manual effort, iterative testing, and significant expertise, leading to prolonged development cycles. AI-driven approaches, however, offer intelligent automation for circuit synthesis, design space exploration, and printed circuit board (PCB) layout optimization . By lever-aging deep learning and evolutionary algorithms, AI enhances decision-making in component placement, routing, and power-performance trade-offs, ultimately reducing design complexity and improving overall system efficiency. This research explores how AI-powered tools can revolutionize circuit design by minimizing human intervention, accelerating prototyping, and optimizing designs for power, performance, and cost. Additionally, we discuss the challenges and future directions in integrating AI into EDA, ensuring reliability, interpretability, and scalability in real-world applications. The study aims to provide insights into the transformative impact of AI in modern electronics, paving the way for more intelligent, automated, and high-performance circuit design methodologies.},
  keywords={Deep learning;Design automation;Printed circuits;Layout;Manuals;Evolutionary computation;Routing;Circuit synthesis;Artificial intelligence;Optimization;Artificial intelligence;Electronics Design Automation;Deep Learning in Circuit Design;Evolutionary Algorithms in EDA;Circuit Design Optimization},
  doi={10.1109/GINOTECH63460.2025.11076866},
  ISSN={},
  month={May},}@INPROCEEDINGS{9871414,
  author={Ellis, Charles A. and Miller, Robyn L. and Calhoun, Vince D.},
  booktitle={2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={A Model Visualization-based Approach for Insight into Waveforms and Spectra Learned by CNNs}, 
  year={2022},
  volume={},
  number={},
  pages={1643-1646},
  abstract={Recent years have shown a growth in the application of deep learning architectures such as convolutional neural networks (CNNs), to electrophysiology analysis. However, using neural networks with raw time-series data makes explainability a significant challenge. Multiple explainability approaches have been developed for insight into the spectral features learned by CNNs from EEG. However, across electrophysiology modalities, and even within EEG, there are many unique waveforms of clinical relevance. Existing methods that provide insight into waveforms learned by CNNs are of questionable utility. In this study, we present a novel model visualization-based approach that analyzes the filters in the first convolutional layer of the network. To our knowledge, this is the first method focused on extracting explainable information from EEG waveforms learned by CNNs while also providing insight into the learned spectral features. We demonstrate the viability of our approach within the context of automated sleep stage classification, a well-characterized domain that can help validate our approach. We identify 3 subgroups of filters with distinct spectral properties, determine the relative importance of each group of filters, and identify several unique waveforms learned by the classifier that were vital to the classifier performance. Our approach represents a significant step forward in explainability for electrophysiology classifiers, which we also hope will be useful for providing insights in future studies. Clinical Relevance– Our approach can assist with the development and validation of clinical time-series classifiers.},
  keywords={Deep learning;Sleep;Perturbation methods;Neural networks;Brain modeling;Information filters;Feature extraction},
  doi={10.1109/EMBC48229.2022.9871414},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10309706,
  author={Hostetter, John Wesley and Chi, Min},
  booktitle={2023 IEEE International Conference on Fuzzy Systems (FUZZ)}, 
  title={Latent Space Encoding for Interpretable Fuzzy Logic Rules in Continuous and Noisy High-Dimensional Spaces}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={This study introduces a general approach for generating fuzzy logic rules in regression tasks with complex, high-dimensional input spaces. The method leverages the power of encoding data into a latent space, where its uniqueness is analyzed to determine whether it merits the distinction of becoming a noteworthy exemplar. The efficacy of the proposed method is showcased through its application in predicting the acceleration of one of the links for the Unimation Puma 560 robot arm, effectively overcoming the challenges posed by non-linearity and noise in the dataset.},
  keywords={Fuzzy logic;Manipulators;Encoding;Noise measurement;Task analysis;Fuzzy systems},
  doi={10.1109/FUZZ52849.2023.10309706},
  ISSN={1558-4739},
  month={Aug},}@ARTICLE{11036611,
  author={Yin, Min and Ma, Xin and Wang, Youqing},
  journal={IEEE Sensors Journal}, 
  title={Semi-Supervised Multitask Learning Approach Boosted by Operation Strategy Expert System for Industrial Process Fault Diagnosis}, 
  year={2025},
  volume={25},
  number={14},
  pages={27250-27264},
  abstract={Fault diagnosis (FD) is crucial for keeping industrial processes safe. Although there is a wealth of historical running data stored, labeled fault data is insufficient. Semi-supervised learning mechanisms can fully utilize unlabeled data to enhance model generalization and performance. Expert systems (ESs) store rich expert knowledge and experience, providing additional decision information and experiential label support for neural networks (NNs). This study proposes a semi-supervised learning approach that combines ESs with NNs for industrial process FD. This approach adopts a multitask learning framework, wherein classification and meta-feature learning are the main and auxiliary tasks, respectively. A rule-based ES is constructed using a mixed integer linear programming (MILP) method. ESs make decisions based on meta-feature learning results, and during the backpropagation process, these decisions are used alongside predictions of the main task for parameter updates. The research findings suggest that the proposed approach has several advantages, including: 1) it enhances the model’s diagnostic performance, interpretability, and interactability; 2) it lessens the demand for labeled samples in the NN; and 3) it provides visual explanations for decision results. This study was validated on a simulated three-tank water storage platform and a gasification process case in a certain factory, achieving satisfactory results.},
  keywords={Mathematical models;Linear programming;Fault diagnosis;Particle swarm optimization;Expert systems;Training;Semisupervised learning;Production facilities;Knowledge engineering;Intelligent sensors;Expert systems (ESs);fault diagnosis (FD);industrial processes;multitask learning;semi-supervised learning},
  doi={10.1109/JSEN.2025.3577708},
  ISSN={1558-1748},
  month={July},}@ARTICLE{9426591,
  author={Wang, Yunhe and Li, Xiangtao and Wong, Ka-Chun and Chang, Yi and Yang, Shengxiang},
  journal={IEEE Transactions on Cybernetics}, 
  title={Evolutionary Multiobjective Clustering Algorithms With Ensemble for Patient Stratification}, 
  year={2022},
  volume={52},
  number={10},
  pages={11027-11040},
  abstract={Patient stratification has been studied widely to tackle subtype diagnosis problems for effective treatment. Due to the dimensionality curse and poor interpretability of data, there is always a long-lasting challenge in constructing a stratification model with high diagnostic ability and good generalization. To address these problems, this article proposes two novel evolutionary multiobjective clustering algorithms with ensemble (NSGA-II-ECFE and MOEA/D-ECFE) with four cluster validity indices used as the objective functions. First, an effective ensemble construction method is developed to enrich the ensemble diversity. After that, an ensemble clustering fitness evaluation (ECFE) method is proposed to evaluate the ensembles by measuring the consensus clustering under those four objective functions. To generate the consensus clustering, ECFE exploits the hybrid co-association matrix from the ensembles and then dynamically selects the suitable clustering algorithm on that matrix. Multiple experiments have been conducted to demonstrate the effectiveness of the proposed algorithm in comparison with seven clustering algorithms, twelve ensemble clustering approaches, and two multiobjective clustering algorithms on 55 synthetic datasets and 35 real patient stratification datasets. The experimental results demonstrate the competitive edges of the proposed algorithms over those compared methods. Furthermore, the proposed algorithm is applied to extend its advantages by identifying cancer subtypes from five cancer-related single-cell RNA-seq datasets.},
  keywords={Clustering algorithms;Linear programming;Optimization;Clustering methods;Cancer;Urban areas;Heuristic algorithms;Ensemble clustering;multiobjective optimization (MOO);patient stratification},
  doi={10.1109/TCYB.2021.3069434},
  ISSN={2168-2275},
  month={Oct},}@ARTICLE{10056837,
  author={Zhong, Shanlin and Wu, Wei},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Motion Learning and Generalization of Musculoskeletal Robot Using Gain Primitives}, 
  year={2024},
  volume={21},
  number={2},
  pages={1580-1591},
  abstract={Organisms have an innate ability to rapidly produce diverse and flexible movement. Biological motor systems are composed of highly redundant muscle actuators and have strongly nonlinear dynamic characteristics. How organisms cope with such complex system is still an open question. Motor primitive theory proposed by neuroscientists has provided a convincing explanation for this extraordinary ability and have supported by several biological evidence. In this paper, based on the gain primitive model of cortical network proposed by neuroscientists, new algorithms for learning and combining gain primitives were designed to control musculoskeletal and robotic system with highly redundant actuators. It can realize dimensionality reduction of controll from the number of actuators to the number of primitives. A parameter adaptation algorithm inspired by monoamines modulation mechanism was applied to improve the learning efficiency of gain primitive. Learned motion experiences and primitives were introduced to effectively generate new movements. Validation experiments were carried out on a musculoskeletal model with 9 muscles and an articulated robot Baxter with redundant actuators. The experimental results demonstrated that the recurrent neural network modulated by gain primitives can generate high-dimensional control signals and realize efficient motion generalization for practical systems. Note to Practitioners—This paper applied a biologically plausible control method with gain primitive to practical systems with highly redundant actuators. To reduce the cumulative error occurring in learning gain primitive for control networks with high dimensional output, a bioinspired Hebbian learning rule was applied to train a recurrent neural network for optimizing the gain primitive. During the training process, systematic entropy was introduced as an index to adaptively adjust learning parameters, so as to reduce the exploration cost and improve the learning stability of the system. To realize easy implementation and generalization, prior motion experiences were introduced into the initialization of the optimization algorithm, and motion generalization was realized by using the particle swarm optimization algorithm. The algorithms designed in this paper are applicable to complex systems with highly redundant actuators such as humanoid robots.},
  keywords={Robots;Actuators;Muscles;Heuristic algorithms;Organisms;Dynamics;Recurrent neural networks;Musculoskeletal system;Musculoskeletal robot;robotic control;motor primitive;bio-inspired algorithm},
  doi={10.1109/TASE.2023.3249228},
  ISSN={1558-3783},
  month={April},}@INPROCEEDINGS{9931831,
  author={Inapakurthi, Ravi kiran and Mitra, Kishalay},
  booktitle={2022 26th International Conference on System Theory, Control and Computing (ICSTCC)}, 
  title={System Identification and Process Modelling of Dynamic Systems Using Machine Learning}, 
  year={2022},
  volume={},
  number={},
  pages={564-569},
  abstract={Nonlinear system identification of complex and nonlinear unit operations and unit processes requires accurate modelling approaches. For this, first-principles based models were initially explored as they enable the causal explanation available among variables. However, the numerical integration issues along with the availability of voluminous data for developing data-based models has resulted in the shift from the conventional modelling approach to Machine Learning (ML) based modelling. In this study, Support Vector Regression (SVR) is used to model complex Industrial Grinding Circuit (IGC). To aid the accurate model requirement in process systems engineering domain, the tunable parameters of SVR are optimized using a novel multi-objective optimization formulation, which helps in minimizing the chances of over-fitting while simultaneously ensuring accurate models for IGC. The formulation is optimized using evolutionary algorithm to track and retain the most accurate models. The Pareto optimal SVR models have a minimum accuracy of 99. 786% and the prediction performance of the best model selected using knee point from the Pareto optimal set is compared with a model selected using arbitrary approach to show the competitiveness of the proposed technique.},
  keywords={Computational modeling;Time series analysis;Predictive models;Pareto optimization;Data models;Numerical models;Modeling;system identification;process modelling;machine learning;multi-objective optimization;evolutionary algorithms},
  doi={10.1109/ICSTCC55426.2022.9931831},
  ISSN={2372-1618},
  month={Oct},}@ARTICLE{10559784,
  author={Wang, Huanan and Zhang, Xinyu and Wang, Hong and Jun, Li},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Si-GAIS: Siamese Generalizable-Attention Instance Segmentation for Intersection Perception System}, 
  year={2024},
  volume={25},
  number={11},
  pages={15759-15774},
  abstract={Instance segmentation of traffic participants using vision-based techniques serves as a cornerstone for numerous intelligent transportation systems. Although existing deep learning-based methods have made significant advancements in this field, these algorithms still present considerable challenges with regard to generalizability for commercial deployment. Specifically, the mean average perception (mAP) of these algorithms degrades rapidly when dealing with intersections that are not included in the training set. To address this limitation, a novel instance segmentation approach named Si-GAIS is proposed, which incorporates a siamese structure for the first time with the proposed Generalizable-Attention Encoder (GA). Through the proposed Foreground-Background Fusion Unit (FBF) within GA, efficient feature-level fusion for the foreground and background images is achieved. Additionally, the Interpretable Attention Neck (IA) in GA enables the feature encoder to focus exclusively on the foreground traffic participants while ignoring various backgrounds. To utilize Si-GAIS, an unsupervised method named P-DBSCAN is proposed to obtain high-quality background image for each intersection with slow-moving traffic and camera jitters. Finally, the first multi-intersection multi-category instance segmentation datasets named RopeIns is proposed for validation. Si-GAIS achieves a 7.7% mAP (All APs used in this paper are abbreviations of AP $_{\textit {50}}$  the same with PASCAL VOC.) accuracy improvement compared to the state-of-the-art (SOTA) methods while using fewer parameters, with only a 6.4% decline in AP for car segmentation in unseen intersections and weather conditions, whereas all other SOTA methods decline more than 10%. The proposed dataset and source code are publicly available at https://github.com/441599828/SiGAIS and we hope Si-GAIS will be a new baseline for IPS instance segmentation research.},
  keywords={Instance segmentation;Cameras;IP networks;Accuracy;Feature extraction;Laser radar;Artificial intelligence;Attention mechanisms;Intelligent manufacturing systems;Traffic control;Artificial intelligence;attention mechanism;instance segmentation;intelligent transportation systems;intersection perception dataset},
  doi={10.1109/TITS.2024.3411647},
  ISSN={1558-0016},
  month={Nov},}@ARTICLE{10839311,
  author={Chen, Xin and Ren, Xueqi and Jiao, Libo and Dai, Xin and Dong, Zhe and Min, Geyong},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={QoE-aware Multicast-Unicast Transmission Scheme For Multi-UAV Assisted Virtual Reality Live Streaming Systems}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={The growing complexity of the sixth-generation (6G) wireless networks has positioned artificial intelligence (AI) as a critical tool for radio resource management (RRM). However, the opacity of AI learning models undermines their trustworthiness, robustness, and interpretability, significantly impeding their application in network resource management for 6G scenarios. Establishing reliable edge intelligence (EI) is crucial for the future of various domains, particularly in virtual reality (VR). By leveraging EI, VR applications can better meet user demands, enhancing service experiences and reliability. The 360-degree video characteristics challenge existing VR transmission networks. We propose a unmanned aerial vehicle (UAV)-assisted joint multicast-unicast system to optimize user QoE by jointly optimizing UAV trajectories, user associations, quality requests, user pairing, and power allocation. To address this optimization problem, we first decompose it into inter-frame and intra-frame components, and propose a twin delayed deep deterministic policy gradient (TD3)-based algorithm to tackle the inter-frame problem. After solving the inter-frame problem, we utilize the quantum particle swarm optimization (QPSO) algorithm and matching theory to address the intra-frame problem. Simulation results demonstrate that our proposed algorithm, significantly improves the QoE for users.},
  keywords={Streaming media;Autonomous aerial vehicles;Quality of experience;Unicast;Resource management;Multicast algorithms;Artificial intelligence;Wireless networks;Spaceborne radar;Rendering (computer graphics);Virtual reality streaming;unmanned aerial vehicle;joint multicast-unicast transmission network;quality of experience},
  doi={10.1109/TCE.2025.3528928},
  ISSN={1558-4127},
  month={},}@INPROCEEDINGS{11059490,
  author={Sudhakara, Sushmitha Halli and Shirajum Munir, Md and Rahman, Mostafizur and Shetty, Sachin},
  booktitle={2025 International Wireless Communications and Mobile Computing (IWCMC)}, 
  title={Native AI-based Predictive Operational Resiliency in Cyber-Physical Energy Systems}, 
  year={2025},
  volume={},
  number={},
  pages={1258-1263},
  abstract={Operational resiliency becomes a key requirement to prevent critical recall of distributed energy resources (DER) in Cyber-Physical Energy Systems (CPES). Thus, CPES requires methods and metrics to observe and understand the potential faults of unwanted cyber-physical events. This paper introduces a new Native-AI-driven predictive maintenance framework to ensure the operational resiliency of CPES. First, a system model is designed to capture operational states such as normal, service mode, and faults by observing the operational behavior of DERs. Second, a predictive maintenance framework is proposed and developed by introducing a dual-method feature selection mechanism while the features of critical recall states are selected by tailored Gini and permutation-based importance metrics. Third, the predictive model is designed by leveraging tree-based models that utilize cyclical temporal encoding and lag features, while a Long Short-Term Memory (LSTM) family models employ sequence learning with normalized temporal and angular representations. Finally, the proposed framework can achieve up to 99.97% Critical Recall detection accuracy with an average of 82.5%. As a result, this work advances AI-driven predictive maintenance by reducing downtime by 15–30% in simulated scenarios.},
  keywords={Measurement;Deep learning;Wireless communication;Accuracy;Computational modeling;Predictive models;Distributed power generation;Long short term memory;Resilience;Predictive maintenance;operational resiliency;native AI;cyber-physical energy system;critical recall;predictive maintenance},
  doi={10.1109/IWCMC65282.2025.11059490},
  ISSN={2376-6506},
  month={May},}@ARTICLE{11016187,
  author={Xue, Chuan and Gao, Jianli and Gu, Zhou},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={IT2-ENFIS: Interval Type-2 Exclusionary Neuro-Fuzzy Inference System, An Attempt Towards Trustworthy Regression Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={As machine learning technologies progress and are increasingly applied to critical and sensitive fields, the reliability issues of earlier technologies are becoming more evident. For the new generation of machine learning solutions, trustworthiness frequently takes precedence over performance when evaluating their applicability for specific applications. This manuscript introduces the IT2-ENFIS neuro-fuzzy model, a robust and trustworthy single-network solution specifically designed for data regression tasks affected by substantial label noise and outliers. The primary architecture applies interval type-2 fuzzy logic and the Sugeno inference engine. A meta-heuristic gradient-based optimizer (GBO), the Huber loss function, and the Cauchy M-estimator are employed for robust learning. IT2-ENFIS demonstrates superior performance on noise-contaminated datasets and excels in real-world scenarios, with excellent generalization capability and interpretability.},
  keywords={Fuzzy logic;Artificial intelligence;Uncertainty;Machine learning;Training;Fuzzy sets;Robustness;Predictive models;Roads;Prediction algorithms;Neuro-fuzzy systems;Interval type-2 inference systems;Data-driven modeling;Trustworthy machine learning},
  doi={10.1109/TAI.2025.3574299},
  ISSN={2691-4581},
  month={},}@ARTICLE{10918987,
  author={Hu, Yi and Chen, Fan and Zhu, Bo and Jiang Liu, Jin and Bing Cheng, Wei and Qing Zhang, Zhong and Su, Shijie},
  journal={IEEE Access}, 
  title={Forward Kinematics for Parallel Platforms Based on the Integration of Residual Networks With Self-Attention and Error Compensation}, 
  year={2025},
  volume={13},
  number={},
  pages={60200-60212},
  abstract={Parallel platforms are widely used in modern industry due to their superior dynamic and static properties. However, different parallel platforms have different closure structures, and their forward kinematics equations have high nonlinearity, leading to poor generalization, low efficiency, and precision in solving these equations. This study presents a novel RNSA-EC algorithm designed to enhance the computational precision and efficiency of forward kinematics for parallel platforms. The algorithm integrates a Self-Attention optimized Residual Network (RNSA) with an Error Compensation (EC) method. The RNSA model initially achieves high-precision nonlinear mapping from joint coordinates to pose coordinates of parallel platforms. Subsequently, the inverse kinematic model is employed to develop an EC method, which further reduces computational errors in the forward kinematics of the parallel platform by iteratively updating the inputs to the RNSA model. We applied the RNSA-EC algorithm to solve the forward kinematics of two 6-degree-of-freedom (6-DOF) parallel platforms with distinct structures. The results demonstrate that the algorithm is approximately 60%, 50%, and 45% more efficient than the Newton-Raphson (NR), the hybrid Backpropagation Neural Network (BPNN)-NR, and the Iterative Artificial Neural Network (IANN) methods, respectively, on the two parallel platforms. Furthermore, the algorithm can be extended to solve forward kinematics for parallel platforms with different structures and DOFs.},
  keywords={Kinematics;Residual neural networks;Computational efficiency;Vectors;Jacobian matrices;Computational modeling;Approximation algorithms;Silicon;Error compensation;Training;Deep learning;forward kinematics solution;residual neural network;parallel mechanism},
  doi={10.1109/ACCESS.2025.3549796},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9995118,
  author={Tan, Haojiang and Wang, Jun and Yu, Guoxian and Guo, Wei and Guo, Maozu},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Phenotype Prediction by Heterogeneous Molecular Network Embedding}, 
  year={2022},
  volume={},
  number={},
  pages={116-121},
  abstract={Phenotype prediction aims to infer the traits of living organisms based on genomics data, which has important applications in biology such as cancer subtype diagnosis and crop breedings. Traditional phenotype prediction approaches only learn the low-dimensional representation of samples, but ignore the interaction between biomolecules and cannot use the topology structure of heterogeneous molecular networks. Furthermore, most of them lack interpretability and do not effectively identify key biomolecules associated with phenotypes. In this paper, we propose a heterogeneous network embedding based solution (PhenoHNE) to predict phenotype by fusing topology information of molecules. PhenoHNE firstly utilizes variational graph autoencode (VGAE) to obtain the embedding representation of molecules in the heterogeneous network. Secondly, PhenoHNE adopts multilayer perceptron (MLP) with attention mechanism to learn sample representation. Finally, it fuses molecular embedding representation and sample representation to predict the phenotype of samples. In this way, the genetics information of heterogeneous molecular network can further guide the feature learning of samples and improve the prediction performance. Experimental results on human and maize datasets confirm that PhenoHNE outperforms competitive methods by a large margin under different evaluation protocols, and it also can effectively identify the key molecules associated with phenotypes of interests.},
  keywords={Representation learning;Protocols;Network topology;Fuses;Molecular biophysics;Heterogeneous networks;Organisms;Phenotype Prediction;Heterogeneous Molecular Network;Multi-Omics Data Fusion;Variational Graph AutoEncode;Multilayer Perceptron},
  doi={10.1109/BIBM55620.2022.9995118},
  ISSN={},
  month={Dec},}@ARTICLE{11068985,
  author={Laursen, Mikal and Huynh, Van-Van and Ha, Duy-Hung and Mahmoud, Mahmoud S. and Khang Huynh, van},
  journal={IEEE Access}, 
  title={Robust Fault Classification in Permanent Magnet Synchronous Machines Under Dynamic and Noisy Conditions}, 
  year={2025},
  volume={13},
  number={},
  pages={115217-115225},
  abstract={Detection and isolation of multiple low-severity faults in permanent magnet synchronous machines (PMSMs) under dynamics and noisy conditions are very important to enhance the system’s reliability, lifetime, and service availability. This study investigates a robust fault classification scheme for PMSMs under low-severity, multiple-fault conditions in noisy and variable-speed scenarios. Two supervised learning models, namely Extra Trees (ET) and Support Vector Machine (SVM), are developed and compared using time-domain (TD) and frequency-domain (FD) features. A hybrid feature extraction method is adopted to balance the computational burden and classification accuracy. Hyperparameters and frequency-domain (FD) features are optimized based on prediction confidence, prediction times, and accuracy. Furthermore, the robustness of the data-driven models is evaluated at different Signal-to-Noise (SNR) ratios under mixed faults and variable speeds in an in-house PMSM, combining an inter-turn short circuit fault of 2.2% in one phase and a 30% demagnetization of one rotor magnet. A comparative study was conducted, confirming the proposed ET model’s superior performance in terms of prediction accuracy and robustness.},
  keywords={Circuit faults;Computational modeling;Support vector machines;Accuracy;Feature extraction;Robustness;Frequency-domain analysis;Kernel;Integrated circuit modeling;Impurities;PMSM;multi-fault classification;fault classification;Extra Trees;Support Vector Machine;inter-turn short circuit (ITSC);demagnetization;frequency-domain features;model robustness},
  doi={10.1109/ACCESS.2025.3585518},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11076360,
  author={D, Shiny Irene and K, Anitha and Rayidi, Sriram and G, Maruthi Jaswanth Reddy and J, Selvin Paul Peter},
  booktitle={2025 7th International Conference on Intelligent Sustainable Systems (ICISS)}, 
  title={Advanced Deep Learning Models for Chest X-ray Disease Detection: EHRT-RWB Algorithm Coupled with CNN-based Vision Transformer}, 
  year={2025},
  volume={},
  number={},
  pages={1653-1659},
  abstract={Accurate and early disease detection from chest X-ray (CX-ray) images is crucial for effective clinical decision-making. This study proposes a novel Ensemble Hybrid Recurrent Transformer-Based Random Wolf Bird (EHRTRWB) algorithm, integrating CNN-based Vision Transformers (ViTs) and metaheuristic optimization for multimodal disease prediction. The proposed approach leverages CNN-based ViTs for extracting local and global image features, while Bi-LSTM and Transformer self-attention mechanisms process sequential dependencies in structured medical data. To enhance model robustness and feature selection, we introduce the Random Wolf Bird (RWB) optimization, inspired by the hunting and migration behavior of wolves and birds, dynamically fine-tuning hyperparameters and ensemble weighting. The ensemble learning framework fuses predictions from deep learning and machine learning classifiers, ensuring high accuracy, resilience to noise, and interpretability. Experimental evaluations on benchmark CX-ray datasets demonstrate that EHRT-RWB outperforms existing models in accuracy, sensitivity, and specificity, providing a reliable AI-driven solution for disease risk prediction. The proposed method offers a novel approach to multimodal fusion, optimizing deep learning with nature-inspired intelligence for improved medical diagnostics.},
  keywords={Deep learning;Computer vision;Accuracy;Predictive models;Transformers;Feature extraction;Prediction algorithms;Birds;X-ray imaging;Diseases;Vision Transformer;CX-ray;RWB},
  doi={10.1109/ICISS63372.2025.11076360},
  ISSN={},
  month={March},}@INPROCEEDINGS{10071081,
  author={Dass, Jyotikrishna and Wu, Shang and Shi, Huihong and Li, Chaojian and Ye, Zhifan and Wang, Zhongfeng and Lin, Yingyan},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention}, 
  year={2023},
  volume={},
  number={},
  pages={415-428},
  abstract={Vision Transformer (ViT) has emerged as a competitive alternative to convolutional neural networks for various computer vision applications. Specifically, ViTs’ multi-head attention layers make it possible to embed information globally across the overall image. Nevertheless, computing and storing such attention matrices incurs a quadratic cost dependency on the number of patches, limiting its achievable efficiency and scalability and prohibiting more extensive real-world ViT applications on resource-constrained devices. Sparse attention has been shown to be a promising direction for improving hardware acceleration efficiency for NLP models. However, a systematic counterpart approach is still missing for accelerating ViT models. To close the above gap, we propose a first-of-its-kind algorithm-hardware codesigned framework, dubbed VITALITY, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, VITALITY unifies both low-rank and sparse components of the attention in ViTs. At the algorithm level, we approximate the dot-product softmax operation via first-order Taylor attention with row-mean centering as the low-rank component to linearize the cost of attention blocks and further boost the accuracy by incorporating a sparsity-based regularization. At the hardware level, we develop a dedicated accelerator to better leverage the resulting workload and pipeline from VITALITY’s linear Taylor attention which requires the execution of only the low-rank component, to further boost the hardware efficiency. Extensive experiments and ablation studies validate that VITALITY offers boosted end-to-end efficiency (e.g., 3× faster and 3× energy-efficient) under comparable accuracy, with respect to the state-of-the-art solution. We make the codes available on https://github.com/GATECH-EIC/ViTaLiTy},
  keywords={Training;Costs;Systematics;Approximation algorithms;Transformers;Boosting;Sparse representation},
  doi={10.1109/HPCA56546.2023.10071081},
  ISSN={2378-203X},
  month={Feb},}@ARTICLE{9495171,
  author={Xie, Zhongwei and Liu, Ling and Wu, Yanzhao and Li, Lin and Zhong, Luo},
  journal={IEEE Transactions on Services Computing}, 
  title={Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service}, 
  year={2022},
  volume={15},
  number={6},
  pages={3304-3316},
  abstract={It is widely acknowledged that learning joint embeddings of recipes with images is challenging due to the diverse composition and deformation of ingredients in cooking procedures. We present a Multi-modal Semantics enhanced Joint Embedding approach (MSJE) for learning a common feature space between the two modalities (text and image), with the ultimate goal of providing high-performance cross-modal retrieval services. Our MSJE approach has three unique features. First, we extract the TFIDF feature from the title, ingredients and cooking instructions of recipes. By determining the significance of word sequences through combining LSTM learned features with their TFIDF features, we encode a recipe into a TFIDF weighted vector for capturing significant key terms and how such key terms are used in the corresponding cooking instructions. Second, we combine the recipe TFIDF feature with the recipe sequence feature extracted through two-stage LSTM networks, which is effective in capturing the unique relationship between a recipe and its associated image(s). Third, we further incorporate TFIDF enhanced category semantics to improve the mapping of image modality and to regulate the similarity loss function during the iterative learning of cross-modal joint embedding. Experiments on the benchmark dataset Recipe1M show the proposed approach outperforms the state-of-the-art approaches.},
  keywords={Semantics;Dairy products;Feature extraction;Sugar;Powders;Optimization;Task analysis;Cross-modal retrieval service;deep learning;joint embedding learning;TF-IDF;multi-modal semantics},
  doi={10.1109/TSC.2021.3098834},
  ISSN={1939-1374},
  month={Nov},}@ARTICLE{10640100,
  author={Wang, Xiaojie and Wang, Beibei and Wu, Yu and Ning, Zhaolong and Guo, Song and Yu, Fei Richard},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={A Survey on Trustworthy Edge Intelligence: From Security and Reliability to Transparency and Sustainability}, 
  year={2025},
  volume={27},
  number={3},
  pages={1729-1757},
  abstract={Edge Intelligence (EI) integrates Edge Computing (EC) and Artificial Intelligence (AI) to push the capabilities of AI to the network edge for real-time, efficient and secure intelligent decision-making and computation. However, EI faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications, which together affect the trustworthiness of EI in the eyes of stakeholders. This survey comprehensively summarizes the characteristics, architecture, technologies, and solutions of trustworthy EI. Specifically, we first emphasize the need for trustworthy EI in the context of the trend toward large models. We then provide an initial definition of trustworthy EI, explore its key characteristics and give a multi-layered architecture for trustworthy EI. Then, we present enabling technologies for trustworthy EI systems. Subsequently, we summarize several important issues that hinder the achievement of trustworthy EI and provide an in-depth literature review of the state-of-the-art solutions for realizing the trustworthiness of EI. Finally, we discuss the corresponding research challenges and open issues.},
  keywords={Artificial intelligence;Security;Surveys;Privacy;6G mobile communication;Computational modeling;Quality of service;Trustworthiness;edge computing;artificial intelligence;limited resources;interpretability},
  doi={10.1109/COMST.2024.3446585},
  ISSN={1553-877X},
  month={June},}@ARTICLE{10479078,
  author={Tong, Haonan and Zhang, Dalin and Liu, Jiqiang and Xing, Weiwei and Lu, Lingyun and Lu, Wei and Wu, Yumei},
  journal={IEEE Transactions on Software Engineering}, 
  title={MASTER: Multi-Source Transfer Weighted Ensemble Learning for Multiple Sources Cross-Project Defect Prediction}, 
  year={2024},
  volume={50},
  number={5},
  pages={1281-1305},
  abstract={Multi-source cross-project defect prediction (MSCPDP) attempts to transfer defect knowledge learned from multiple source projects to the target project. MSCPDP has drawn increasing attention from academic and industry communities owing to its advantages compared with single-source cross-project defect prediction (SSCPDP). However, two main problems, which are how to effectively extract the transferable knowledge from each source dataset and how to measure the amount of knowledge transferred from each source dataset to the target dataset, seriously restrict the performance of existing MSCPDP models. In this paper, we propose a novel multi-source transfer weighted ensemble learning (MASTER) method for MSCPDP. MASTER measures the weight of each source dataset based on feature importance and distribution difference and then extracts the transferable knowledge based on the proposed feature-weighted transfer learning algorithm. Experiments are performed on 30 software projects. We compare MASTER with the latest state-of-the-art MSCPDP methods with statistical test in terms of famous effort-unaware measures (i.e., PD, PF, AUC, and MCC) and two widely used effort-aware measures ($P_{opt}20\%$Popt20% and IFA). The experiment results show that: 1) MASTER can substantially improve the prediction performance compared with the baselines, e.g., an improvement of at least 49.1% in MCC, 48.1% in IFA; 2) MASTER significantly outperforms each baseline on most datasets in terms of AUC, MCC, $P_{opt}20\%$Popt20% and IFA; 3) MSCPDP model significantly performs better than the mean case of SSCPDP model on most datasets and even outperforms the best case of SSCPDP on some datasets. It can be concluded that 1) it is very necessary to conduct MSCPDP, and 2) the proposed MASTER is a more promising alternative for MSCPDP.},
  keywords={Predictive models;Training;Weight measurement;Feature extraction;Software;Genetic algorithms;Ensemble learning;Multiple source datasets;cross-project defect prediction;software defect proneness;feature weighting;transfer learning},
  doi={10.1109/TSE.2024.3381235},
  ISSN={1939-3520},
  month={May},}@INPROCEEDINGS{10174239,
  author={Ma, Kwondo and Amarnath, Chandramouli and Chatterjee, Abhijit},
  booktitle={2023 IEEE European Test Symposium (ETS)}, 
  title={Error Resilient Transformers: A Novel Soft Error Vulnerability Guided Approach to Error Checking and Suppression}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Transformer networks have achieved remarkable success in Natural Language Processing (NLP) and Computer Vision applications. However, the underlying large volumes of Transformer computations demand high reliability and resilience to soft errors in processor hardware. The objective of this research is to develop efficient techniques for design of error resilient Transformer architectures. To enable this, we first perform a soft error vulnerability analysis of every fully connected layers in Transformer computations. Based on this study, error detection and suppression modules are selectively introduced into datapaths to restore Transformer performance under anticipated error rate conditions. Memory access errors and neuron output errors are detected using checksums of linear Transformer computations. Correction consists of determining output neurons with out-of-range values and suppressing the same to zero. For a Transformer with nominal BLEU score of 52.7, such vulnerability guided selective error suppression can recover language translation performance from a BLEU score of 0 to 50.774 with as much as 0.001 probability of activation error, incurring negligible memory and computation overheads.},
  keywords={Neurons;Memory management;Europe;Transformers;Natural language processing;Hardware;Reliability;Transformer;error resilience},
  doi={10.1109/ETS56758.2023.10174239},
  ISSN={1558-1780},
  month={May},}@INPROCEEDINGS{10475663,
  author={Boyapati, Mallika and Aygun, Ramazan},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)}, 
  title={Semanformer: Semantics-aware Embedding Dimensionality Reduction Using Transformer-Based Models}, 
  year={2024},
  volume={},
  number={},
  pages={134-141},
  abstract={In recent years, transformer-based models, particularly BERT (Bidirectional encoder Representations from Transformers), have revolutionized natural language processing tasks, achieving state-of-the-art performance in various domains. In the context of natural language processing (NLP) and linguistics, understanding the semantic aspects of text is crucial for tasks like information retrieval, sentiment analysis, machine translation, and many others. However, the high dimensionality of BERT embeddings presents challenges in real-world applications due to increased memory and computational requirements. Reducing the dimensionality of BERT embeddings would benefit many application downstream tasks by reducing the computational requirements. Although there are prevalently used dimensionality reduction methods which focus on feature representation with lower dimensions, their application on NLP tasks may not yield semantically correct results. We propose a novel framework named as semanformer (semantics-aware encoder-decoder dimensionality reduction method) that leverages transformer-based encoder-decoder model architecture to perform dimensionality reduction on BERT embeddings for a corpus while preserving crucial semantic information. To evaluate the effectiveness of our approach, we conduct a comprehensive use case evaluation on diverse text datasets by sentence reconstruction. Our experiments show that our proposed method achieves high sentence reconstruction accuracy (SRA) more than 83% compared to the traditional dimensionality reduction methods such as PCA (SRA < 66%) and t-SNE (SRA < 9%).},
  keywords={Dimensionality reduction;Sentiment analysis;Semantics;Memory management;Bidirectional control;Transformers;Encoding;BERT;Transformer;Dimensionality reduction;Sentence reconstruction;Embedding reconstruction},
  doi={10.1109/ICSC59802.2024.00027},
  ISSN={2472-9671},
  month={Feb},}@ARTICLE{10056320,
  author={Zhu, Hanqing and Wilson, Sean and Feron, Eric},
  journal={IEEE Transactions on Robotics}, 
  title={The Design, Education and Evolution of a Robotic Baby}, 
  year={2023},
  volume={39},
  number={3},
  pages={2488-2507},
  abstract={Inspired by Alan Turing's idea of a child machine, in this article, we introduce the formal definition of a robotic baby, an integrated system with minimal world knowledge at birth, capable of learning incrementally and interactively, and adapting to the world. Within the definition, fundamental capabilities and system characteristics of the robotic baby are identified and presented as the system-level requirements. As a minimal viable prototype, the Baby architecture is proposed with a systems engineering design approach to satisfy the system-level requirements, which has been verified and validated with simulations and experiments on a robotic system. We demonstrate the capabilities of the robotic baby in natural language acquisition and semantic parsing in English and Chinese, as well as in natural language grounding, natural language reinforcement learning, natural language programming, and system introspection for explainability. The education and evolution of the robotic baby are illustrated with real-world robotic demonstrations. Inspired by the genetic inheritance in human beings, knowledge inheritance in robotic babies and its benefits regarding evolution are discussed.},
  keywords={Pediatrics;Robots;Natural languages;Semantics;Programming profession;Robot sensing systems;Context modeling;Evolutionary robotics;explainable artificial intelligence (AI);learning and adaptive systems;natural language programming;robotic architecture;robotic baby;semantic representation},
  doi={10.1109/TRO.2023.3240619},
  ISSN={1941-0468},
  month={June},}@INPROCEEDINGS{10849393,
  author={Liu, Xinyun and Xiao, Pengcheng and Esposito, Michele and Raavi, Manohar and Zhao, Chen},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={AGFA-Net: Attention-Guided Feature-Aggregated Network for Coronary Artery Segmentation Using Computed Tomography Angiography}, 
  year={2024},
  volume={},
  number={},
  pages={327-334},
  abstract={coronary artery disease (CAD) remains a prevalent cardiovascular condition, posing significant health risks worldwide. Accurate segmentation of coronary arteries from coronary computed tomography angiography (CCTA) images is crucial for diagnosis and treatment planning. Despite advancements, coronary artery segmentation (CAS) remains a significant challenge due to the considerable variability in the scale of coronary arteries, the complex anatomical structure and morphology, and the low contrast between blood vessels and the surrounding tissue. To effectively tackle these issues, we propose an attention-guided feature-aggregated network (AGFA-Net) specifically designed for CAS using CCTA images. AGFA-Net leverages attention mechanisms, feature refinement and hierarchical feature aggregation to capture salient features, acquire a more distinctive semantic representation and enhance segmentation accuracy. Evaluation on a dataset comprising 1,000 CCTAs demonstrates AGFA-Net's superior performance, achieving an average Dice coefficient similarity of 86.74 % and a 95% Hausdorff distance of 0.82 mm during 5-fold cross-validation. Ablation studies further validate the effectiveness of the proposed modules, highlighting their contributions to improved segmentation accuracy. Overall, AGFA-Net offers a robust and reliable solution for CAS, addressing challenges posed by varying arterial sizes, complex anatomies, and low image contrast.},
  keywords={Image segmentation;Accuracy;Attention mechanisms;Computed tomography;Angiography;Semantics;Morphology;Planning;Reliability;Arteries;CCTA;Image Segmentation;Attention Mechanisms;Feature Aggregation},
  doi={10.1109/ICTAI62512.2024.00055},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10690672,
  author={Saghiri, Ali Mohammad and Wang, Nan},
  booktitle={2024 International Conference on Computing, Internet of Things and Microwave Systems (ICCIMS)}, 
  title={Self-Evolving Programs: A Novel Approach Leveraging LLMs and Quine Programs}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={In an era where software systems undergo rapid evolution, the demand for self-evolving programs has received much attention. This paper introduces a groundbreaking methodology that amalgamates the predictive power of Large Language Model-Based Methods with the self-replicating nature of Quine Programs to organize self-evolving software. This innovative integration not only facilitates dynamic code adaptation to fluctuating runtime conditions but also pioneers a shift from traditional static coding paradigms. Through this approach, we aim to revolutionize software development, particularly enhancing security and performance with a focus on countering selfish mining attacks in Bitcoin. It should be noted that the suggested approach has not yet been considered in the field of software engineering, and its applications are not limited to software security. It has high potential to be used in every domain that requires adaptive programs. Since the suggested approach is novel, the proposed solution in Bitcoin is also novel. The results of simulation show the efficiency of the proposed solution.},
  keywords={Microwave integrated circuits;Codes;Runtime;Bitcoin;Predictive models;Software systems;Microwave theory and techniques;Security;Microwave FET integrated circuits;Software development management;Self-Evolving Programs;Language Model-based Methods;Quine Programs;Dynamic Code Optimization;Selfish Mining Defense},
  doi={10.1109/ICCIMS61672.2024.10690672},
  ISSN={},
  month={July},}@ARTICLE{11048407,
  author={Li, Bowen and Lv, Yong and Wu, Hongan and Yuan, Rui and Li, Zhun and Ilbeigi, Shahab and Gedikli, Ersegun Deniz},
  journal={IEEE Sensors Journal}, 
  title={Adaptive Extracting Wavelet Transform-based Damage Indicator-Integrated Improved GPR Model for Bearing RUL Prediction}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={The bearing is one of the most essential parts of rotating machinery and equipment, and it is of great engineering significance in the accurate prediction of the remaining useful life (RUL) of the bearing. Aiming at this problem, a novel hybrid prediction strategy based on time-frequency representation (TFR) is proposed in this paper. Firstly, adaptive extracting wavelet transform (AEWT) is proposed to obtain accurate TFR by creatively exploiting the window width by adjusting center frequency and scale factor based on an attention mechanism, and taking the advantage of synchroextracting operator in IF estimation operator. Then this paper introduces a novel damage indicator (DI) based on time-frequency fractional dispersion entropy (TFDE) to directly extract fault information from TFR to avoid information loss. Finally, the obtained DIs are used as inputs for a prediction model, where the adaptive particle swarm optimization algorithm (APSO) is employed to optimize Gaussian process regression (GPR). This method effectively tracks the bearing’s degradation state and accurately predicts its remaining useful life. The experimental verification was carried out with data sets of bearings running to failure. The results show that the proposed time-frequency damage indicator (TFDI) has better monotonicity, and the hybrid prediction model is more accurate to predict bearing RUL.},
  keywords={Feature extraction;Entropy;Accuracy;Time-frequency analysis;Transforms;Estimation;Continuous wavelet transforms;Predictive models;Gaussian processes;Degradation;Time-Frequency Analysis;Dispersion Entropy;RUL Prediction;Gaussian Process Regression},
  doi={10.1109/JSEN.2025.3580070},
  ISSN={1558-1748},
  month={},}@ARTICLE{10636811,
  author={Huang, Heng and Zhao, Lin and Dai, Haixing and Zhang, Lu and Hu, Xintao and Zhu, Dajiang and Liu, Tianming},
  journal={IEEE Transactions on Multimedia}, 
  title={BI-AVAN: A Brain-Inspired Adversarial Visual Attention Network for Characterizing Human Visual Attention From Neural Activity}, 
  year={2024},
  volume={26},
  number={},
  pages={11191-11203},
  abstract={Visual attention is a fundamental mechanism in the human brain, and it inspires the design of attention mechanisms in deep neural networks. However, most of the visual attention studies adopted eye-tracking data rather than the direct measurement of brain activity to characterize human visual attention. In addition, the adversarial relationship between the attention-related objects and attention-neglected background in the human visual system was not fully exploited. To bridge these gaps, we propose a novel brain-inspired adversarial visual attention network (BI-AVAN) to characterize human visual attention directly from functional brain activity. Our BI-AVAN model imitates the biased competition process between attention-related/neglected objects to identify and locate the visual objects in a movie frame the human brain focuses on in an unsupervised manner. We use independent eye-tracking data as ground truth for validation and experimental results show that our model achieves robust and promising results when inferring meaningful human visual attention and mapping the relationship between brain activities and visual stimuli. Our BI-AVAN model contributes to the emerging field of leveraging the brain's functional architecture to inspire and guide the model design in artificial intelligence (AI), e.g., deep neural networks.},
  keywords={Visualization;Brain modeling;Brain;Gaze tracking;Predictive models;Functional magnetic resonance imaging;Feature extraction;fMRI;visual attention;brain;brain-inspired AI},
  doi={10.1109/TMM.2024.3443623},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10637093,
  author={Podder, Kanchon Kanti and Zhang, Jian and Wu, Yongshuai},
  booktitle={2024 IEEE International Conference on Advanced Intelligent Mechatronics (AIM)}, 
  title={IHSR: A Framework Enables Robots to Learn Novel Hand Signals From a Few Samples}, 
  year={2024},
  volume={},
  number={},
  pages={959-965},
  abstract={This project introduces a framework to enable robots to recognize human hand signals, a reliable and feasible device-free means of communication in many noisy environments such as construction sites and airport ramps, to facilitate efficient human-robot collaboration. Various hand signal systems are accepted in many small groups for specific purposes, such as Marshalling on airport ramps and construction site crane operations. Robots must be robust to unpredictable conditions, including various backgrounds and human appearances, an extreme challenge imposed by open environments. To address these challenges, we propose Instant Hand Signal Recognition (IHSR), a learning-based framework with world knowledge of human gestures embedded, for robots to learn novel hand signals in a few samples. It also offers robust zero-shot generalization to recognize learned signals in novel scenarios. Extensive experiments show that our IHSR can learn a novel hand signal in only 50 samples, which is 30+ times more efficient than the state-of-the-art method. It also demonstrates a robust zero-shot generalization for deploying a learned model in unseen environments to recognize hand signals from unseen human users.},
  keywords={Training;Mechatronics;Collaboration;Implants;Network architecture;Airports;Robustness;Zero-shot Generalization;Gesture Recognition;Hand signals;Cross-modality Embedding},
  doi={10.1109/AIM55361.2024.10637093},
  ISSN={2159-6255},
  month={July},}@ARTICLE{10699385,
  author={Wei, Hongguang and Wang, Nan and Liu, Yuan and Ma, Pengge and Pang, Dongdong and Sui, Xiubao and Chen, Qian},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Spatio-Temporal Feature Fusion and Guide Aggregation Network for Remote Sensing Change Detection}, 
  year={2024},
  volume={62},
  number={},
  pages={1-16},
  abstract={The field of remote sensing change detection (RSCD) has seen significant advancements recently, focusing on the precise identification and analysis of temporal changes in remote sensing images. Existing deep learning-based RSCD methods primarily rely on concatenation or subtraction to integrate features of bi-temporal images and reconstruct change features through a feature pyramid network (FPN) decoding architecture. However, these methods face challenges related to inadequate spatio-temporal change representation and insufficient aggregation of multilevel semantic information, resulting in pseudo-changes and poor completeness of detected change objects. In this article, we propose an innovative RSCD framework via spatio-temporal feature fusion and guide aggregation (STFF-GA) to address the aforementioned challenges. The architecture of this network comprises two key components: the STFF module and the GA module. The STFF module is designed as a low-parameter and low-computation structure, effectively enhancing the representation of spatio-temporal change information through split, interaction, and fusion strategies. The GA module uses deep feature guidance (DFG) mapping as prior information to guide the aggregation of multilevel semantic information, thereby correcting the positional information of change objects and filtering out pseudo-changes and other noise interference. In addition, it utilizes convolution kernels of various scales to extract fine-grained features, facilitating the complete reconstruction of change objects. Extensive experiments conducted on three benchmark change detection datasets demonstrate that the proposed STFF-GA consistently outperforms other state-of-the-art (SOTA) detectors. The code is available at https://github.com/NjustHGWei/STFF-GA.},
  keywords={Feature extraction;Transformers;Semantics;Image reconstruction;Remote sensing;Convolutional neural networks;Interference;Decoding;Noise;Convolution;Change detection;convolutional neural network (CNN);guide aggregation (GA);spatio-temporal feature fusion (STFF)},
  doi={10.1109/TGRS.2024.3470314},
  ISSN={1558-0644},
  month={},}@ARTICLE{9314924,
  author={Eerapu, Karuna Kumari and Lal, Shyam and Narasimhadhan, A. V.},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={O-SegNet: Robust Encoder and Decoder Architecture for Objects Segmentation From Aerial Imagery Data}, 
  year={2022},
  volume={6},
  number={3},
  pages={556-567},
  abstract={The segmentation of diversified roads and buildings from high-resolution aerial images is essential for various applications, such as urban planning, disaster assessment, traffic congestion management, and up-to-date road maps. However, a major challenge during object segmentation is the segmentation of small-sized, diverse shaped roads, and buildings in dominant background scenarios. We introduce O-SegNet- the robust encoder and decoder architecture for objects segmentation from high-resolution aerial imagery data to address this challenge. The proposed O-SegNet architecture contains Guided-Attention (GA) blocks in the encoder and decoder to focus on salient features by representing the spatial dependencies between features of different scales. Further, GA blocks guide the successive stages of encoder and decoder by interrelating the pixels of the same class. To emphasize more on relevant context, the attention mechanism is provided between encoder and decoder after aggregating the global context via an 8 Level Pyramid Pooling Network (PPN). The qualitative and quantitative results of the proposed and existing semantic segmentation architectures are evaluated by utilizing the dataset provided by Kaiser et al. Further, we show that the proposed O-SegNet architecture outperforms state-of-the-art techniques by accurately preserving the road connectivity and structure of buildings.},
  keywords={Feature extraction;Image segmentation;Roads;Computer architecture;Semantics;Decoding;Buildings;Segmentation;aerial images;self-attention;GA blocks},
  doi={10.1109/TETCI.2020.3045485},
  ISSN={2471-285X},
  month={June},}@ARTICLE{9792273,
  author={Meqdad, Maytham N. and Abdali-Mohammadi, Fardin and Kadry, Seifedine},
  journal={IEEE Access}, 
  title={Meta Structural Learning Algorithm With Interpretable Convolutional Neural Networks for Arrhythmia Detection of Multisession ECG}, 
  year={2022},
  volume={10},
  number={},
  pages={61410-61425},
  abstract={Detection of arrhythmia of electrocardiogram (ECG) signals recorded within several sessions for each person is a challenging issue, which has not been properly investigated in the past. This arrhythmia detection is challenging since a classification model that is constructed and tested using ECG signals maintains generalization when dealing with unseen samples. This article has proposed a new interpretable meta structural learning algorithm for this challenging detection. Therefore, a compound loss function was suggested including the structural feature extraction fault and space label fault with GUMBEL-SOFTMAX distribution in the convolutional neural network (CNN) models. The collaboration between models was carried out to create learning to learn features in models by transferring the knowledge among them when confronted by unseen samples. One of the deficiencies of a meta-learning algorithm is the non-interpretability of its models. Therefore, to create an interpretability feature for CNN models, they are encoded as the evolutionary trees of the genetic programming (GP) algorithms in this article. These trees learn the process of extracting deep structural features in the course of the evolution in the GP algorithm. The experimental results suggested that the proposed detection model enjoys an accuracy of 98% regarding the classification of 7 types of arrhythmia in the samples of the Chapman ECG dataset recorded from 10646 patients in different sessions. Finally, the comparisons demonstrated the competitive performance of the proposed model concerning the other models based on the big deep models.},
  keywords={Electrocardiography;Classification algorithms;Heart;Feature extraction;Deep learning;Convolutional neural networks;Solid modeling;Arrhythmia detection;convolutional neural network;deep features representation;meta learning;multi-session ECG},
  doi={10.1109/ACCESS.2022.3181727},
  ISSN={2169-3536},
  month={},}@ARTICLE{10491287,
  author={Yang, Zhigang and Liu, Yiming and Wen, Guiwei and Xia, Xiangyu and Zhang, Wei Emma and Chen, Tao},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={Object Detection in Remote Sensing Images With Parallel Feature Fusion and Cascade Global Attention Head}, 
  year={2024},
  volume={21},
  number={},
  pages={1-5},
  abstract={Convolutional neural networks (CNNs) have driven significant development in remote sensing (RS) object detection. To achieve concise and effective optimization, we propose a two-stage detector with a parallel feature fusion strategy and a cascade global attention (GA) mechanism for object detection in RS images, named PC-RCNN. We first design a feature pyramid network with two parallel branches (PB-FPN), corresponding to the top-down and bottom-up feature fusion pathways, respectively. Different optimization modules can be adopted in different pathways to avoid potential module incompatibility when connected in series. Such parallel feature fusion strategy can achieve both higher detection accuracy and higher computational efficiency compared with previous series fusion modes. Furthermore, we design a GA block to enhance feature representations of regions and propose a cascade GA head network (CGA-Head) for accurate category prediction and location estimation. Experiments on a challenging large-scale dataset, namely DOTA, show that the proposed PC-RCNN achieves a mean average precision (mAP) of 77.63%, which is comparable to other state-of-the-art CNN-based models. Parallel feature fusion, cascade GA, object detection, RS images.},
  keywords={Feature extraction;Head;Task analysis;Optimization;Object detection;Neck;Vectors;Cascade global attention (GA);object detection;parallel feature fusion;remote sensing (RS) images},
  doi={10.1109/LGRS.2024.3385231},
  ISSN={1558-0571},
  month={},}@INPROCEEDINGS{10150239,
  author={Fujimoto, Ryusei and Nakamura, Yugo and Arakawa, Yutaka},
  booktitle={2023 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, 
  title={Differential Privacy with Weighted ∊ for Privacy-Preservation in Human Activity Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={634-639},
  abstract={Many services based on human activity recognition (HAR) have been developed; however, user activity data include a large amount of private information. Although privacy protection is important in activity recognition, it has not been sufficiently explored. Therefore, we propose a privacy-preserving mechanism for HAR services that uses differential privacy. The proposed method reduces the user recognition accuracy to a level that satisfies the privacy requirements by adding weighted noise to the features in the learning model construction and then improves the activity recognition accuracy (service usefulness). The results indicate that when the privacy requirement is defined as less than the probability of a user being identified by chance, the proposed method improves the activity recognition accuracy by approximately 10 % compared to the conventional method.},
  keywords={Pervasive computing;Privacy;Differential privacy;Conferences;Feature extraction;Noise measurement;Standards;HAR;differential privacy;privacy-preserving mechanism;weighted noise;service usefulness;privacy protection},
  doi={10.1109/PerComWorkshops56833.2023.10150239},
  ISSN={2766-8576},
  month={March},}@INPROCEEDINGS{10047809,
  author={Vani, K. and Sujatha, S.},
  booktitle={2022 Second International Conference on Advanced Technologies in Intelligent Control, Environment, Computing & Communication Engineering (ICATIECE)}, 
  title={A Machine Learning Framework for Job Failure Prediction in Cloud using Hyper Parameter Tuned MLP}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Failures are inevitable in cloud computing systems (CCS) because of their enormous size and complexity, which results in reliability and efficiency losses. It is possible to take actions to increase the reliability and effectiveness of CCS through failure mitigation, fault tolerance, and recovery. Failure prediction and risk identification approaches could forecast such failure occurrences using data gathered during CCS operation. In order to handle the present state-of-the-art evolving computing systems, standard runtime fault-tolerance (FT) solutions like data replication and periodic check-pointing are not very successful. This has made it essential to have a robust method with a thorough knowledge of component and system failures as well as the capability to accurately predict probable system failures in the future. In this research, we develop a paradigm for improving the reliability and efficiency of cloud environment by risk assessment. This study starts with analyzing the failure task behavior and their related operational information. A predictive model is built using cat boost, genetic algorithm and hyper parameter tuned multilayer perceptron for finding the feature importance, selecting the most relevant features and to predict the high risk cloud tasks respectively. The present method is evaluated on Google Custer data with necessary performance metrics and compared with other machine learning approaches.},
  keywords={Cloud computing;Fault tolerance;Fault tolerant systems;Machine learning;Predictive models;Multilayer perceptrons;Feature extraction;Task termination status;Feature selection;fault tolerance;machine learning feature importance},
  doi={10.1109/ICATIECE56365.2022.10047809},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10356240,
  author={Lei, Shanglin and Wang, Xiaoping and Dong, Guanting and Li, Jiang and Liu, Yingjian},
  booktitle={2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion Recognition in Conversation With Emotion Disentanglement}, 
  year={2023},
  volume={},
  number={},
  pages={881-888},
  abstract={Emotion Recognition in Conversation (ERC) has attracted widespread attention in the natural language processing field due to its enormous potential for practical applications. Existing ERC methods face challenges in achieving generalization to diverse scenarios due to insufficient modeling of context, ambiguous capture of dialogue relationships and overfitting in speaker modeling. In this work, we present a Hybrid Continuous Attributive Network (HCAN) to address these issues in the perspective of emotional continuation and emotional attribution. Specifically, HCAN adopts a hybrid recurrent and attention-based module to model global emotion continuity. Then a novel Emotional Attribution Encoding (EAE) is proposed to model intra- and inter-emotional attribution for each utterance. Moreover, aiming to enhance the robustness of the model in speaker modeling and improve its performance in different scenarios, A comprehensive loss function emotional cognitive loss $\mathcal{L}_{EC}$ is proposed to alleviate emotional drift and overcome the overfitting of the model to speaker modeling. Our model achieves state-of-the-art performance on three datasets, demonstrating the superiority of our work. Another extensive comparative experiments and ablation studies on three benchmarks are conducted to provided evidence to support the efficacy of each module. Further exploration of generalization ability experiments shows the plug-and-play nature of the EAE module in our method.},
  keywords={Emotion recognition;Face recognition;Oral communication;Benchmark testing;Feature extraction;Robustness;Natural language processing;Natural language processing;emotion recognition in conversation;context modeling;dialogue relationship},
  doi={10.1109/ICTAI59109.2023.00133},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10500094,
  author={Dávila-Montero, Sylmarie and Mason, Andrew J.},
  booktitle={SoutheastCon 2024}, 
  title={Machine Learning Fusion Model Approach for the Real-Time Detection of Head Gestures using IMUs}, 
  year={2024},
  volume={},
  number={},
  pages={909-913},
  abstract={Modern sensor technology has contributed to the study of human behaviors during social interactions. Inertial movement units (IMUs) have shown great promise in the recognition of communication cues displayed by head gestures, which are important for healthy interactions. However, no gold standard exists to automatically detect head actions from IMUs. This paper presents the design of a real-time head-action detection (HAD) unit based on a new real-time fusion model architecture approach. An analysis of buffer sizes and feature contribution using a decision tree (DT) classifier and a predictor importance fusion is presented. The fusion model is composed of two classification stages wherein the first stage focus on recognizing head position and the second on recognizing head motion. The designed HAD unit uses a data buffer size of 3s, 7 features in total, and a DT classifier. Results show a testing accuracy of 97.91% and an F1-score of 98.5%. The use of the designed HAD unit and its architecture could allow for easy re-training to add recognition of additional head actions by having specialized head action classification models.},
  keywords={Time-frequency analysis;Machine learning;Feature extraction;Magnetic heads;Real-time systems;Data models;Complexity theory;fusion model;machine learning;real-time;inertial movement sensors;head gestures;healthy interactions;wearables},
  doi={10.1109/SoutheastCon52093.2024.10500094},
  ISSN={1558-058X},
  month={March},}@INPROCEEDINGS{9881016,
  author={Curbelo, Victor M. O. and Alzeidan, Ahmad and Quivy, Alain A.},
  booktitle={2022 36th Symposium on Microelectronics Technology (SBMICRO)}, 
  title={Capping of InAs quantum dots by migration enhanced epitaxy}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={Molecular beam epitaxy is a deposition technique providing high-quality single-crystalline samples with extremely high carrier mobility and very sharp heterointerfaces. It is often used to produce very specific devices like high-mobility transistors, high-detectivity infrared photodetectors, and high-efficiency solar cells, which may eventually contain quantum dots. As a consequence, optimization of the optical properties of such nanostructures is very important as it can improve device performance. Here, we compare photoluminescence data of samples containing InAs quantum dots covered by a thin GaAs layer which was deposited either by conventional molecular beam epitaxy or by an alternative method called migration enhanced epitaxy. We show that the latter provides better samples but also enhances indium segregation. We provide a consistent explanation of the data and discuss ways to improve the growth conditions and get better results.},
  keywords={Performance evaluation;Ultraviolet sources;Photovoltaic cells;Quantum dots;Gallium arsenide;Molecular beam epitaxial growth;Photoluminescence;migration enhanced epitaxy;InAs quantum dots;In segregation;molecular beam epitaxy;photoluminescence},
  doi={10.1109/SBMICRO55822.2022.9881016},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10505511,
  author={Zhang, Lei and He, Pei and Chang, Chi-Chang and Ting, Wen-Chien},
  booktitle={2023 13th International Conference on Information Technology in Medicine and Education (ITME)}, 
  title={Grammatical Evolution for Predicting Cervical Cancer Recurrence Risk}, 
  year={2023},
  volume={},
  number={},
  pages={851-859},
  abstract={Cervical cancer is one of the most common tumors in women and has a high rate of recurrence after surgery. Early detection of symptoms leading to earlier treatment can significantly reduce the risk of patients. Artificial intelligence assists in diagnosing the recurrence of cervical cancer, which can reduce diagnostic costs and increase early identification of symptoms for timely treatment. In this study, grammatical evolution was used to develop a predictive model. The model included the corresponding grammar, evolutionary strategies and adaptations for common influences and important independent features. The grammatical evolution model was compared to the predictive model constructed using decision tree algorithms. The results show that the predictive model based on grammatical evolution performs well in predicting cervical cancer recurrence. Furthermore, due to the flexibility and adaptability of GE, the model can achieve desirable results by adjusting parameters. The model's efficiency improves when using important independent features obtained from the decision tree algorithm as the grammatical evolution inputs. Then, the importance of all features in the model was determined using the permutation importance. In this proposed model, the key features for predicting recurrence are pathologic T, pathologic stage, LNM, surgical margin involvement, and RT target summary in descending order of importance. These results will help clinicians to pay attention to patients with such risk factors before recurrence occurs.},
  keywords={Adaptation models;Costs;Surgery;Predictive models;Germanium;Prediction algorithms;Classification algorithms;Cervical Cancer;Decision Tree;Grammatical Evolution;Permutation Importance},
  doi={10.1109/ITME60234.2023.00173},
  ISSN={2474-3828},
  month={Nov},}@INPROCEEDINGS{10822453,
  author={Sun, Fangling and Liang, Chuang and Adali, Tülay and Zhang, Daoqiang and Jiang, Rongtao and Calhoun, Vince D. and Qi, Shile},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={GPR-SCSANet: Unequal-Length Time Series Normalization with Split-Channel Residual Convolution and Self-Attention for Brain Age Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={5097-5103},
  abstract={Functional magnetic resonance imaging (fMRI), as a non-invasive method to reveal brain function alterations, frequently yields time series with unequal lengths in real-world scenarios, which may arise from factors such as motion artifacts, participant state, and differing scan protocols. This variability conflicts with the traditional methods relying on isometric inputs, which poses a significant challenge for the downstream applications such as brain age prediction. To address this challenge, we introduced Gaussian Process Regression (GPR) to normalize the length of time series and proposed split-channel residual convolution (SC) and self-attention mechanisms (SA) to perform brain age estimation, called GPR-SCSANet. Results showed that the proposed framework, GPR-SCSANet, is able to fully utilize the inherent information and learn richer feature representations from unequal-length fMRI time courses, which significantly improved the prediction accuracy across 3 brain atlases and 5 prediction models. The results demonstrated the effectiveness and robustness of the proposed GPR-SCSANet, showcasing the potential for broader applications in brain age prediction task.},
  keywords={Accuracy;Protocols;Convolution;Time series analysis;Predictive models;Functional magnetic resonance imaging;Brain modeling;Robustness;Data models;Stability analysis;brain age prediction;unequal-length time courses;fMRI;gaussian process regression;convolutional neural network},
  doi={10.1109/BIBM62325.2024.10822453},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{9912019,
  author={Kornelsen, M. L. and Mozafari, S. H. and Clark, J. J. and Meyer, B. H. and Gross, W. J.},
  booktitle={2022 IEEE 33rd International Conference on Application-specific Systems, Architectures and Processors (ASAP)}, 
  title={Fast Heterogeneous Task Mapping for Reducing Edge DNN Latency}, 
  year={2022},
  volume={},
  number={},
  pages={64-71},
  abstract={To meet DNN inference latency constraints on resource-constrained edge devices, we employ heterogeneous computing, utilizing multiple processing elements (e.g. CPU + GPU) accelerate inference. This leads to the challenge of efficiently mapping DNN operations to heterogeneous processing elements. For this task, we introduce a novel genetic algorithm (GA) optimizer. Through intelligent initialization and a customized mutation operation, we are able to evaluate 20x fewer generations while finding superior configurations compared with a baseline GA. Using our mapping optimizer, we find device placement configurations that achieve 15%, 24%, and 31% inference speed-up for BERT, SqueezeBERT, and InceptionV3,respectively.},
  keywords={Bit error rate;Systems architecture;Graphics processing units;Heterogeneous networks;Task analysis;Genetic algorithms;edge;dnn;heterogeneous;mapping;scheduling;neural network;bert},
  doi={10.1109/ASAP54787.2022.00020},
  ISSN={2160-052X},
  month={July},}@INPROCEEDINGS{10825731,
  author={Quesada, Jorge and Fowler, Zoe and Alotaibi, Mohammad and Prabhushankar, Mohit and AlRegib, Ghassan},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Benchmarking Human and Automated Prompting in the Segment Anything Model}, 
  year={2024},
  volume={},
  number={},
  pages={1625-1634},
  abstract={The remarkable capabilities of the Segment Anything Model (SAM) for tackling image segmentation tasks in an intuitive and interactive manner has sparked interest in the design of effective visual prompts. Such interest has led to the creation of automated point prompt selection strategies, typically motivated from a feature extraction perspective. However, there is still very little understanding of how appropriate these automated visual prompting strategies are, particularly when compared to humans, across diverse image domains. Additionally, the performance benefits of including such automated visual prompting strategies within the finetuning process of SAM also remains unexplored, as does the effect of interpretable factors like distance between the prompt points on segmentation performance. To bridge these gaps, we leverage a recently released visual prompting dataset, PointPrompt, and introduce a number of benchmarking tasks that provide an array of opportunities to improve the understanding of the way human prompts differ from automated ones and what underlying factors make for effective visual prompts. We demonstrate that the resulting segmentation scores obtained by humans are approximately 29% higher than those given by automated strategies and identify potential features that are indicative of prompting performance with R2 scores over 0.5. Additionally, we demonstrate that performance when using automated methods can be improved by up to 68% via a finetuning approach. Overall, our experiments not only showcase the existing gap between human prompts and automated methods, but also highlight potential avenues through which this gap can be leveraged to improve effective visual prompt design. Further details along with the dataset links and codes are available at https://github.com/olivesgatech/PointPrompt},
  keywords={Bridges;Visualization;Image segmentation;Codes;Benchmark testing;Big Data;Feature extraction;Data models;foundation models;prompting;visual;segmentation},
  doi={10.1109/BigData62323.2024.10825731},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{10328715,
  author={Wang, Shouhui and Qin, Biao},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={A Novel Joint Training Model for Knowledge Base Question Answering}, 
  year={2024},
  volume={32},
  number={},
  pages={666-679},
  abstract={In knowledge base question answering (KBQA) systems, relation detection and entity recognition are two core components. However, since the relation detection in KBQA contains thousands of relations and this task always becomes a zero-shot learning task due to the relations in some test samples while they have not appeared in training data, relation detection is more difficult than entity recognition. In addition, previous studies only considered these two tasks separately and did not take full advantage of their correlation. This article proposes a novel relation and entity joint extraction framework, named Gated-Attention-based Joint Training Model (Ga-JTM), to integrate relation detection and entity recognition. In addition, to train the two models simultaneously, a knowledge-driven gated unit based on a multi-head attention mechanism is designed. It combines the knowledge graph embeddings and the current context semantic information to process relation detection and entity recognition tasks, respectively. The experiments are conducted on a single-relation dataset (SimpleQuestions) and a multiple-relation dataset (WebQSP), and the experimental results demonstrate that our Ga-JTM is superior to the state-of-the-art (SOTA) performance of relation detection and can improve the performance of entity recognition and entity linking. Finally, these improvements contribute to the SOTA performance in our KBQA system.},
  keywords={Semantics;Task analysis;Knowledge based systems;Training;Question answering (information retrieval);Speech processing;Joining processes;Knowledge base question answering;joint training model;relation detection;entity recognition;multi-task learning},
  doi={10.1109/TASLP.2023.3336526},
  ISSN={2329-9304},
  month={},}@ARTICLE{10915627,
  author={Meng, Lingwu and Wang, Jing and Huang, Yan and Xiao, Liang},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={RSIC-GMamba: A State-Space Model With Genetic Operations for Remote Sensing Image Captioning}, 
  year={2025},
  volume={63},
  number={},
  pages={1-16},
  abstract={Recent advancements indicate that the novel Mamba framework, with its linear reasoning capabilities and performance on par with the Transformer framework, has become a popular alternative to Transformers. However, there is still a lack of exploration in remote sensing image captioning (RSIC). One important challenge is that the 1-D selective state-space model (SSM) is not suitable for 2-D remote sensing images (RSIs). To mitigate this challenge, this article proposes an SSM with genetic operations for RSIC (RSIC-GMamba) that integrates the search capabilities of heuristic genetic algorithm and the global modeling capabilities of Transformer into the Mamba framework. To comprehensively capture the multiscale global context of RSIs, we design a genetic SSM by incorporating a dilated convolution, genetic operations (crossover and mutation), and self-attention mechanism into the selective SSM. Specifically, dilated convolutions are employed to extract multiscale visual features through varying dilation rates. Crossover operation expands the spatial arrangement of image regions to thoroughly capture contextual information and mutation operation introduces randomness to enhance the model’s robustness. The self-attention mechanism is integrated to model relationships among SSM hidden states, thereby enhancing visual context. In addition, to fully utilize high- and low-level visual-semantic information, we propose a vision and scene text aggregation (ViSTA) module based on gating mechanisms. Experimental results on four RSIC datasets demonstrate the effectiveness of the proposed RSIC-GMamba. The code will be publicly available at https://github.com/One-paper-luck/RSIC-GMamba.},
  keywords={Transformers;Visualization;Remote sensing;Genetics;Computational modeling;Feature extraction;Training;Context modeling;Decoding;Aggregates;Genetic;Mamba;remote sensing image captioning (RSIC);scan mechanism},
  doi={10.1109/TGRS.2025.3548664},
  ISSN={1558-0644},
  month={},}@ARTICLE{9632408,
  author={Valdes, Gilmer and Interian, Yannet and Gennatas, Efstathios and Van der Laan, Mark},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The Conditional Super Learner}, 
  year={2022},
  volume={44},
  number={12},
  pages={10236-10243},
  abstract={Using cross validation to select the best model from a library is standard practice in machine learning. Similarly, meta learning is a widely used technique where models previously developed are combined (mainly linearly) with the expectation of improving performance with respect to individual models. In this article we consider the Conditional Super Learner (CSL), an algorithm that selects the best model candidate from a library of models conditional on the covariates. The CSL expands the idea of using cross validation to select the best model and merges it with meta learning. We propose an optimization algorithm that finds a local minimum to the problem posed and proves that it converges at a rate faster than $O_p(n^{-1/4})$Op(n-1/4). We offer empirical evidence that: (1) CSL is an excellent candidate to substitute stacking and (2) CLS is suitable for the analysis of Hierarchical problems. Additionally, implications for global interpretability are emphasized.},
  keywords={Libraries;Mathematical models;Training data;Machine learning algorithms;Stacking;Partitioning algorithms;Training;Cross-validation;meta learning;super learner;interpretability;nonparametric hierarchical models},
  doi={10.1109/TPAMI.2021.3131976},
  ISSN={1939-3539},
  month={Dec},}@INPROCEEDINGS{10803333,
  author={Jiangtao, Zhang and Xiaoxia, Zhang and Wenjing, Lei and Guoyin, Wang},
  booktitle={2024 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={Cancer Gene Selection Algorithm Based on Neighborhood Rough Sets Combined with Quantum Genetic Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={558-563},
  abstract={Rough Set Theory (RST) is a commonly used and effective data processing tool. To extend the range of data that RST and its related theories can handle, researchers have proposed many innovative approaches based on the Pawlak model. This paper presents an efficient phased method that incorporates a quantum genetic algorithm to address the prob-lem of attribute reduction in high-dimensional data analysis and accelerate the computation process. The research method is mainly divided into three stages: first, feature importance assessment is used to classify the feature set into three categories: deterministic, chaotic, and redundant, with redundant features being eliminated; second, the quantum genetic algorithm is applied to optimize the chaotic feature set, ensuring effective partitioning while retaining dataset information; third, the core set undergoes fine-tuning to adjust the distribution of specific attributes, making it closely align with the decision distribution, thereby achieving rapid attribute reduction. To further discuss the ability of this reduction algorithm to eliminate redundant attributes, comparative experiments were conducted using four different algorithms, and speed validation was performed using five cancer gene datasets with over 10,000 dimensions. The exper-imental results show that this method not only effectively reduces computational complexity but also overcomes the limitations of rough set theory, significantly enhancing the ability to handle large datasets, and demonstrates its potential and application prospects in high-dimensional data analysis.},
  keywords={Quantum computing;High dimensional data;Rough sets;Genomics;Finance;Feature extraction;Computational efficiency;Partitioning algorithms;Genetic algorithms;Cancer;attribute reduction;quantum genetic algorithm;high-dimensional data;rough set theory},
  doi={10.1109/MedAI62885.2024.00079},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10946710,
  author={Wan, Zishen and Yang, Hanchen and Raj, Ritik and Liu, Che-Kai and Samajdar, Ananda and Raychowdhury, Arijit and Krishna, Tushar},
  booktitle={2025 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={CogSys: Efficient and Scalable Neurosymbolic Cognition System via Algorithm-Hardware Co-Design}, 
  year={2025},
  volume={},
  number={},
  pages={775-789},
  abstract={Neurosymbolic AI is an emerging compositional paradigm that fuses neural learning with symbolic reasoning to enhance the transparency, interpretability, and trustworthiness of AI. It also exhibits higher data efficiency making it promising for edge deployments. Despite the algorithmic promises and demonstrations, unfortunately executing neurosymbolic workloads on current hardware (CPU/GPU/TPU) is challenging due to higher memory intensity, greater compute heterogeneity and access pattern irregularity, leading to severe hardware underutilization. This work proposes CogSys, a characterization and co-design framework dedicated to neurosymbolic AI system acceleration, aiming to win both reasoning efficiency and scalability. On the algorithm side, CogSys proposes an efficient factorization technique to alleviate compute and memory overhead. On the hardware side, CogSys proposes a scalable neurosymbolic architecture with reconfigurable neuro/symbolic processing elements ($n s P E$) and bubble streaming (BS) dataflow with spatial-temporal (ST) mapping for highly parallel and efficient neurosymbolic computation. On the system side, CogSys features an adaptive workload-aware scheduler (adSCH) to orchestrate heterogeneous kernels and enhance resource utilization. Evaluated across cognitive workloads, CogSys enables reconfigurable support for neural and symbolic kernels and exhibits $\gt75 \times$ speedup over TPU-like systolic array with only $\lt5 \%$ area overhead, as benchmarked under the TSMC 28nm technology node. CogSys achieves $4 \times$ $96 \times$ speedup compared to desktop and edge GPUs. For the first time, CogSys enables real-time abduction reasoning towards human fluid intelligence, requiring only 0.3 s per reasoning task with $4 \mathrm{~mm}^{2}$ area and 1.48 W power consumption.},
  keywords={Adaptive systems;Power demand;Scalability;Cognition;Hardware;Real-time systems;Systolic arrays;Resource management;Artificial intelligence;Kernel},
  doi={10.1109/HPCA61900.2025.00064},
  ISSN={2378-203X},
  month={March},}@INPROCEEDINGS{10917581,
  author={Chabrier, Lisa and Crombach, Anton and Peignier, Sergio and Rigotti, Christophe},
  booktitle={2024 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={Detecting gene regulatory network rewiring as changes in marker regulatory associations using only single-cell gene expression data}, 
  year={2024},
  volume={},
  number={},
  pages={850-858},
  abstract={Understanding changes in gene regulatory networks, known as rewiring, is a challenge. Methods addressing this problem require multi-omics input data, which is costly to generate for model organisms, and simply not available for non-model organisms. We propose a new approach to detect rewiring events between cell types, that uses only single-cell RNA sequencing data. The approach consists of training machine learning models to predict the expression of target genes from the expression of transcription factors. Then a local (i.e., per cell), model-agnostic explainability method, SHAP, determines the importance of each transcription factor for each target gene. We consider that these transcription factor importances capture the gene network state of a cell. A new space is created describing cells in terms of regulatory network associations between genes. We compute in this space the statistically different associations between cell types, resulting in a set of marker regulatory associations. Several visualizations help the selection of putative rewiring events between GRN states in different cell types. The proposed workflow is applied to two datasets, demonstrating the potential for detecting rewiring. The implementation of our workflow follows the conventions of the scverse initiative and allows easy integration with Scanpy and other packages for single-cell analysis.},
  keywords={Training;Sequential analysis;RNA;Computational modeling;Machine learning;Network analyzers;Predictive models;Organisms;Gene expression;Immune system;Explainable ML;SHAP;gene regulatory network;network rewiring;differential network analysis},
  doi={10.1109/ICDMW65004.2024.00117},
  ISSN={2375-9259},
  month={Dec},}@ARTICLE{10971392,
  author={Shi, Haotian and Jin, Zipeng},
  journal={IEEE Access}, 
  title={Multi-Condition Magnetic Core Loss Prediction and Magnetic Component Performance Optimization Based on Improved Deep Forest}, 
  year={2025},
  volume={13},
  number={},
  pages={82261-82277},
  abstract={In recent years, power electronic technology has been widely applied in energy conversion in the fields of renewable energy and information and communication technology. Magnetic components play a crucial role in voltage stabilization, conversion, and filtering, conventional loss models struggle to accurately characterize the nonlinear loss characteristics under multi-physics coupling and complex operating conditions. To achieve high efficiency and high power density designs, it is essential to study core loss characteristics and optimize operating parameters. This paper proposes a core loss prediction model based on an improved deep forest algorithm and information entropy-enhanced genetic algorithm. First, a multi-factor analysis of variance (ANOVA) and Bayesian inference are employed to explore the effects of temperature, material, and excitation waveform on core losses, as well as their optimal combinations. Subsequently, a temperature correction factor is introduced into the conventional Steinmetz equation, and the ridge regression regularization technique is applied to improve the goodness-of-fit from 0.954 to 0.986. Next, the deep forest algorithm is employed to perform multi-granularity scanning for feature extraction from the samples, which are then fed into an enhanced cascade forest (with two random forests replaced by XGBoost and LightGBM), key parameters (maximum tree depth and number of trees) are optimized via grid search, constructing a highly accurate and robust core loss prediction model. The model’s superiority is validated from multiple perspectives, accompanied by feature importance analysis using SHAP values. Finally, an information entropy and game theory-based genetic algorithm is utilized to optimize the operating conditions of magnetic components, yielding an optimal combination of low losses and high magnetic energy transfer efficiency.},
  keywords={Magnetic cores;Core loss;Mathematical models;Magnetic hysteresis;Magnetic flux density;Random forests;Optimization;Regression analysis;Predictive models;Genetic algorithms;Core loss;Steinmetz equation;deep forest;multi-condition analysis;SHAP value;genetic algorithm},
  doi={10.1109/ACCESS.2025.3562736},
  ISSN={2169-3536},
  month={},}@ARTICLE{9761886,
  author={Si, Yupeng and Wang, Rongjie and Zhang, Shiqi and Zhou, Wenting and Lin, Anhui and Wang, Yichun},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Fault Diagnosis Based on Attention Collaborative LSTM Networks for NPC Three-Level Inverters}, 
  year={2022},
  volume={71},
  number={},
  pages={1-16},
  abstract={To address the problem, it is difficult to extract fault features for neutral-point-clamped (NPC) three-level inverters under nonstationary conditions. This article proposes a multi-information feature fusion diagnosis method based on attention collaborative stacked long short-term memory (ASLSTM) neural networks. First, parallel structural stacked LSTM networks are constructed, which are used to automatically extract features from multisource time-series data. Then, the attention mechanism is applied to weight these features adaptively. Finally, the fault is identified by features that integrate multiple sources of information. In addition, the quantum particle swarm optimization (QPSO) algorithm is used to intelligently tune the hyperparameters of ASLSTM to improve the reasonableness of hyperparameter selection for the diagnostic model. The simulation results of the fault diagnosis of the NPC three-level inverter circuit show that the proposed method is able to extract high-discriminative features from raw data and has better diagnostic results under various conditions compared with other schemes.},
  keywords={Circuit faults;Feature extraction;Fault diagnosis;Inverters;Data mining;Mathematical models;Integrated circuit modeling;Attention mechanism;fault diagnosis;neutral-point-clamped (NPC) three-level inverters;parallel structural stacked long short-term memory (LSTM) networks;quantum particle swarm algorithm (QPSO)},
  doi={10.1109/TIM.2022.3169545},
  ISSN={1557-9662},
  month={},}@ARTICLE{10511280,
  author={Chen, Zhen and Laili, Yuanjun and Zhang, Lin and Wang, Ling},
  journal={IEEE Internet of Things Journal}, 
  title={A Trans-Ptr-Nets-Based Transfer Optimization Method for Multiobjective Flexible Job-Shop Scheduling in IIoT}, 
  year={2024},
  volume={11},
  number={14},
  pages={25382-25393},
  abstract={Industrial Internet of Things (IIoT) is considered an emerging infrastructure for enhancing manufacturing efficiency by facilitating the sharing of resources across multiple factories. With increasing requirements on customized production in IIoT, the tasks and objectives of the flexible job-shop scheduling problem for different orders vary greatly, leading to repetitive algorithm adjustment and time-consuming solver invocation. To accelerate the efficiency for the production orders in different scheduling scenarios, this article proposed a transfer optimization method based on pointer networks improved by transformer (Trans-Ptr-Nets). A historical solution selection strategy accompanied with a historical solution data set to retrieve solutions similar to the current scenario are established. Then, the Trans-Ptr-Nets is designed to transfer the candidate solutions to new solutions that are feasible to the target scheduling scenario. Subsequently, the new solutions are introduced as the additional new population of evolutionary algorithm to accelerate the optimization process. Experimental results conducted on four transfer scenarios show that the proposed method can realize at most 50% reduction in running time while improving the solution quality by at least 10%, compared with six typical evolutionary algorithms and three typical transfer learning networks.},
  keywords={Job shop scheduling;Task analysis;Optimization;Industrial Internet of Things;Costs;Manufacturing;Energy consumption;Flexible job-shop scheduling problem;Industrial Internet of Things (IIoT);transfer optimization},
  doi={10.1109/JIOT.2024.3395296},
  ISSN={2327-4662},
  month={July},}@INPROCEEDINGS{9982147,
  author={Sur, Lingfeng and Tang, Chen and Niu, Yaru and Sachdeva, Enna and Choi, Chiho and Misu, Teruhisa and Tomizuka, Masayoshi and Zhan, Wei},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Domain Knowledge Driven Pseudo Labels for Interpretable Goal-Conditioned Interactive Trajectory Prediction}, 
  year={2022},
  volume={},
  number={},
  pages={13034-13041},
  abstract={Motion forecasting in highly interactive scenarios is a challenging problem in autonomous driving. In such scenarios, we need to accurately predict the joint behavior of interacting agents to ensure the safe and efficient navigation of autonomous vehicles. Recently, goal-conditioned methods have gained increasing attention due to their advantage in performance and their ability to capture the multimodality in trajec-tory distribution. In this work, we study the joint trajectory prediction problem with the goal-conditioned framework. In particular, we introduce a conditional-variational-autoencoder-based (CVAE) model to explicitly encode different interaction modes into the latent space. However, we discover that the vanilla model suffers from posterior collapse and cannot induce an informative latent space as desired. To address these issues, we propose a novel approach to avoid KL vanishing and induce an interpretable interactive latent space with pseudo labels. The proposed pseudo labels allow us to incorporate domain knowledge on interaction in a flexible manner. We motivate the proposed method using an illustrative toy example. In addition, we validate our framework on the Waymo Open Motion Dataset with both quantitative and qualitative evaluations.},
  keywords={Navigation;Toy manufacturing industry;Predictive models;Trajectory;Space exploration;Behavioral sciences;Forecasting},
  doi={10.1109/IROS47612.2022.9982147},
  ISSN={2153-0866},
  month={Oct},}@ARTICLE{10183835,
  author={Jiao, Siyu and Goel, Vidit and Navasardyan, Shant and Yang, Zongxin and Khachatryan, Levon and Yang, Yi and Wei, Yunchao and Zhao, Yao and Shi, Humphrey},
  journal={IEEE Transactions on Image Processing}, 
  title={Collaborative Content-Dependent Modeling: A Return to the Roots of Salient Object Detection}, 
  year={2023},
  volume={32},
  number={},
  pages={4237-4246},
  abstract={Salient object detection (SOD) aims to identify the most visually distinctive object(s) from each given image. Most recent progresses focus on either adding elaborative connections among different convolution blocks or introducing boundary-aware supervision to help achieve better segmentation, which is actually moving away from the essence of SOD, i.e., distinctiveness/salience. This paper goes back to the roots of SOD and investigates the principles of how to identify distinctive object(s) in a more effective and efficient way. Intuitively, the salience of one object should largely depend on its global context within the input image. Based on this, we devise a clean yet effective architecture for SOD, named Collaborative Content-Dependent Networks (CCD-Net). In detail, we propose a collaborative content-dependent head whose parameters are conditioned on the input image’s global context information. Within the content-dependent head, a hand-crafted multi-scale (HMS) module and a self-induced (SI) module are carefully designed to collaboratively generate content-aware convolution kernels for prediction. Benefited from the content-dependent head, CCD-Net is capable of leveraging global context to detect distinctive object(s) while keeping a simple encoder-decoder design. Extensive experimental results demonstrate that our CCD-Net achieves state-of-the-art results on various benchmarks. Our architecture is simple and intuitive compared to previous solutions, resulting in competitive characteristics with respect to model complexity, operating efficiency, and segmentation accuracy.},
  keywords={Convolution;Task analysis;Head;Feature extraction;Decoding;Transformers;Object detection;Salient object detection;content-dependent modeling},
  doi={10.1109/TIP.2023.3293759},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{10248008,
  author={Lu, Yi-Chen and Chan, Wei-Ting and Guo, Deyuan and Kundu, Sudipto and Khandelwal, Vishal and Lim, Sung Kyu},
  booktitle={2023 60th ACM/IEEE Design Automation Conference (DAC)}, 
  title={RL-CCD: Concurrent Clock and Data Optimization using Attention-Based Self-Supervised Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Concurrent Clock and Data (CCD) optimization is a well-adopted approach in modern commercial tools that resolves timing violations using a mixture of clock skewing and delay fixing strategies. However, existing CCD algorithms are flawed. Particularly, they fail to prioritize violating endpoints for different optimization strategies correctly, leading to flow-wise globally sub-optimal results. In this paper, we overcome this issue by presenting RL-CCD, a Reinforcement Learning (RL) agent that selects endpoints for useful skew prioritization using the proposed EP-GNN, an endpoint-oriented Graph Neural Network (GNN) model, and a Transformer-based self-supervised attention mechanism. Experimental results on 19 industrial designs in 5 − 12nm technologies demonstrate that RL-CCD achieves up to 64% Total Negative Slack (TNS) reduction and 66.5% number of violating endpoints (NVE) improvement over the native implementation of a commercial tool.},
  keywords={Charge coupled devices;Design automation;Reinforcement learning;Transformers;Graph neural networks;Delays;Optimization},
  doi={10.1109/DAC56929.2023.10248008},
  ISSN={},
  month={July},}@INPROCEEDINGS{10611768,
  author={Yang, Xiaoying and Gu, Yujing and Jia, Fuhua and Li, Yiran and Wang, Hongru and Du, Nanjiang and Cui, Tianxiang and Ye, Yujian and Bai, Ruibin},
  booktitle={2024 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Evolution-Assisted Deep Reinforcement Learning for Fast Charging Station Coordinated Operation}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={The shift towards transportation electrification, marked by the rising use of electric vehicles (EVs) and the development of fast charging stations (FCS), plays a crucial role in transport decarbonization initiatives. To optimize the rollout of FCS and set appropriate charging service fees (CSF)-a process referred to as the coupled FCS multi-stage bi-level operation problem (FCS-MBOP)-is essential for improving both investment and operational efficiency within the integrated power distribution and transportation network (CPTN). For operators, it's not only necessary to adapt to short-term fluctuations within the environment but also to swiftly respond to changes in the FCS layout resulting from various long-term investment decisions. To address this complexity, we introduce a dual-timescale evolutionary assist deep reinforcement learning framework, which includes two specialized agents with distinct functions: an investment agent (planner) and an operational agent (operator). The planner focuses on annual investments, evolving long-term strategies that weigh social benefits against investment costs through the use of a genetic algorithm (GA). In contrast, the operator acts on an hourly basis, fine-tuning CSF to alleviate traffic congestion and minimize the social costs, while taking into account the planner's feasible investment decisions. Leveraging the integrated capabilities of a graph neural network (GNN), long-short-term memory (LSTM), and attention mechanisms, our framework's agents are adept at extracting both temporal and spatial features and facilitating the transfer of experiences across different investment stages. Empirical evidence underscores the effectiveness of our approach, showcasing its ability to surpass conventional methodologies in delivering high-quality solutions.},
  keywords={Training;Costs;Transportation;Feature extraction;Deep reinforcement learning;Graph neural networks;Fast charging;Evolution-assisted deep reinforcement learning;Fast charging station;Coordinated operation;Charging pricing},
  doi={10.1109/CEC60901.2024.10611768},
  ISSN={},
  month={June},}@INPROCEEDINGS{9870410,
  author={Peng, Wei and Hu, Yue and Xie, Yuqiang and Xing, Luxi and Sun, Yajing},
  booktitle={2022 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={CogIntAc: Modeling the Relationships between Intention, Emotion and Action in Interactive Process from Cognitive Perspective}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Intention, emotion and action are important psychological factors in human activities, which play an important role in the interaction between individuals. How to model the interaction process between individuals by analyzing the relationship of their intentions, emotions, and actions at the cognitive level is challenging. In this paper, we propose a novel cognitive framework of individual interaction. The core of the framework is that individuals achieve interaction through external action driven by their inner intention. Based on this idea, the interactions between individuals can be constructed by establishing relationships between the intention, emotion and action. Furthermore, we conduct analysis on the interaction between individuals and give a reasonable explanation for the predicting results. To verify the effectiveness of the framework, we reconstruct a dataset and propose three tasks as well as the corresponding baseline models, including action abduction, emotion prediction and action generation. The novel framework shows an interesting perspective on mimicking the mental state of human beings in cognitive science.},
  keywords={Analytical models;Computational modeling;Psychology;Evolutionary computation;Predictive models;Cognitive science;Task analysis;action abduction;emotion prediction;action generation;interaction},
  doi={10.1109/CEC55065.2022.9870410},
  ISSN={},
  month={July},}@INPROCEEDINGS{10030209,
  author={Risser-Maroix, Olivier and Chamand, Benjamin},
  booktitle={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={What can we Learn by Predicting Accuracy?}, 
  year={2023},
  volume={},
  number={},
  pages={2389-2398},
  abstract={This paper seeks to answer the following question: "What can we learn by predicting accuracy?". Indeed, classification is one of the most popular tasks in machine learning, and many loss functions have been developed to maximize this non-differentiable objective function. Unlike past work on loss function design, which was guided mainly by intuition and theory before being validated by experimentation, here we propose to approach this problem in the opposite way: we seek to extract knowledge by experimentation. This data-driven approach is similar to that used in physics to discover general laws from data. We used a symbolic regression method to automatically find a mathematical expression highly correlated with a linear classifier’s accuracy. The formula discovered on more than 260 datasets of embeddings has a Pearson’s correlation of 0.96 and a r2 of 0.93. More interestingly, this formula is highly explainable and confirms insights from various previous papers on loss design. We hope this work will open new perspectives in the search for new heuristics leading to a deeper understanding of machine learning theory.},
  keywords={Computer vision;Correlation;Pipelines;Machine learning;Feature extraction;Linear programming;Task analysis;Algorithms: Machine learning architectures;formulations;and algorithms (including transfer);Explainable;fair;accountable;privacy-preserving;ethical computer vision},
  doi={10.1109/WACV56688.2023.00242},
  ISSN={2642-9381},
  month={Jan},}@ARTICLE{10616190,
  author={Gong, Heyan and Liu, Jiefeng and Jiang, Zaijun and Zhou, Jiacheng and Fan, Xianhao and He, Dongxin},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={A Time-Domain Fractional Element Model for Aging Condition Analysis of Hot-Spot Region Insulation in Power Transformer}, 
  year={2024},
  volume={73},
  number={},
  pages={1-11},
  abstract={Power transformers play a critical role in electric energy transmission, while making the assessment of their aging condition highly significant. By measuring the dielectric response data in the hot-spot region of transformer oil-paper insulation, their aging condition can be effectively analyzed. However, the occurrence of non-Debye phenomena during time-domain dielectric response measurements severely impacts the accuracy of aging condition analysis. In response, this article constructs a time-domain fractional element (TFE) model to analyze these non-Debye deviations. The model combines the principles of fractional-order calculus and extended Debye (ED) model, providing an explanation for the occurrence of errors. Then the genetic algorithm (GA) has been applied to optimize the polarization and depolarization current (PDC) data in hot-spot region to obtain TFE model parameters. The final results show that the model can well verify the occurrence of wavefront exponent peaks and long power-law tail phenomena and enhance the fitting accuracy in PDC curve. Meanwhile, the relationship model of parameters with variations in test temperature and aging degree is established, enabling more accurate analysis for the aging condition of hot-spot region insulation in transformer when compared with the extended Debye (ED) model.},
  keywords={Aging;Power transformer insulation;Oil insulation;Dielectrics;Temperature measurement;Analytical models;Data models;Fractional-order calculus;genetic algorithm (GA);non-Debye dielectric relaxation;polarization and depolarization current (PDC);transformer oil–paper insulation},
  doi={10.1109/TIM.2024.3436118},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10578425,
  author={Pundir, Sumit and Murugan, V. G. and Raman, P. and Rameshkumaar, V.P. and R, Jahnavi. and Sudharsan, P.},
  booktitle={2024 5th International Conference on Recent Trends in Computer Science and Technology (ICRTCST)}, 
  title={Automatic Stock Price Prediction and Classification Based on Hybrid with AI Feature Selection Method}, 
  year={2024},
  volume={},
  number={},
  pages={149-154},
  abstract={In this research, we investigate the problem of automatically predicting and classifying stock prices, with an eye towards creating and testing a Hybrid AI Feature Selection Method. This work employs a fictitious dataset to offer a new method that integrates Genetic Algorithm (GA) and Recursive Feature Elimination (RFE) to isolate the most important characteristics for predicting stock prices and classifying market fluctuations. The findings demonstrate that the hybrid strategy is effective in reducing the complexity of features and greatly improving model performance over more conventional methods. Furthermore, a simulation of a trading strategy based on the categorization findings reveals its potential to produce more efficient and successful investment methods, highlighting the practical relevance of this study. This research contributes to the developing field of financial technology by laying the groundwork for a new way of thinking about financial prediction and decision making, giving professionals and investors access to cutting-edge resources that can help them make better, more profitable choices.},
  keywords={Computer science;Fluctuations;Decision making;Predictive models;Feature extraction;Market research;Artificial intelligence;Hybrid AI Feature Selection;Stock Price Prediction;Recursive Feature Elimination;Investment Strategies;Trading Strategies;Classification;Financial Technology;Feature Selection;Predictive Modeling;Explainable AI;Machine Learning},
  doi={10.1109/ICRTCST61793.2024.10578425},
  ISSN={},
  month={April},}@INPROCEEDINGS{10226039,
  author={Yumlembam, Rahul and Issac, Biju and Yang, Longzhi and Jacob, Seibu Mary},
  booktitle={IEEE INFOCOM 2023 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={Android Malware Classification and Optimisation Based on BM25 Score of Android API}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={With the growth of Android devices, there is a rise in malware applications affecting these networked devices. Android malware classification is an important task in ensuring the security and privacy of Android devices. One promising approach to this problem is to capture the difference in the usage of API in benign and malware applications through the BM25 (Best Matching 25) scoring function by calculating the BM25 score of each API (Application Program Interface). A linear regression model is fitted using the BM25 score to select the 1000 most important APIs using the feature importance weight of the linear regression model. The selected API's BM25 score and the Permission and Intents of an application are used to train Naive Bayes, Random Forest, Decision Tree, Support Vector Machine, and CNN (Convolutional Neural Network) for classification. To illustrate the effectiveness of using the BM25 score of APIs for malware classification, we train the optimised Particle Swarm Optimisation (PSO) based Machine learning and Deep Learning algorithms using Permission and Intents features with and without the BM25 score. Experiments show that the BM25 score improves the result. Overall, this study demonstrates the potential of using the BM25 score of API calls, in combination with Permissions and Intents, as a valuable tool for Android malware classification.},
  keywords={Support vector machines;Privacy;Operating systems;Linear regression;Malware;Convolutional neural networks;Security;Android Malware;Optimisation;Machine Learning;Convolutional Neural Network;Android API},
  doi={10.1109/INFOCOMWKSHPS57453.2023.10226039},
  ISSN={2833-0587},
  month={May},}@INPROCEEDINGS{10348294,
  author={Zhao, Aihua and Zhu, Xin and Du, Xin and Wang, Wenwen and Ma, Wenqing and Wang, Shixuan},
  booktitle={2023 IEEE 4th International Conference on Pattern Recognition and Machine Learning (PRML)}, 
  title={Deep Learning for Diagnosis of Endometrial Cancer and Atypical Endometrial Hyperplasia}, 
  year={2023},
  volume={},
  number={},
  pages={307-312},
  abstract={Endometrial cancer(EC) is the most common and rapidly increasing female cancer globally. Atypical endometrial hyperplasia (AEH) is a precancerous condition of EC. Although hysteroscopy serves as the primary modality for diagnosing lesions, it relies on the subjective judgment of hysteroscopists. Therefore, this study proposed a computer-aided diagnostic system utilizing the EfficientNet network as a baseline, incorporating ParNet attention mechanism and class weighting to accurately classify EC/AEH from benign lesions. This study included 49,556 hysteroscopy images from 1,237 cases as a training set and 3,412 hysteroscopy images from 85 cases as a testing set. AUC, accuracy, sensitivity, specificity, PPV, Kappa, and F1-Score of the proposed method are 0.941, 89.4%, 93.7%, 87.1%, 73.3%, 0.755, and 0.8225, respectively. The proposed model may be used as a computer-aided tool for the diagnosis of EC/AEH.},
  keywords={Training;Deep learning;Sensitivity;Computational modeling;Pattern recognition;Lesions;Cancer;Hysteroscopy;deep learning;endometrial cancer;atypical endometrial hyperplasia;EfficientNet},
  doi={10.1109/PRML59573.2023.10348294},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10356565,
  author={Cao, Ruihao and Ma, Zhirou and Kang, Liangyi and Wang, Shuai and Liu, Jie},
  booktitle={2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={CSTCN: A Novel Causal-Based Framework for Air Quality Medium- and Long-term Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={970-977},
  abstract={Modeling spatial and temporal dependencies is essential for achieving accurate air quality prediction. Current air quality prediction models often overlook the underlying causal relationships in the data and primarily focus on statistical correlations. As a result, these models lack sufficient predictive power for medium- and long-term forecasts. This paper introduces causality into air quality prediction and proposes a Causal Spatio-Temporal Convolutional Network (CSTCN). We utilize an attention mechanism to automatically assign attention weights to each air quality monitoring site, enabling the capture of causal relationships between sites in the spatial dimension. Conducting tests on the identified relationships ensures the causality of the data in space. Furthermore, convolution operations are applied to extract the spatio-temporal features of the monitoring stations, while also utilizing causal convolution to ensure the causality of the data over time. The experiments conducted on the Beijing air quality dataset demonstrate that CSTCN exhibits outstanding performance in medium- and long-term predictions.},
  keywords={Correlation;Convolution;Atmospheric modeling;Predictive models;Air quality;Feature extraction;Information leakage;air quality prediction;causality;spatio-temporal dependency;attention mechanism;causal convolutional network},
  doi={10.1109/ICTAI59109.2023.00145},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10364177,
  author={Moharreri, Sadaf and Rezaei, Shahab and Dabanloo, Nader Jafarnia and Parvaneh, Saman},
  booktitle={2023 Computing in Cardiology (CinC)}, 
  title={Study of Traditional and Enhanced Poincare Plot Descriptors for Atrial Fibrillation Detection}, 
  year={2023},
  volume={50},
  number={},
  pages={1-4},
  abstract={This article studies extracted from the Poincare Plot of RR intervals for atrial fibrillation (AF) detection. This study used AF, normal sinus rhythm (NSR), and other rhythms from the PhysioNet/Computing in Cardiology Challenge 2017 training dataset (NSR: 5,074, AF: 757, other rhythms: 2,415, and noise: 279). After QRS detection, 2D and 3D Poincare plots of RR intervals were constructed. Two traditional features from the 2D Poincare plot and seventeen geometric features extracted from the 3D Poincare plot were calculated. AutoML was used to find the best classifier, maximizing the f1-score. AutoML was trained on 80% of the data as a train set, and the f1-score was evaluated on 20% as a test set. Catboost was selected as a final model, which led to the f1-score of 0.87, 0.6, and 0.63 for NSR, AF, and other rhythms, respectively. Using AutoML with extracted geometric features facilitates finding the best model.},
  keywords={Training;Three-dimensional displays;Computational modeling;Atrial fibrillation;Feature extraction;Rhythm;Cardiology},
  doi={10.22489/CinC.2023.397},
  ISSN={2325-887X},
  month={Oct},}@INPROCEEDINGS{10363962,
  author={An, Junmo and Bailey, Ben and Gregg, Richard E},
  booktitle={2023 Computing in Cardiology (CinC)}, 
  title={Paradigm Shift from Feature-Based Machine Learning to End-to-End Deep Residual Neural Networks for Pediatric Age Classification from 12-Lead ECG}, 
  year={2023},
  volume={50},
  number={},
  pages={1-4},
  abstract={ECG criteria vary significantly by age in early years of childhood. Accurate knowledge of age is vital to select appropriate ECG criteria for the patient. Two machine learning and deep learning models were trained on three different inputs from a dataset of 151, 725 recordings of 12-lead ECGs. The feature-based machine learning model utilizing multi-layer perceptron (MLP) was trained with handcrafted features. The end-to-end residual network (ResNet) model received two inputs: a 10-second raw ECG (RawECG) signal and a 1.2-second average representative beat (RepBeat) signal. The ResNet was trained with four data augmentation techniques using RandAugment. The feature-based MLP achieved an F1 score of 0.72 for pediatric detection (age < 16 years) and an average F1 score of 0.65 for classifying pediatric groups (neonate, infant, child, and adolescent) and adult. The end-to-end ResNet achieved an F1 score of 0.88 for pediatric detection from RawECG input and an F1 score of 0.82 from RepBeat input. The model also achieved an average F1 score of 0.78 for classifying pediatric groups and adult from RawECG input, and an F1 score of 0.74 from RepBeat input. The proposed end-to-end ResNet achieves the highest F1 score and outperforms the feature-based MLP, making it the best choice for pediatric age classification.},
  keywords={Deep learning;Pediatrics;Electrocardiography;Feature extraction;Data augmentation;Recording;Cardiology},
  doi={10.22489/CinC.2023.376},
  ISSN={2325-887X},
  month={Oct},}@ARTICLE{11045817,
  author={Liu, Jun and Wu, Mengyuan and Lin, Mingwei and Xu, Zeshui},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={Anisotropic Gaussian Kernel-Based Fuzzy Clustering Algorithm for Feature Selection}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Soft clustering algorithms based on fuzzy C-means (FCM) have been extensively applied to complex data analysis. However, existing FCM variants still encounter key limitations: a large number of iterations due to slow convergence on highdimensional data, equal weights of all samples which increases sensitivity to noise, and strong dependence on empirically chosen fuzzy parameters, often resulting in suboptimal solutions. In order to address these challenges, in this work, we propose an adaptive FCM clustering algorithm based on anisotropic Gaussian kernel function (AGK-FCM for short), which facilitates the process of feature selection for multidimensional data with fewer iterations after feature reduction based on updating the kernel width vector. In order to enhance the classification accuracy, we assign adaptive weights to each sample and adjust these weights to mitigate the impact of outliers on classification accuracy. Unlike traditional fuzzy clustering algorithms, we employ the PSO-TVAC algorithm to determine the global optimal parameters, thus reducing the computational resources and enhancing the algorithm's efficiency. The experimental results based on 16 publicly available datasets validate that the proposed algorithm achieves higher classification accuracy with minimal number of iterations.},
  keywords={Kernel;Clustering algorithms;Anisotropic;Feature extraction;Accuracy;Linear programming;Classification algorithms;Noise;Machine learning algorithms;Vectors;Fuzzy C-means (FCM);Anisotropic Gaussian kernel (AGK);Feature reduction;Adaptive weights;PSO-TVAC algorithm},
  doi={10.1109/TFUZZ.2025.3581918},
  ISSN={1941-0034},
  month={},}@INPROCEEDINGS{11077557,
  author={Vayyala, Rajesh},
  booktitle={2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)}, 
  title={Ethical AI and Analytics in Cloud-Based Data Ecosystems}, 
  year={2025},
  volume={},
  number={},
  pages={264-268},
  abstract={As the reliance of organizations on cloud-based data ecosystems to drive their AI and analytics initiatives increases, ethical considerations with regard to these technologies come to the fore. This research uses the highly regarded [insert specific dataset name, e.g., “Google Cloud Public Datasets”] to explore how ethical issues related to data privacy, algorithmic fairness, transparency, and accountability are addressed in cloud environments. We identify the main challenges and best practices for implementing ethical AI solutions in the cloud by qualitative analysis and empirical evaluation. The findings underline the need for robust governance frameworks that create a culture of responsibility to make sure AI-driven analytics deliver valuable insights while upholding societal values and protecting individual rights. By paying attention to these ethical dimensions, organizations can build more trustworthy and sustainable data ecosystems that benefit business and the greater community.},
  keywords={Ethics;Cloud computing;Differential privacy;Technological innovation;Ecosystems;Organizations;Prediction algorithms;Solids;Artificial intelligence;Convergence;Ethical AI;Cloud Computing;Data Privacy;Algorithmic Fairness;Transparency},
  doi={10.1109/AIRC64931.2025.11077557},
  ISSN={},
  month={May},}@INPROCEEDINGS{10236637,
  author={Zhang, Binyan},
  booktitle={2023 International Conference on Networking, Informatics and Computing (ICNETIC)}, 
  title={Research on Multimodal Visual Saliency Detection Based on BP Neural Network Algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={400-404},
  abstract={When the number of pictures or video signals to be processed by the computer reaches hundreds of millions, the useless energy consumed will be greatly increased. Therefore, how to make the computer automatically pay attention to and only deal with the areas of human interest is of great significance to improve the efficiency of computer vision tasks. In this paper, the research of multimodal visual saliency detection based on BPNN (BP neural network) algorithm is carried out. Establish a BPNN algorithm based on GA (genetic algorithm) optimization. Using GA to roughly search out a certain weight range, and taking the weight at this time as the initial weight of BPNN can improve the shortcomings of BPNN, such as easily falling into local minimum, slow convergence speed and causing oscillation effect. The research results show that the proposed method has excellent performance in multimodal visual saliency detection, with precision, recall and F value reaching 0.8207, 0.8455 and 0.7935 respectively. The conclusion shows that this model can detect prominent targets more accurately than other methods, which proves the effectiveness of the algorithm and can adaptively fuse visible and thermal infrared information.},
  keywords={Visualization;Adaptation models;Computational modeling;Neural networks;Visual systems;Task analysis;Optimization;BP neural network;Multimodal;Visual saliency detection},
  doi={10.1109/ICNETIC59568.2023.00089},
  ISSN={},
  month={May},}@INPROCEEDINGS{11042974,
  author={Nikolikj, Ana and Muñoz, Mario Andrés and Tuba, Eva and Eftimov, Tome},
  booktitle={2025 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper leverages the recently introduced concept of algorithm footprints to investigate the interplay between algorithm configurations and problem characteristics. Performance footprints are calculated for six modular variants of the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the BBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional. These footprints provide insights into why different configurations of the same algorithm exhibit varying performance and identify the problem features influencing these outcomes. Our analysis uncovers shared behavioral patterns across configurations due to common interactions with problem properties, as well as distinct behaviors on the same problem driven by differing problem features. The results demonstrate the effectiveness of algorithm footprints in enhancing interpretability and guiding configuration choices.},
  keywords={Heuristic algorithms;Evolutionary computation;Benchmark testing;Optimization;single-objective continuous optimization;landscape analysis;algorithm configuration footprint},
  doi={10.1109/CEC65147.2025.11042974},
  ISSN={},
  month={June},}@ARTICLE{10040680,
  author={Gong, Xiaoming},
  journal={IEEE Access}, 
  title={Asymmetric Information Dissemination in Double-Layer Networks Helps Explain the Emergence of Cooperation}, 
  year={2023},
  volume={11},
  number={},
  pages={13202-13210},
  abstract={This paper proposes a double-layer network game model based on asymmetric information, hoping to explore the impact of asymmetric information dissemination on the evolution of cooperation. The model assumes that agents in heterogeneous states play the Prisoner’s Dilemma game in the network’s physical layer, and heterogeneous information disseminates asymmetrically in the virtual layer of the network. Through mean-field theory analysis and Monte Carlo experiments, we found that the dissemination of asymmetric information significantly impacts cooperation. The positive information generated by the defector can promote cooperation in the short term but will hinder cooperation in the long term. Positive information generated by cooperators can promote cooperation in the long run. The system’s final state depends on the relative intensity of the two kinds of information dissemination. Asymmetric information dissemination can promote cooperation because heterogeneous information has distinct dissemination intensities, which makes the number of active agents around the agents different. The positive information generated by cooperators can attract more active agents in the long run, thus obtaining more payoffs, making the agents in the system tend to cooperate. The positive information generated by defectors produces more silent neighbours in the long run, thus reducing the overall payoffs, which makes the agents in the system tend to defect. This paper provides a new explanation for the emergence of cooperation, which helps expand the existing research field.},
  keywords={Games;Physical layer;Statistics;Cooperative systems;Particle measurements;Information processing;Economics;Evolutionary computation;Social factors;Evolutionary computation;cooperative systems;complexity theory;social engineering},
  doi={10.1109/ACCESS.2023.3243461},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11053041,
  author={Ramkumar, Gautham and L A, Hariharan and A, Aishwariya and K, Prashanthini and M S, Sureshkumar},
  booktitle={2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)}, 
  title={Rocker-Bogie Mechanisms for Stair-Climbing Robots: A Comprehensive Review of Design, Optimization, and Control}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents an overview of rocker-bogie mechanisms for stair-climbing robots. First conceived by NASA for Mars rover missions, these mechanisms have a lot of potential for applications on Earth related to mobility over rough terrain. We discuss design requirements for stair-climbing, such as geometric considerations, stability constraints, and traction optimization. Optimization methods for mechanisms are discussed, with focus on Taguchi methods, genetic algorithms, and simulation modeling. We supply kinematic and dynamic analysis necessary to the explanation of stair-climbing motion features and force interaction. The article covers control strategies from elementary feedback systems through complex autonomous navigation and sensory demands for terrain adaptability. Limitations existing presently, such as excessive torque demand and restriction on uneven stairs, are discussed, including recent innovations in materials, processes, and hybrid configurations. Examples of search and rescue case studies, assistive equipment, and logistics use show real-world applications. Lastly, potential future research directions such as AI integration, energy-harvesting designs, and novel configurations are mentioned. This survey offers engineers designing mobile robotic platforms for staired spaces with helpful insights.},
  keywords={Space vehicles;Torque;Kinematics;Stairs;Robot sensing systems;Stability analysis;Mobile robots;Robots;Genetic algorithms;Design optimization;rocker-bogie mechanism;stair-climbing robots;passive suspension;design optimization;Taguchi method;genetic algorithms;kinematic analysis;dynamic analysis;autonomous navigation;robotics control;terrain adaptation;mobility systems},
  doi={10.1109/ICCRTEE64519.2025.11053041},
  ISSN={},
  month={May},}@INPROCEEDINGS{11077560,
  author={Kumar, Vimlesh},
  booktitle={2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)}, 
  title={QwinSR: A Simplified MLP-Based Super-Resolution Model Integrating Swin-Mixer and SwinIR Architectures}, 
  year={2025},
  volume={},
  number={},
  pages={457-462},
  abstract={Despite advances in image processing, resolution loss remains a common challenge, creating a demand for superresolution (SR) techniques that reconstruct high-fidelity images from lower-resolution sources. This paper introduces QwinSR, a novel hybrid model for single-image super-resolution, which leverages the shifted window approach from Swin Transformer and the all-MLP design philosophy. QwinSR combines the shallow feature extraction capabilities of convolutional layers with the deep feature extraction of Swin-Mixer blocks, a derivative of the Swin Transformer that substitutes self-attention with multilayer perceptrons (MLPs). While focusing on simplicity and efficiency, QwinSR aims to achieve competitive performance with state-of-the-art SR models by utilizing residual connections and convolutional layers in its architecture. Our proposed model is evaluated through a series of ablation studies, where we analyze the impact of various hyperparameters, such as channel numbers, patch sizes, and the presence of residual connections, on the peak signal-to-noise ratio (PSNR) of upscaled images. Due to resource constraints, a full experimental benchmark was not conducted; however, the potential of QwinSR is discussed in relation to existing benchmarks in SR literature. We conclude by outlining future work, including formal testing on larger GPU clusters and further architectural enhancements, to refine the performance and efficiency of QwinSR in real-world applications.},
  keywords={PSNR;Philosophical considerations;Convolution;Superresolution;Benchmark testing;Multilayer perceptrons;Transformers;Feature extraction;Robots;Image reconstruction;Super-resolution;Swin Transformer;MLPbased architecture;image enhancement;deep learning},
  doi={10.1109/AIRC64931.2025.11077560},
  ISSN={},
  month={May},}
