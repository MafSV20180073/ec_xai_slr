@ARTICLE{10473037,
  author={Jin, Cong and Huang, Jinjie},
  journal={IEEE Access}, 
  title={Inner Loop-Based Modified Differentiable Architecture Search}, 
  year={2024},
  volume={12},
  number={},
  pages={41918-41933},
  abstract={Differentiable neural architecture search, which significantly reduces the computational cost of architecture search by several orders of magnitude, has become a popular research issue in recent years. Architecture search can fundamentally be described as an optimization problem. The differentiable architecture search updates the search process based on gradients, then derives the final sub-network architecture from the super network of the search space. However, the gap between the super network and its sub-networks together with the inaccuracy of the gradient approximation during architecture optimization bring performance collapse problems in the architecture search, making the search process extremely unstable. To this end, we propose an inner loop-based modified differentiable neural architecture search method (InLM-NAS). Firstly, we redefine the objective function of the architecture optimization process in the search process by introducing an inner-loop mechanism to prevent overfitting problems of architecture parameters and avoid convergence of the architecture search to suboptimal architectures. Secondly, a novel approximation calculation is introduced in the architecture optimization process, which reduces the error caused by the gradient approximation. It alleviates the sensitivity to the hyper-parameters setting during the architecture search and enhances the stability of the architecture search. Finally, extensive validation experiments on public datasets demonstrate that our proposed method has a more robust search process, and the searched neural network architecture has a superior network performance.},
  keywords={Computer architecture;Optimization;Search problems;Neural networks;Network architecture;Evolutionary computation;Convergence;Deep learning;Costs;Computational modeling;Neural network;differentiable architecture search;deep learning;implicit regularization},
  doi={10.1109/ACCESS.2024.3377888},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10290159,
  author={Ainapur, Santoshkumar S and Virupakshappa, Virupakshappa and Veerashetty, Sachinkumar S},
  booktitle={2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Enhancing Diabetic Retinopathy Detection with an Innovative ACSE-CLF Algorithm: A Deep Learning Approach}, 
  year={2023},
  volume={},
  number={},
  pages={758-765},
  abstract={Diabetic Retinopathy (DR) is an increasingly prevalent consequence of diabetes that can cause vision impairment if left untreated. Early identification is the key to effective DR therapy, hence it is crucial to identify DR at an early stage in order to achieve the best results. Conventional screening techniques, however, demand specialised equipment, are imprecise, time-consuming, and inaccurate. An novel Attention Convolutional Squeeze Excitation-based Chaotic Levy Flight (ACSE-CLF) technique is thus proposed for the precise identification of DR. This study emphasises possible impacts associated with using the proposed ACSE-CLF algorithm with data pre-processing, feature extraction, and classification for recognising DR. The proposed ACSE-CLF method differentiates between DR and healthy images, improving screening precision. The performance has been evaluated using Kaggle datasets, which also included patient retinal images. These retinal images served as the proposed model’s input parameters, and achieved an accuracy rate of 98.7% for DR recognition.},
  keywords={Training;Diabetic retinopathy;Solid modeling;Image recognition;Visual impairment;Metaheuristics;Medical treatment;Convolution Neural Network;attention mechanism;chaotic levy flight distribution;retinal image;Diabetic Retinopathy},
  doi={10.1109/I-SMAC58438.2023.10290159},
  ISSN={2768-0673},
  month={Oct},}@INPROCEEDINGS{10394547,
  author={Jiang, Wenxiang and Xu, Lihong},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Surrogate-Assisted Evolutionary Optimization Based on Interpretable Convolution Network*}, 
  year={2023},
  volume={},
  number={},
  pages={542-547},
  abstract={When performing evolutionary optimization for computationally expensive objective, surrogate-assisted evolutionary algorithm(SAEA) is an effective approach. However, due to the limited availability of data in these scenarios, it can be challenging to create a highly accurate surrogate model, leading to reduced optimization effectiveness. To address this issue, we propose an Interpretable Convolution Network(ICN) for offline surrogate-assited evolutionary optimization. ICN retains the non-linear expression ability of traditional neural networks, while possessing the advantages of clear physical structure and the ability to incorporate prior knowledge during network parameter design and training process. We compare ICN-SAEA with tri-training method(TT-DDEA) and model-ensemble method(DDEA-SA) in several benchmark problems. Experimental results show that ICN-SAEA is better in searching optimal solution than compared algorithms.},
  keywords={Knowledge engineering;Training;Convolution;Search problems;Mathematical models;Structural engineering;Optimization},
  doi={10.1109/SMC53992.2023.10394547},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{10677482,
  author={Zhou, Jinhan and Tang, Diyin and Yu, Jinsong and Song, Yue and Li, Xuhui and Liu, Hao},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Construction of Interpretable Health Indicators for System-Level Fault Prognosis Driven by Inductive Generative Learning}, 
  year={2024},
  volume={73},
  number={},
  pages={1-12},
  abstract={The construction of health indicators (HIs) is critical for the system-level fault prognosis. Due to its high interpretability and credible identifiability, the model-driven approaches are more welcome. However, for a complex system, it exists always some intercomponent coupling mechanism among model-driven HIs, which makes the fault representations difficult to distinguish based on these HIs. Therefore, this article proposes a model-driven construction method of interpretable HI driven by inductive generative learning, aiming to optimize the accuracy of the system-level fault prognosis via the decoupling among the model-driven HIs. The inductive architecture attempts to treat automatically the high coupling performance knowledge inside the mathematical model to extract the correspondent identifiable HIs, while the integrated generative architecture realizes an efficient identification of the extracted HIs in order to guarantee a real-time fault prognosis. Experiments on a simplified electrical hydrostatic actuator (EHA) model and a high-quality electromechanical actuator (EMA) model are conducted to demonstrate the state-of-the-art level of the proposed approach on the HI construction for the system-level fault prognosis under intercomponent coupling circumstances.},
  keywords={Prognostics and health management;Fault diagnosis;Mathematical models;Complex systems;Couplings;Circuit faults;Monitoring;Complex system;generative learning;health indicator (HI);inductive learning;model driven;system-level fault prognosis},
  doi={10.1109/TIM.2024.3458064},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10546562,
  author={Luo, Donger and Sun, Qi and Xu, Qi and Chen, Tinghuan and Geng, Hao},
  booktitle={2024 Design, Automation & Test in Europe Conference & Exhibition (DATE)}, 
  title={Attention-Based EDA Tool Parameter Explorer: From Hybrid Parameters to Multi-QoR metrics}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Improving the outcomes of very-large-scale integration design without altering the underlying design enablement, such as process, device, interconnect, and IPs, is critical for integrated circuit (IC) designers. Parameter tuning for electronic design automation (EDA) tools is an emerging technology for improving the final design Quality-of-Result (QoR). However, many complex heuristics have been accreted upon previous complex heuristics integrated into tools, resulting in a vast number of tunable parameters. Even worse, these parameters include both continuous and discrete ones, making the parameter tuning process laborious and challenging. In this paper, we propose an attention-based EDA tool parameter explorer. A self-attention mechanism is developed to navigate the parameter importance. A hybrid space Gaussian process model is leveraged to optimize continuous and discrete parameters jointly, capturing their complex interactions. In addition, considering multiple QoR metrics and the large amount of time required to invoke EDA tools, a customized acquisition function based on expected hypervolume improvement (EHVI) is proposed to enable multi-objective optimization and parallel evaluation. Experimental results on a set of IWLS2005 benchmarks demonstrate the effectiveness and efficiency of our method.},
  keywords={Design automation;Navigation;Integrated circuit interconnections;Gaussian processes;Benchmark testing;Very large scale integration;Parallel processing},
  doi={10.23919/DATE58400.2024.10546562},
  ISSN={1558-1101},
  month={March},}@ARTICLE{10516672,
  author={Bi, Jing and Ma, Haisen and Yuan, Haitao and Buyya, Rajkumar and Yang, Jinhong and Zhang, Jia and Zhou, MengChu},
  journal={IEEE Internet of Things Journal}, 
  title={Multivariate Resource Usage Prediction With Frequency-Enhanced and Attention-Assisted Transformer in Cloud Computing Systems}, 
  year={2024},
  volume={11},
  number={15},
  pages={26419-26429},
  abstract={Resource usage prediction in cloud data centers is critically important. It can improve providers’ service quality and avoid resource wastage and insufficiency. However, the time series of resource usage in cloud environments is characterized by multidimensional, nonlinear, and high-volatility characteristics. Achieving high-accuracy prediction for time series with such characteristics is necessary but difficult. Traditional prediction methods based on regression algorithms and recurrent neural networks cannot effectively extract nonlinear features from data sets. Besides, many deep learning models suffer from gradient explosion or gradient vanishing during the training stage. Current commonly used prediction methods fail to uncover some vital information about the frequency domain features in the time series. To resolve these challenges, we design a Forecasting method based on the Integration of a Savitzky–Golay (SG) filter, a frequency enhanced decomposed transformer (FEDformer) model, and a frequency-enhanced channel attention mechanism (FECAM), named FISFA. It adopts the SG filter to reduce noise and smooth sequences in the raw sequences of resources. Then, we develop a hybrid transformer-based model integrating FEDformer and the FECAM, effectively capturing the frequency domain patterns. Besides, a meta-heuristic optimization algorithm, i.e., genetic simulated annealing-based particle swarm optimizer, is proposed to optimize key hyperparameters of FISFA. Then, FISFA predicts the future needs for multidimensional resources in highly fluctuating traces in real-life cloud environments. Experimental results demonstrate that FISFA achieves higher accuracy and performs more efficient prediction than several benchmark forecasting methods with realistic data sets collected from Alibaba and Google cluster traces. FISFA improves the prediction accuracy on average by 32.14%, 25.49%, and 27.71% over vanilla long short-term memory, transformer, and Informer methods, respectively.},
  keywords={Time series analysis;Transformers;Cloud computing;Predictive models;Forecasting;Feature extraction;Long short term memory;Cloud computing;deep learning;frequency enhancement;Savitzky–Golay (SG) filter;time series prediction},
  doi={10.1109/JIOT.2024.3395610},
  ISSN={2327-4662},
  month={Aug},}@ARTICLE{10104101,
  author={Hou, Yaqing and Sun, Mingyang and Zeng, Yifeng and Ong, Yew-Soon and Jin, Yaochu and Ge, Hongwei and Zhang, Qiang},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A Multiagent Cooperative Learning System With Evolution of Social Roles}, 
  year={2024},
  volume={28},
  number={2},
  pages={531-543},
  abstract={Recent developments in reinforcement learning (RL) have been able to derive optimal policies for sophisticated and capable agents, and shown to achieve human-level performance on a number of challenging tasks. Unfortunately, when it comes to multiagent systems (MASs), complexities, such as nonstationarity and partial observability bring new challenges to the field. Building a flexible and efficient multiagent RL (MARL) algorithm capable of handling complex tasks has to date remained an open challenge. This article presents a multiagent learning system with the evolution of social roles (eSRMA). The main interest is placed on solving the key issues in the definition and evolution of suitable roles, and optimizing the policies accompanied by social roles in MAS efficiently. Specifically, eSRMA incorporates and cultivates role division awareness of agents to improve the ability to deal with complex cooperative tasks. Each agent is assigned a role module, which can dynamically generate roles based on the individuals’ local observations. A novel MARL algorithm is designed as the principal driving force that governs the role-policy learning process by a role-attention credit assignment mechanism. Moreover, a role evolution process is developed to help agents dynamically choose appropriate roles in decision making. Comprehensive experiments on the StarCraft II micromanagement benchmarkhave demonstrated that eSRMA exhibits superiority in achieving higher learning capability and efficiency for multiple agents compared to the state-of-the-art MARL methods.},
  keywords={Task analysis;Decision making;Heuristic algorithms;Behavioral sciences;Training;Sociology;Optimization;Decision making;deep reinforcement learning (RL);evolutionary roles;multiagent systems (MASs)},
  doi={10.1109/TEVC.2023.3268076},
  ISSN={1941-0026},
  month={April},}@ARTICLE{10851274,
  author={Logeswari, G. and Deepika Roselind, J. and Tamilarasi, K. and Nivethitha, V.},
  journal={IEEE Access}, 
  title={A Comprehensive Approach to Intrusion Detection in IoT Environments Using Hybrid Feature Selection and Multi-Stage Classification Techniques}, 
  year={2025},
  volume={13},
  number={},
  pages={24970-24987},
  abstract={The rapid expansion of Internet of Things (IoT) devices has led to an increasingly complex threat landscape, challenging traditional Intrusion Detection Systems (IDS) to effectively handle the vast and diverse data generated by IoT networks. This paper presents a novel IDS that integrates Quantum-Inspired Particle Swarm Optimization (QIPSO) with Adaptive Neuro-Fuzzy Inference System (ANFIS) for optimized feature selection, followed by a multi-stage classification pipeline using Capsule Networks (CapsNets) and Attention Augmented-Recurrent Neural Networks (RNNs). The proposed approach addresses the limitations of existing methods by capturing both hierarchical and temporal dependencies in network traffic, improving the detection of subtle and advanced attacks. Evaluation on the TON-IoT and BOT-IoT datasets demonstrates that the proposed method significantly outperforms state-of-the-art techniques. Specifically, it achieves 98.83% accuracy, 98.56% precision, and 98.65% F-Measure on the TON-IoT dataset, and 98.6% accuracy, 98.5% precision, and 98.94% F-Measure on the BOT-IoT dataset, showcasing its superior performance over existing IDS models. This paper’s key contribution lies in the integration of feature selection and classification techniques tailored for IoT environments, filling a critical gap in the state of the art and offering a more adaptive and efficient solution for real-time intrusion detection.},
  keywords={Internet of Things;Feature extraction;Security;Accuracy;Intrusion detection;Pipelines;Performance evaluation;Computational modeling;Adaptive systems;Adaptation models;Enhanced recurrent neural network;adaptive neuro-fuzzy inference system;capsule networks;Internet of Things;intrusion detection system;quantum-inspired particle swarm optimization},
  doi={10.1109/ACCESS.2025.3532895},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10395809,
  author={Williams, D.R Prince and Srinivas, G. and Bajpai, Ayushman and Ghamande, Manasi Vyankatesh and Dhyani, Bijesh and Pal, Subharun},
  booktitle={2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)}, 
  title={Fine Grained Air Quality Monitoring Based on BiGRU and AM Model for Enhanced Public Health}, 
  year={2023},
  volume={},
  number={},
  pages={235-240},
  abstract={Particles, biological molecules, and other harmful chemicals in the Earth's atmosphere generate air pollution, which in turn causes many negative health impacts, human deaths, animal deaths, agricultural failures, and infrastructure deterioration. Air pollution can be caused by both human actions and natural occurrences. Large-scale industry, which is relocating away from cities, is rarely held responsible for worsening air quality. Recent studies have found that automobile emissions account for the vast majority of urban air pollution. There are three steps to the proposed method: data preprocessing, feature selection, and finally training the model. Preprocessing in data science typically involves k-means and variational mode decomposition. Using the Q-learning-based swarm optimization method, features are chosen. When information gain is used for feature selection, BiGRU-AM is then used to train the model. When compared to GRU and BiGRU, two recognized alternatives, the proposed technique produces better overall results.},
  keywords={Training;Atmospheric modeling;Urban areas;Feature extraction;Air pollution;Particle swarm optimization;Monitoring;Air Quality Index (AQI);Gated Recurrent Unit (GRU);Bidirectional Gated Recurrent Unit (BiGRU)},
  doi={10.1109/ICECA58529.2023.10395809},
  ISSN={},
  month={Nov},}@ARTICLE{10899749,
  author={Haixiao, Cao and Chuanlong, Ding and Liang, Jiang and Yonghong, Zhang and Qi, Liu},
  journal={IEEE Sensors Journal}, 
  title={A Parallel Dual-Branch Deep Learning Framework Integrating VMD Feature Engineering and Multiscale Temporal Feature Fusion for Small-Sample Fault Diagnosis}, 
  year={2025},
  volume={25},
  number={7},
  pages={12479-12495},
  abstract={Rolling bearings are crucial in mechanical systems, where failures pose safety risks and economic losses. However, their rarity makes timely fault detection and data collection challenging. To address this, a parallel dual-branch fault diagnosis model was proposed. One branch applied variational mode decomposition (VMD) with the hybrid particle swarm optimization (HPSO) algorithm to automatically and optimally decompose the raw signals. Then, continuous wavelet transform (CWT) was used on the decomposed modes to create stacked feature maps, which was followed by the normalization with a  ${1} \times {1}$  convolution to accomplish the feature engineering. Next, the improved convolutional neural networks (CNNs) and the bidirectional long short-term memory (BiLSTM) models were used in sequence to extract features from both the frequency and temporal domains. In the other branch, the original signal was downsampled at three different step sizes to capture multiscale temporal representations, and the multilayer perceptrons (MLPs) were employed to learn temporal features from different time scales, which were fused into one feature output thereafter. Eventually, the time-frequency and the temporal features from both branches were concatenated and passed to the output for fault diagnosis. The proposed feature engineering method and the parallel dual-branch fault diagnosis model were evaluated on small datasets containing 20 samples from Case Western Reserve University (CWRU), achieving an impressive 100% accuracy. Moreover, the model was compared with recent mainstream models on the Paderborn (PU) dataset. The results demonstrated that it excelled at learning key features from small samples, highlighting its potential for small-sample rolling bearing fault diagnosis.},
  keywords={Feature extraction;Fault diagnosis;Time-frequency analysis;Noise;Continuous wavelet transforms;Accuracy;Vibrations;Signal resolution;Noise measurement;Data mining;Fault diagnosis;feature engineering;hybrid particle swarm optimization (HPSO);parallel structure;small samples},
  doi={10.1109/JSEN.2025.3542170},
  ISSN={1558-1748},
  month={April},}@ARTICLE{10540124,
  author={Zhang, Xiaoqing and Qi, Qingqing and Liu, Weike},
  journal={IEEE Access}, 
  title={Combined Hybrid Neural Networks and Swarm Intelligence Optimization Algorithms for Photovoltaic Panel Segmentation From Remote Sensing Images}, 
  year={2024},
  volume={12},
  number={},
  pages={75941-75950},
  abstract={In the context of traditional energy shortage and climate warming, the development of solar energy, as a clean and renewable energy, is crucial. As an effective way to utilize solar energy resources, photovoltaic (PV) power generation technology has been widely used around the world. Using remote sensing images to extract PV panel information, including location, area, has a positive effect on understanding the development status, planning and construction of regional PV new energy. In this study, a semantic segmentation network called HCT-Net, combined with the hybrid neural networks and the swarm intelligence optimization algorithms, is designed to segment solar PV panels from remote sensing images automatically and accurately. To address the problem of inconsistent segmentation within PV regions, a hybrid encoder, which combines a convolutional neural network and a Transformer, is designed to extract local features with rich detail information and global features with global context dependencies, resulting in enhanced feature representations. The foreground relation module is designed to solve the problem of mis-segmentation of the background into PVs. This module strengthens the model’s focus on the target object and suppresses the feature representations of non-PVs by explicitly learning the similarity relationship between the global PV feature representation and the feature representations of other objects, and by adaptively assigning weights according to the similarity. The swarm intelligence optimization algorithm is applied to adjust the learning rate and the balance coefficient of the composite loss function of HCT-Net during training. Experimental results show that compared with the current mainstream semantic segmentation network, the method in this study effectively alleviates the problem of inconsistent segmentation within PV regions and mis-segmentation and has advantages in the complete and accurate extraction of PV panels.},
  keywords={Feature extraction;Optimization;Particle swarm optimization;Transformers;Semantics;Photovoltaic systems;Remote sensing;Semantic segmentation;Convolutional neural networks;Photovoltaic panel extraction;remote sensing image;semantic segmentation;swarm intelligence optimization algorithm;CNN;transformer},
  doi={10.1109/ACCESS.2024.3406551},
  ISSN={2169-3536},
  month={},}@ARTICLE{11087209,
  author={Alkhazraji, Emad and Mukhtar, Sani and Khaled, Haitham and Viegas, Jaime},
  journal={IEEE Access}, 
  title={AI-Driven Optimization of Multilayer Thin-Film Structures for Advanced Optical Applications}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Designing multilayer thin-film structures with tailored optical responses is a complex and computationally intensive task, often requiring repeated simulations and limited physical intuition about the role of each layer. In this study, we propose a data-driven framework that leverages a fully connected neural network (FCNN) as a surrogate model to accelerate and interpret the design of a high-reflectivity multilayer structure composed of alternating Ta2O5 and MgF2 layers. The FCNN model is trained to predict reflectance spectra with high accuracy over a wide design space defined by the thicknesses of individual layers. Beyond fast prediction, we analyze the physical influence of each design parameter using Principal Component Analysis (PCA), Global Sensitivity Analysis, and SHapley Additive exPlanations (SHAP). These techniques provide deep insights into layer significance, symmetry, and redundancy in the design, shedding light on the complex relationships between structure and spectral response. Finally, we integrate the surrogate model with the Non-dominated Sorting Genetic Algorithm II (NSGA-II) multi-objective optimization algorithm to maximize both the transmission peak and its full width at half maximum (FWHM) for a desired wavelength band.We then elect the best solution using a Multi-Criteria Decision-Making (MCDM) approach. This work demonstrates the synergy between AI and photonic design, enabling efficient optimization and facilitating physical interpretability. The framework provides a robust pathway for the intelligent design of optical filters and mirrors, with potential applications across telecommunications, sensing, and laser systems.},
  keywords={Nonhomogeneous media;Optimization;Reflectivity;Optical reflection;Computational modeling;Optical filters;Optical films;Nonlinear optics;Optical sensors;Artificial intelligence;Multilayer Thin Films;Optical Performance Optimization;Artificial Intelligence;Reflectance Optimization;Computational Photonics;Optical Coatings;Machine Learning},
  doi={10.1109/ACCESS.2025.3591001},
  ISSN={2169-3536},
  month={},}@ARTICLE{10783032,
  author={Wan, Cheng and Nnamdi, Micky C. and Shi, Wenqi and Smith, Benjamin and Purnell, Chad and Wang, May D.},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Advancing Sleep Disorder Diagnostics: A Transformer-Based EEG Model for Sleep Stage Classification and OSA Prediction}, 
  year={2025},
  volume={29},
  number={2},
  pages={878-886},
  abstract={Sleep disorders, particularly Obstructive Sleep Apnea (OSA), have a considerable effect on an individual's health and quality of life. Accurate sleep stage classification and prediction of OSA are crucial for timely diagnosis and effective management of sleep disorders. In this study, we develop a sequential network that enhances sleep stage classification by incorporating self-attention mechanisms and Conditional Random Fields (CRF) into a deep learning model comprising multi-kernel Convolutional Neural Networks (CNNs) and Transformer-based encoders. The self-attention mechanism enables the model to focus on the most discriminative features extracted from single-channel electroencephalography (EEG) recordings, while the CRF module captures the temporal dependencies between sleep stages, improving the model's ability to learn more plausible sleep stage sequences. Moreover, we explore the relationship between sleep stages and OSA severity by utilizing the predicted sleep stage features to train various regression models for Apnea-Hypopnea Index (AHI) prediction. Our experiments demonstrate an improved sleep stage classification performance of 78.7%, particularly on datasets with diverse AHI values, and highlight the potential of leveraging sleep stage information for monitoring OSA. By employing advanced deep learning techniques, we thoroughly explore the intricate relationship between sleep stages and sleep apnea, laying the foundation for more precise and automated diagnostics of sleep disorders.},
  keywords={Sleep;Brain modeling;Feature extraction;Sleep apnea;Electroencephalography;Predictive models;Transformers;Convolutional neural networks;Convolution;Recording;Sleep stage classification;obstructive sleep apnea (OSA);transformer;clinical decision support},
  doi={10.1109/JBHI.2024.3512616},
  ISSN={2168-2208},
  month={Feb},}@INPROCEEDINGS{10757558,
  author={Munir, Aiman and Parasuraman, Ramviyas and Ye, Jin and Song, WenZhan},
  booktitle={2024 IEEE 100th Vehicular Technology Conference (VTC2024-Fall)}, 
  title={Route Planning for Electric Vehicles with Charging Constraints}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Recent studies demonstrate the efficacy of machine learning algorithms for learning strategies to solve combinatorial optimization problems. This study presents a novel solution to address the Electric Vehicle Routing Problem with Time Windows (EVRPTW), leveraging deep reinforcement learning (DRL) techniques. Existing DRL approaches frequently encounter challenges when addressing the EVRPTW problem: RNN-based decoders struggle with capturing long-term dependencies, while DDQN models exhibit limited generalization across various problem sizes. To overcome these limitations, we introduce a transformer-based model with a heterogeneous attention mechanism. Transformers excel at capturing long-term dependencies and demonstrate superior generalization across diverse problem instances. We validate the efficacy of our proposed approach through comparative analysis against two state-of-the-art solutions for EVRPTW. The results demonstrated the efficacy of the proposed model in minimizing the distance traveled and robust generalization across varying problem sizes.},
  keywords={Vehicular and wireless technologies;Machine learning algorithms;Vehicle routing;Transformers;Electric vehicles;Routing;Deep reinforcement learning;Planning;Mobile robots;Optimization},
  doi={10.1109/VTC2024-Fall63153.2024.10757558},
  ISSN={2577-2465},
  month={Oct},}@INPROCEEDINGS{10395331,
  author={V, Haritha and Archana, K V and D, Balasubramaniam and S, Uma Maheswari and Sasireka, P. and G P, Suja},
  booktitle={2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)}, 
  title={Prostate Cancer Classification using Cat Swarm Optimization with Deep Learning on MRI Images}, 
  year={2023},
  volume={},
  number={},
  pages={1008-1013},
  abstract={Prostate Cancer (PC) is a common malignancy amongst men globally, and accurate classification plays a significant part in its diagnoses and treatment planning. Due to its superior soft tissue contrast, Magnetic Resonance Imaging (MRI) has developed as a valuable imaging modality for the assessment of PC. PC classification on MRI is a major task in medical imaging analysis, and deep learning (DL) approaches are employed to accomplish automated and accurate classification. In recent times, Convolutional Neural Network (CNN) structure was selected for the classification of PC. This manuscript introduces an automated Prostate Cancer Classification using Cat Swarm Optimization with Deep Learning (PCC-CSODL) technique on MRI Images. The main purpose of the PCC-CSODL algorithm lies in the effectual and accurate identification of PC. For achieving this, the PCC-CSODL technique involves NASNet feature extractor at the initial stages. In addition, the CSO algorithm is used for the optimum hyperparameter selection of the NASNet model. For classification purposes, attention based long short term memory (ALSTM) model can be used. The results demonstrated the effectiveness of the PCC-CSODL technique in accurately classifying PC on MRI. It has the potential to assist clinicians in making informed decisions and enhancing patient outcomes in PC management.},
  keywords={Deep learning;Magnetic resonance imaging;Feature extraction;Classification algorithms;Convolutional neural networks;Prostate cancer;Particle swarm optimization;Medical Imaging;Magnetic Resonance Imaging;Prostate Cancer;Deep Learning;Cat Swarm Optimization},
  doi={10.1109/ICECA58529.2023.10395331},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10796422,
  author={Prashanth Kumar, K and Kumar, Jaidev and Malik, Neeru and Jadhav, Archana and Sukumar, P. and Nishant, Neerav},
  booktitle={2024 First International Conference on Software, Systems and Information Technology (SSITCON)}, 
  title={Monitoring in-Vehicle Air Quality in Real Time Through IoT and Enhanced Deep Learning Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Globally, air pollution (AP) is among the most pressing environmental crises. With human health in mind, numerous researchers have focused on these issues. Predictive data on air quality is one of the most effective means of alerting the public to the dangers of air pollution and encouraging them to take preventative measures. One of the most difficult environmental problems in many large cities is air pollution. Preprocessing, feature selection, and model training are the three stages that make up the process. Data scaling, pollution comparison, and status fixing are all parts of preprocessing. The process of feature selection makes use of Q-learning based bee swarm optimization. The model was trained using an A-CNN-LSTM-XGBoost. With an average accuracy of $\mathbf{9 4. 6 1 \%}$, the suggested method outperforms LSTM and A-CNN.},
  keywords={Q-learning;Accuracy;Atmospheric modeling;Urban areas;Air pollution;Feature extraction;Internet of Things;Particle swarm optimization;Monitoring;Long short term memory;air quality monitoring;air quality index (AQI);longshort-term memory (LSTM)},
  doi={10.1109/SSITCON62437.2024.10796422},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10696691,
  author={Srinivasan, P. Santhosh and Regan, M.},
  booktitle={2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)}, 
  title={Enhancing Brain Tumor Diagnosis with Substructure Aware Graph Neural Networks and Fuzzy Linguistic Segmentation}, 
  year={2024},
  volume={},
  number={},
  pages={1613-1618},
  abstract={Brain tumors, which are characterized by uncontrolled cell proliferation, pose significant diagnostic and treatment challenges that require precise medical intervention. Current methods for brain tumor detection often suffer from low precision, accuracy, and high error rates. To address these issues, this study proposes a novel approach utilizing a Random-Coupled Neural Network with Substructure Aware Graph Neural Network Attention optimized by the Salp Swarm Optimization Algorithm (RCNN-SAGNN-SSOA) for accurate brain tumor detection. The proposed approach starts with pre-processing and feature extraction using a Contextual Attention Network with Convolutional Auto Encoder (CAN-CAE) to remove noise and enhance feature extraction efficiency. Image segmentation is performed using Adaptively Regularized Kernel-based Fuzzy C Means (ARKFCM). The classification is then carried out using the RCNN-SAGNN, with optimization provided by the Salp Swarm Optimization Algorithm (SSOA) to accurately differentiate between normal and abnormal brain conditions. Evaluations on the Brats 2021 and MRI datasets demonstrate that the proposed RCNN-SAGNN-SSOA achieves an accuracy of 99.78% and a recall of 97.34%, significantly outperforming existing methods. This high level of accuracy highlights the model's potential to significantly improve the speed and effectiveness of brain tumor diagnosis, offering substantial benefits for patient treatment.},
  keywords={Accuracy;Error analysis;Magnetic resonance imaging;Feature extraction;Brain modeling;Graph neural networks;Classification algorithms;Particle swarm optimization;Optimization;Tumors;Brain Tumor Detection;Adaptively Regularized Kernel-based Fuzzy C Means;Random-coupled Neural Network;Salp Swarm Optimization Algorithm},
  doi={10.1109/ICoICI62503.2024.10696691},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10724899,
  author={Sreevani, M and Latha, R.},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Classification of Breast Medical Images to Diagnose Cancer Using Triplet Attention Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The early and remote identification of chronic diseases is now achievable for the first time. In this study, mammography images from the Cancer Imaging Archive (TCIA) were used to build a deep transfer learning (DTL) model for an autonomous breast cancer diagnostic system. The enhanced medical treatment supplied by the smart healthcare system can benefit modern e-healthcare. Accurate diagnoses based on Internet of Things-based medical images are required for early detection and effective treatment of breast cancer. However, noise in these images can significantly reduce the precision of the diagnostic process. In this paper, we propose a new Triplet Attention Model (TAM) to improve the accuracy of breast cancer diagnosis from noisy medical images. The TAM use attention mechanisms and triplet loss to learn distinguishing features from noisy images. The triplet loss ensures that related images are closer together in the learned feature space, and the attention mechanism assists the model in focusing on those areas. As a result, categorization accuracy improves. Extensive tests are run on a large and diverse dataset of noisy breast medical images, and the results are compared to state-of-the-art approaches. As demonstrated in our trials, our proposed Triplet Attention Model surpasses earlier techniques in a number of significant ways, including its capacity to accurately classify noisy breast medical images for cancer diagnosis.},
  keywords={Representation learning;Accuracy;Attention mechanisms;Transfer learning;Noise;Breast cancer;Planning;Noise measurement;Medical diagnostic imaging;Testing;Breast cancer;Image classification;Triplet Attention Model;Deeplearning},
  doi={10.1109/ICCCNT61001.2024.10724899},
  ISSN={2473-7674},
  month={June},}@INPROCEEDINGS{10914740,
  author={Saarika, Kathula and Varsha, Vasaraju and Harsha, P.Sri and Sheikh, Anjum Nabi},
  booktitle={2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Deep Learning for Automated Image Captioning: A CNN and Transformer Model Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={2160-2165},
  abstract={Automatic image captioning is a multidisciplinary effort that combines machine learning, natural language processing, and computer vision in creating meaningful and context-rich captions for images. Within a short time, this research problem has garnered a lot of attention and new techniques are continuously being proposed. However, there are many dismaying factors as well. For instance, the current models still have a long way to go in attaining human accuracy. This paper seeks to conduct a systematic review of the recent trends and developments in the field of image captioning techniques and models with a focus on 2023 and 2024 by systematically analyzing the 13 publications located in Springer and IEEEXplore libraries. According to our research, Convolutional Neural Networks allow the identification of image content at an early stage, and Recurrent Neural Networks or Long Short-Term Memory networks usually appear at the text generation stage. The most common image datasets are MS COCO and Flickr8k. Almost all studies utilized the BLEU scores (1, 2, 3, and 4) mechanism. It is alleviated that CNN with LSTM models tends to perform better when compared to RNN-modeled networks. Attention mechanisms and Encoder-Decoder architecture are aggressively becoming the two major solutions to strengthening image captioning. This review will help and provide direction to those who are interested in practicing research that aims to promote the techniques and processes of automatic image captioning.},
  keywords={Deep learning;Analytical models;Recurrent neural networks;Transformers;Market research;Natural language processing;Convolutional neural networks;Particle swarm optimization;Long short term memory;Systematic literature review;Image Captioning;Deep Learning;Neural Networks;Recurrent Neural Networks (RNN);Convolutional Neural Networks (CNN);Long Short-Term Memory (LSTM)},
  doi={10.1109/IDCIOT64235.2025.10914740},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10644816,
  author={Gallup, Ethan R. and Tuttle, Jacob F. and Immonen, Jake and Billings, Blake and Powell, Kody M.},
  booktitle={2024 American Control Conference (ACC)}, 
  title={Transformer Neural Networks with Spatiotemporal Attention for Predictive Control and Optimization of Industrial Processes}, 
  year={2024},
  volume={},
  number={},
  pages={382-387},
  abstract={In the context of real-time optimization and model predictive control of industrial systems, machine learning, and neural networks represent cutting-edge tools that hold promise for enhancing dynamic modeling. This work presents a transformer neural network architecture for real-time optimization and model predictive control. This network design includes a modified attention mechanism inspired by positional embedding attention from vision transformers and task-specific modifications to the input-output structure of the transformer's decoder stack. Experiments were conducted using data from a 450 MW coal-fired power plant to evaluate this approach's effectiveness. The transformer neural network was compared with conventional recurrent models, including GRU and LSTM neural networks. The transformer exhibited a 6% increase in the R-squared value of predictions and an 83% reduction in mean squared error. Computation time was also reduced by 84% compared to conventional recurrent models.},
  keywords={Computational modeling;Neural networks;Computer architecture;Predictive models;Transformers;Real-time systems;Optimization},
  doi={10.23919/ACC60939.2024.10644816},
  ISSN={2378-5861},
  month={July},}@INPROCEEDINGS{11061564,
  author={Ou, Ting-Chia and Chen, Yu-Tung and Jhang, Yi-Wei},
  booktitle={2025 IEEE Industry Applications Society Annual Meeting (IAS)}, 
  title={Optimization of hybrid renewable power forecasting model for microgrid applications}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={To address the intermittency of renewable energy and the complexities associated with dispatching in microgrids, this study proposes a hybrid forecasting framework that integrates machine learning, deep learning, and swarm intelligence algorithms to enhance prediction accuracy. For solar power forecasting, a model that combines Convolutional Neural Networks (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention Mechanisms is utilized to extract both spatial and temporal features. In the case of wind power forecasting, a Least Squares Support Vector Machine (LSSVM) optimized through Bee Swarm Optimization (BSO) is employed. The dataset comprises hourly meteorological and power generation data from the Datan region in Taiwan. Following feature selection and data imputation, seasonal classification and K-means clustering are applied to assess model performance across varying data structures. The results indicate that the proposed solar model reduces the Mean Absolute Error (MAE) by 4.01%, 1.23%, and 1.12% compared to Long Short-Term Memory (LSTM), BiLSTM, and the CNN + BiLSTM models, respectively. The BSO-optimized LSSVM outperforms the Autoregressive Integrated Moving Average with Exogenous Variables (ARIMAX), Random Forest (RF), Support Vector Regression (SVR), and non-optimized LSSVM by 44.75%, 16.53%, 5.34%, and 1.76%, respectively. Overall, the framework significantly enhances forecasting accuracy as measured by MAE, Root Mean Square Error (RMSE), and Mean Relative Error (MRE), thereby improving energy management and dispatching in smart microgrids.},
  keywords={Support vector machines;Renewable energy sources;Bidirectional long short term memory;Microgrids;Wind power generation;Predictive models;Feature extraction;Convolutional neural networks;Forecasting;Particle swarm optimization;Microgrid;Renewable Energy Forecasting;Solar Power Prediction;Wind Power Prediction;Convolutional Neural Network (CNN);Bidirectional Long Short-Term Memory (BiLSTM);Least Squares Support Vector Machine (LSSVM);Bee Swarm Optimization (BSO)},
  doi={10.1109/IAS62731.2025.11061564},
  ISSN={2576-702X},
  month={June},}@ARTICLE{10982255,
  author={Choi, Min-Hwa and Yoon, Woongchang},
  journal={IEEE Access}, 
  title={Predicting Ship Waiting Times Using Machine Learning for Enhanced Port Operations}, 
  year={2025},
  volume={13},
  number={},
  pages={81377-81391},
  abstract={Port congestion and prolonged ship waiting times pose challenges for global trade and increase operational costs and inefficiencies. In this study, a novel machine learning-based predictive approach was proposed to improve port operations by accurately forecasting vessel waiting times. By using a dataset of 121,401 voyage records, we evaluated nine regression models, including conventional, ensemble-based, and deep learning models. Shapley additive explanation (SHAP)-based feature selection is typically applied to enhance interpretability, and its effect is compared with principal component analysis-based dimensionality reduction and nonselection methods. The XGBoost Regressor (XGBR) is optimized using genetic-algorithm-based hyperparameter tuning, reducing mean squared error (RMSE) from 20.9531 to 19.6387, mean absolute error (MAE) from 13.6821 to 12.6753, and improving coefficient of determination (R2) from 0.2791 to 0.2949. A stacking ensemble model, integrating random forest regressor, XGBR, LightGBM regressor, and CatBoost regressor, improves performance, achieving an RMSE of 18.9023, MAE of 12.3287, and an R2 of 0.3265. ANOVA tests confirm numerous differences in model performance and computational complexity. The results demonstrated that tree-based ensemble models outperform deep learning models in this setting. The proposed approach enables proactive scheduling, reduces congestion, and cost savings. The scalability of the model renders it suitable for broad maritime logistics and intelligent transportation systems.},
  keywords={Computational modeling;Predictive models;Seaports;Marine vehicles;Feature extraction;Training;Biological system modeling;Costs;Deep learning;Computational efficiency;Hyperparameter tuning;machine learning;prediction;regression;ship waiting time},
  doi={10.1109/ACCESS.2025.3566429},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10646448,
  author={Shu, Nicolas and Wang, You and Caulley, Desmond and Anderson, David V.},
  booktitle={2024 IEEE International Conference on Edge Computing and Communications (EDGE)}, 
  title={SlimNet: A Lightweight Attentive Network for Speech-Music-Noise Classification and Voice Activity Detection}, 
  year={2024},
  volume={},
  number={},
  pages={157-161},
  abstract={In this work, we present SlimNet, a hybrid architecture that incorporates a long short-term memory with an attention mechanism for speech-music-noise classification and speech vs non-speech categorization. Deep neural network approaches for these tasks have achieved remarkable success. However, deep models are computationally and energy intensive. When compared to state-of-the-art models, SlimNet uses up to 7 times fewer parameters, with up to 5 times faster inference while achieving comparable performance. The small footprint of SlimNet makes it a simple yet powerful candidate for devices with limited computational resources, and for edge computing devices in particular.},
  keywords={Performance evaluation;Voice activity detection;Attention mechanisms;Computational modeling;Computer architecture;Task analysis;Long short term memory;Audio classification;deep neural networks;long short-term memory network;attention network;edge computing},
  doi={10.1109/EDGE62653.2024.00028},
  ISSN={2767-9918},
  month={July},}@INPROCEEDINGS{10862006,
  author={Li, Brandon Kah Wei and Jasser, Muhammed Basheer and Issa, Bayan and Ajibade, Samuel-Soma M. and Chua, Hui Na and Wong, Richard T.K. and Hamzah, Muzaffar},
  booktitle={2024 IEEE 12th Conference on Systems, Process & Control (ICSPC)}, 
  title={A Comparison Study on Generative Adversarial Networks (GANs) for Satellite Image Upscaling}, 
  year={2024},
  volume={},
  number={},
  pages={298-303},
  abstract={Satellite images that are publicly available on the internet such as Google Maps are mostly low-quality and blurry which is not very useful for research or decision-making due to the lack of details. Recently image upscaling has been increasing in popularity to address several real-world problems where one approach is using Convolutional Neural Networks (CNNs). However, CNNs have difficulty in restoring fine details and this has led to the use of Generative adversarial networks (GANs) for image scaling. In this paper, three GAN models are trained which are Enhanced Super-Resolution Generative Adversarial Networks (Real-ESRGAN), Image Restoration Using Swin Transformer (SwinIR-Medium), and Dual Aggregation Transformer (DAT-2). The models are evaluated with well-known image metrics to determine each model’s effectiveness in upscaling low-resolution satellite images. The results show that all models can reconstruct the image to some degree where the best out of the three is DAT-2. Further fine-tuning of the best model by changing the loss functions has shown improvements in structural integrity for the upscaled image.},
  keywords={Training;Photography;Measurement;Web services;Generative adversarial networks;Transformers;Satellite images;Image restoration;Particle swarm optimization;Image reconstruction;GAN;Satellite Image Processing;Image Upscaling;DAT;Image restoration},
  doi={10.1109/ICSPC63060.2024.10862006},
  ISSN={2769-7916},
  month={Dec},}@INPROCEEDINGS{10967850,
  author={J, Sujatha and Patil, Nagaraj B and Supriya and Abas, Hayder Muhamed and Jagadesh, Gona},
  booktitle={2025 3rd International Conference on Integrated Circuits and Communication Systems (ICICACS)}, 
  title={Anomaly Detection in Internet of Things Using Weighted Botox Optimization Algorithm with Gated Recurrent Unit}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Anomaly Detection (AD) play a significant part in risk management and data security among various domains like medical image recognition, economic insurance security, Internet of Things (IoT) management. However, several types of AD have been established in traditional research that concentrate on identifying anomaly with the help of classification issues. To overcome this problem, Weighted Botox Optimization Algorithm with Gated Recurrent Unit (WBOA-GRU) is proposed in this research for AD. The WBOA chooses optimal features and helps to reduce dimensionality through updating their locations based on its behavior. The GRU effectively handle sequential data when requiring less computational sources thereby enhancing the classification accuracy. The preprocessing uses two methods where hash encoding is applied to convert categorical data into numerical format and then normalization is applied to scale the data for ensuing reliability. The WBOA-GRU achieves accuracy of 99.25% for UNSW-NB15 dataset which is better than Rat Swarm Optimization with Improved Self-Attention based Gated Recurrent Unit (RSO-ISAGRU).},
  keywords={Training;Accuracy;Feature extraction;Encoding;Internet of Things;Risk management;Integrated circuit modeling;Particle swarm optimization;Optimization;Anomaly detection;anomaly detection;gated recurrent unit;hash encoding;normalization;weighted botox optimization algorithm},
  doi={10.1109/ICICACS65178.2025.10967850},
  ISSN={},
  month={Feb},}@ARTICLE{9722957,
  author={Zhu, Shixiang and Bukharin, Alexander and Xie, Liyan and Yamin, Khurram and Yang, Shihao and Keskinocak, Pinar and Xie, Yao},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Early Detection of COVID-19 Hotspots Using Spatio-Temporal Data}, 
  year={2022},
  volume={16},
  number={2},
  pages={250-260},
  abstract={Recently, the Centers for Disease Control and Prevention (CDC) has worked with other federal agencies to identify counties with increasing coronavirus disease 2019 (COVID-19) incidence (hotspots) and offers support to local health departments to limit the spread of the disease. Understanding the spatio-temporal dynamics of hotspot events is of great importance to support policy decisions and prevent large-scale outbreaks. This paper presents a spatio-temporal Bayesian framework for early detection of COVID-19 hotspots (at the county level) in the United States. We assume both the observed number of cases and hotspots depend on a class of latent random variables, which encode the underlying spatio-temporal dynamics of the transmission of COVID-19. Such latent variables follow a zero-mean Gaussian process, whose covariance is specified by a non-stationary kernel function. The most salient feature of our kernel function is that deep neural networks are introduced to enhance the model’s representative power while still enjoying the interpretability of the kernel. We derive a sparse model and fit the model using a variational learning strategy to circumvent the computational intractability for large data sets. Our model demonstrates better interpretability and superior hotspot-detection performance compared to other baseline methods.},
  keywords={COVID-19;Kernel;Testing;Mathematical models;Gaussian processes;Computational modeling;Diseases;COVID-19 hotspots;Gaussian processes;non-stationary kernel;spatio-temporal model},
  doi={10.1109/JSTSP.2022.3154972},
  ISSN={1941-0484},
  month={Feb},}@ARTICLE{10381684,
  author={Chen, Tianao and Chen, Aotian},
  journal={IEEE Access}, 
  title={VisionTwinNet: Gated Clarity Enhancement Paired With Light-Robust CD Transformers}, 
  year={2024},
  volume={12},
  number={},
  pages={4544-4560},
  abstract={Deep learning has shown superiority in change detection (CD) tasks, notably the Transformer architecture with its self-attention mechanism, capturing long-range dependencies and outperforming traditional models. This capability provides the Transformer with significant advantages in capturing global-level features of complex changes in objects within high-resolution remote sensing images. Though Transformers are mature in Natural Language Processing (NLP), their application in computer vision, particularly CD tasks, is nascent. Current research on leveraging Transformers for CD reveals limitations, especially under varied lighting and seasonal changes. To address this, we propose VisionTwinNet, a two-stage strategy. First, our Gated EnhanceClearNet, a specially designed deep network reduces image noise and enhances brightness, preserving shadows and correcting color distortions. With its unique gating mechanism, this network can adaptively adjust the importance of features, thereby exhibiting superior performance in various remote sensing image degradation issues. Secondly, we have developed Hybrid Light-Robust CDNet, a hybrid robust lightweight network custom-designed for CD in remote sensing images. This module deeply integrates the advantages of CNN and Transformer and introduces an innovative attention mechanism design, optimizing the key/value dimensions separately, instead of adopting traditional single linear transformations, ensuring efficient detection. Specifically, the LR-Transformer Block employs a lightweight multi-head self-attention mechanism, optimizing computational efficiency while providing richer feature representations. Comparative studies with six CD methods on three public datasets validate VisionTwinNet’s robustness and efficacy. Our approach notably reduces algorithmic complexity and enhances the efficiency of the model.},
  keywords={Transformers;Lighting;Remote sensing;Task analysis;Logic gates;Computational modeling;Feature extraction;Deep learning;Automatically adjustable framework;change detection;deep learning;multi-scale feature extraction;transformer},
  doi={10.1109/ACCESS.2024.3350173},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10611970,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Chiang, Jun-Kui and Kubota, Naoyuki and Sato-Shimokawara, Eri and Nojima, Yusuke and Acampora, Giovanni and Wu, Pei-Yu and Chiu, Szu-Chi and Yang, Sheng-Chi and Siow, Chyan-Zheng},
  booktitle={2024 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={Quantum Computational Intelligence with Generative AI Image for Human-Machine Interaction}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper introduces a Quantum Computational Intelligence (QCI) agent equipped with a content attention ontology model, specifically designed to enhance human-machine interaction based on a Generative Artificial Intelligence (GAI) image generation agent for Taiwanese/English learning and experience. Its diverse primary applications include social media analysis on Facebook groups and YouTube learning videos related to the 2023 IEEE CIS Education Portal (EP) Subcommittee, as well as in the areas of Taiwanese/English language learning and dialogue experience with GAI image generation. To establish the knowledge and inference models for the QCI agent, we initially developed a Taiwanese/English learning and experience ontology, including a content attention ontology, and an image attention ontology. The QCI agent utilizes metrics such as the number of views, posts, and comments to predict the fuzzy number of reactions. In addition, the GAI image agent generates Taiwanese speech-based/English text-based images and evaluates the fuzzy similarity score between Taiwanese/English and the attention ontology together with the Sentence BERT (SBERT) agent. This Taiwanese/English fuzzy similarity score is further validated through human assessments, with these evaluations subsequently serving as an additional metric for comparative analysis of Human-Machine Interaction (HMI). Furthermore, the GAI image agent is designed to create images and Chinese/English texts from text/speech translated by the Meta AI Universal Speech Translator (UST) Taiwanese/English agent. A Particle Swarm Optimization (PSO)-based machine learning mechanism is employed to train the QCI model for assessing learners' performance and predicting the performance of others. The National University of Tainan (NUTN) Taiwan-Large Language Model (NUTN.TW-LLM) agent has been further enhanced to support interactive learning experiences for HMI. An SBERT-based assessment agent is used to calculate fuzzy similarities between questions and answers in Taiwanese/English experiences and dialogues. Experimental results demonstrate the feasibility and efficacy of the proposed QCI model, equipped with QCI&AI-FML (Artificial Intelligence-Fuzzy Markup Language) and machine learning capabilities, for social media and language learning applications on HMI. In the future, we will extend the QCI model to various HMI applications for student learning around the world.},
  keywords={Human computer interaction;Measurement;Quantum computing;Social networking (online);Image synthesis;Generative AI;Computational modeling;Quantum CI Agent;Content Attention Ontology;ChatGPT;Generative AI Image Agent;IEEE CIS Education Portal;Fuzzy Markup Language;Sentence BERT;NUTN.TW-LLM},
  doi={10.1109/FUZZ-IEEE60900.2024.10611970},
  ISSN={1558-4739},
  month={June},}@INPROCEEDINGS{9995680,
  author={Liu, Yue and Zhang, Junfeng and Wang, Shulin and Zhang, Wei and Zeng, Xiangxiang and Kwoh, Chee Keong},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A heterogeneous graph cross-omics attention model for single-cell representation learning}, 
  year={2022},
  volume={},
  number={},
  pages={270-275},
  abstract={Single-cell multi-omics sequencing technologies allow simultaneous measurement of transcriptome and epigenome profiles in the same cell, providing unprecedented opportunities to dissect cell heterogeneity. Despite great efforts, conjoint analysis of single-cell multi-omics data still suffers from sparsity, high dimensionality and binary. In this study, we present a heterogeneous graph cross-omics attention model (scHGA), a computational tool based on a heterogeneous graph neural network combining two attention mechanisms to jointly analyze single-cell multi-omics data based on different protocols data, including SNARE-seq, scMT-seq and sci-CAR. To avoid the cell heterogeneity of single-omics data, scHGA automatically learns a cell association graph to capture neighbor information. The latent representation of aggregated cells generated by hierarchical attention can fuse knowledge across different omics to dissect cellular heterogeneity, providing a better scheme to characterize the features of cells. scHGA is an effective exploration of graph neural networks in single-cell multi-omics analysis, providing new insights into the understanding of single-cell sequencing data.},
  keywords={Heating systems;Analytical models;Sequential analysis;Correlation;Protocols;Data analysis;Biological system modeling;Graph neural network;attention mechanism;single-cell multi-omics data analysis},
  doi={10.1109/BIBM55620.2022.9995680},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9945515,
  author={Owens, Jelani and Gupta, Kishor Datta and Yan, Xuyang and Zeleke, Lydia Asrat and Homaifar, Abdollah},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Interpretable Convolutional Learning Classifier System (C-LCS) for higher dimensional datasets}, 
  year={2022},
  volume={},
  number={},
  pages={846-853},
  abstract={The purpose of this paper is to devise an interpretable hybrid classification model for Convolutional Neural Networks (CNN) and a Learning Classifier System (LCS). The presented hybrid system integrates the fundamental attributes from both types of these classifiers. In the proposed hybrid model CNN works as an automatic feature extractor, and LCS works to provide interpretable rule-based classification results. Although LCS has limitations working on higher dimensional datasets, we resolve this limitation by using CNN as a feature extractor. The other concept of the non-interpretability of CNN is addressed by using the LCS rule. Furthermore, our experiment with higher dimensional datasets like CIFAR-10 and Fashion-MNIST shows that extended LCS provides comparable performance to the standard neural network model while also providing interpretable results. We named this extended LCS method Convolutional Learning Classifier Cystem (C-LCS).},
  keywords={Neural networks;Feature extraction;Convolutional neural networks;Standards;Cybernetics},
  doi={10.1109/SMC53654.2022.9945515},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{10172800,
  author={Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)}, 
  title={CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={919-931},
  abstract={Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.},
  keywords={Software testing;Codes;Benchmark testing;Software;Space exploration;Test pattern generators;Software engineering;search based software testing;codex;test suite generation;python;large language model;automated testing},
  doi={10.1109/ICSE48619.2023.00085},
  ISSN={1558-1225},
  month={May},}@INPROCEEDINGS{10765033,
  author={Yang, Lin and Yang, Chen and Gao, Shutao and Wang, Weijing and Wang, Bo and Zhu, Qihao and Chu, Xiao and Zhou, Jianyi and Liang, Guangtai and Wang, Qianxiang and Chen, Junjie},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={On the Evaluation of Large Language Models in Unit Test Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1607-1619},
  abstract={Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs’ capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.CCS CONCEPTS • Software and its engineering → Software testing and debugging.},
  keywords={Software testing;Codes;Large language models;Training data;Writing;Syntactics;Software;Test pattern generators;Defect detection;Software engineering;Large Language Model;Unit Test Generation;Empirical Study},
  doi={},
  ISSN={2643-1572},
  month={Oct},}@INPROCEEDINGS{10392342,
  author={Raj, Rakesh S. and Kusuma, M.},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={A Comprehensive Analysis of Chronic Health Diseases using Big Data}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In recent times, individuals have been confronted with a multitude of health diseases as a result of their living environments and the prevailing environmental conditions. A disease that lasts for over three months is categorized as a chronic disease, and it is projected to become the primary global cause of mortality.A chronic ailment progresses slowly, leading to damage in the body. The circumstances could potentially respond to medical treatment, or they might not. The chronic disease can be managed, but neither medicines nor vaccines can cure it. Chronic disease is mainly divided into various types such as cancer, heart disease, diabetes, asthma, Parkinson’s disease, arthritis, rheumatoid, stroke, and kidney disease. Identifying and diagnosing chronic diseases in their early stages is of paramount importance, yet it remains a challenging task for doctors to accurately detect and treat these conditions. Thegoalof this research is to create a model with high interpretability for predicting and diagnosing chronic diseases. The analytics of big data plays a significant role in treating and predicting chronic diseases. This analytics enables higher accuracy results, effectiveness, and less computational and time complexity, as demonstrated in this survey. The goal of this study is to create a model with high interpretability for predicting future diagnosis.},
  keywords={Surveys;Computational modeling;Medical treatment;Predictive models;Big Data;Vaccines;Time complexity;Arthritis;Big Data Analytics;Cancer;Chronic Diseases;Diabetes;Heart Diseases},
  doi={10.1109/EASCT59475.2023.10392342},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10647604,
  author={Cherukuri, Teja Krishna and Shaik, Nagur Shareef and Ye, Dong Hye},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)}, 
  title={Guided Context Gating: Learning To Leverage Salient Lesions in Retinal Fundus Images}, 
  year={2024},
  volume={},
  number={},
  pages={3098-3104},
  abstract={Effectively representing medical images, especially retinal images, presents a considerable challenge due to variations in appearance, size, and contextual information of pathological signs called lesions. Precise discrimination of these lesions is crucial for diagnosing vision-threatening issues such as diabetic retinopathy. While visual attention-based neural networks have been introduced to learn spatial context and channel correlations from retinal images, they often fall short in capturing localized lesion context. Addressing this limitation, we propose a novel attention mechanism called Guided Context Gating, an unique approach that integrates Context Formulation, Channel Correlation, and Guided Gating to learn global context, spatial correlations, and localized lesion context. Our qualitative evaluation against existing attention mechanisms emphasize the superiority of Guided Context Gating in terms of explainability. Notably, experiments on the Zenodo-DR-7 dataset reveal a substantial 2.63* accuracy boost over advanced attention mechanisms & an impressive 6.53% improvement over the state-of-the-art Vision Transformer for assessing the severity grade of retinopathy, even with imbalanced and limited training samples for each class.},
  keywords={Pathology;Diabetic retinopathy;Visualization;Attention mechanisms;Correlation;Accuracy;Retina;Representation Learning;Visual Attention;Guided Context Gating;Diabetic Retinopathy},
  doi={10.1109/ICIP51287.2024.10647604},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10393547,
  author={Ding, Zhijin},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={Construction and Exploration of a Financial Risk Control Model Based on Machine Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={With the development and innovation of the financial industry, effective risk management has become an important task in the financial industry. Therefore, more and more financial institutions are adopting machine learning based methods to construct risk control models. This article discussed the construction and analysis methods of financial risk control models based on machine learning. Machine learning models can learn latent patterns and patterns from historical data, more accurately assess risks and detect fraudulent behavior. The construction process includes steps such as data preprocessing, feature engineering, model selection and training, model evaluation and optimization. Empirical analysis showed that machine learning models have advantages in tasks such as credit evaluation, fraud detection, and loan rejection. However, challenges such as data quality, feature selection, and model interpretability still need to be addressed. Further research can improve model performance and interpretability, enabling financial institutions to better manage risks and provide customers with safe and reliable financial services.},
  keywords={Analytical models;Machine learning;Predictive models;Data models;Behavioral sciences;Fraud;Risk management;financial risk control;machine learning;credit evaluation;fraud detection;loan rejection},
  doi={10.1109/EASCT59475.2023.10393547},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10309396,
  author={Kelly, Erin and Wheaton, Lewis A. and Hammond, Frank L.},
  booktitle={2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={The Effect of Tactor Composition and Vibrotactile Stimulation on Sensory Memory for a Haptic Feedback Display}, 
  year={2023},
  volume={},
  number={},
  pages={456-463},
  abstract={Previously, a wearable multimodal sensory feedback device (SFD) was developed to communicate proprioceptive information from a robotic gripper onto the operator’s forearm. The SFD showed promise in that it could effectively communicate proprioceptive sensory information and enhance the body’s natural proprioceptive sense. This study delves into the feedback modes implemented into the feedback device by evaluating the effect that increased skin-stretch in combination with vibrotactile stimulation has on the users’ abilities to discern the location of the tactor after time has passed. The SFD used in this study implements a tactor, made of either silicone or foam that translates laterally across the ventral side of the forearm. Subjects were asked to sense the location of the tactor after it had been stationary for a period of time. The experiments’ results indicate that a material that provides increased skin-stretch sensation can benefit the duration of skin-stretch feedback for sensory feedback devices. Additionally, vibrotactile stimulation has shown to be promising though its compatibility with the silicone material was not ideal.},
  keywords={Vibrations;Friction;Propioception;Virtual reality;Switches;Silicon;Skin},
  doi={10.1109/RO-MAN57019.2023.10309396},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{11029738,
  author={Cheng, Xiang and Sang, Fan and Zhai, Yizhuo and Zhang, Xiaokuan and Kim, Taesoo},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)}, 
  title={Rug: Turbo Llm for Rust Unit Test Generation}, 
  year={2025},
  volume={},
  number={},
  pages={2983-2995},
  abstract={Unit testing improves software quality by evaluating isolated sections of the program. This approach alleviates the need for comprehensive program-wide testing and confines the potential error scope within the software. However, unit test development is time-consuming, requiring developers to create appropriate test contexts and determine input values to cover different code regions. This problem is particularly pronounced in Rust due to its intricate type system, making traditional unit test generation tools ineffective in Rust projects. Recently, large language models (LLMs) have demonstrated their proficiency in understanding programming language and completing software engineering tasks. However, merely prompting LLMs with a basic prompt like “generate unit test for the following source code” often results in code with compilation errors. In addition, LLM-generated unit tests often have limited test coverage. To bridge this gap and harness the capabilities of LLM, we design and implement RUG, an end-to-end solution to automatically generate the unit test for Rust projects. To help LLM's generated test pass Rust strict compilation checks, RUG designs a semantic-aware bottom-up approach to divide the context construction problem into dependent sub-problems. It solves these sub-problems sequentially using an LLM and merges them to a complete context. To increase test coverage, RUG integrates coverage-guided fuzzing with LLM to prepare fuzzing harnesses. Applying RUG on 17 real-world Rust programs (average $24,937 \text{LoC}$), we show that RUG can achieve a high code coverage, up to $\mathbf{7 1. 3 7 \%}$, closely comparable to human effort $(\mathbf{7 3. 1 8 \%})$. We submitted 113 unit tests generated by RUG covering the new code: 53 of them have been accepted, 17 rejected, and 43 are pending for review.},
  keywords={Codes;Reviews;Large language models;Source coding;Scalability;Software quality;Fuzzing;Test pattern generators;Testing;Software engineering;Unit testing;Large language model;Rust},
  doi={10.1109/ICSE55347.2025.00097},
  ISSN={1558-1225},
  month={April},}@INPROCEEDINGS{10782885,
  author={Khimani, Asma and Hornback, Andrew and Jain, Neha and Avula, Pavithra and Jaishankar, Anirudh and Wang, May D.},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Predicting Cardiovascular Disease Risk in Tobacco Users Using Machine Learning Algorithms}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Cardiovascular Diseases (CVDs) present a substantial global health burden, with tobacco use as a major risk factor. While extensive research has identified several risk factors for CVDs, there is a gap in predictive models that account for a combination of clinical factors, lifestyle factors, and other determinants in order to predict CVD risk. In addition, existing studies tend to overlook the interactions among risk factors within high-risk populations, such as tobacco users. In this study, we examined phenotype data from over 15,000 tobacco users from the UK Biobank dataset to investigate which additional phenotype factors in the population showed predictive power for CVD. We explored the application of multiple Machine Learning (ML) algorithms, including Decision Trees (DT), Gradient Boosting (GB), Logistic Regression (LR), Random Forest (RF), and Support Vector Classification (SVC) in predicting CVD risk and individual phenotype feature importance. By analyzing the rich phenotype data in the UK Biobank via various algorithms, we were able to understand factors related to risk prediction and offer insights into the interplay of risk factors that contribute to cardiovascular events in this high-risk population.},
  keywords={Radio frequency;Phenotypes;Machine learning algorithms;Static VAr compensators;Predictive models;Prediction algorithms;Vectors;Classification algorithms;Cardiovascular diseases;Random forests;Cardiovascular Disease;Tobacco Use;UK Biobank;Machine Learning},
  doi={10.1109/EMBC53108.2024.10782885},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10984554,
  author={Qian, Liangxin and Yang, Ping and Wu, Gang and Tang, Wanbin and Chen, Ze},
  booktitle={2024 International Conference on Future Communications and Networks (FCN)}, 
  title={Self-Evolving Wireless Communications: A Novel Intelligence Trend for 6G and Beyond}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Wireless communication is rapidly evolving, and future wireless communications (6G and beyond) will be more heterogeneous, multi-layered, and complex, which poses challenges to traditional communications. Adaptive technologies in traditional communication systems respond to environmental changes by modifying system parameters and structures on their own and are not flexible and agile enough to satisfy requirements in future communications. To tackle these challenges, we propose a novel self-evolving communication framework, which consists of three layers: data layer, information layer, and knowledge layer. The first two layers allow communication systems to sense environments, fuse data, and generate a knowledge base for the knowledge layer. When dealing with a variety of application scenarios and environments, the generated knowledge is subsequently fed back to the first two layers for communication in practical application scenarios to obtain self-evolving ability and enhance the robustness of the system. In this paper, we first highlight the limitations of current adaptive communication systems and the need for intelligence, automation, and self-evolution in future wireless communications. We overview the development of self-evolving technologies and conceive the concept of self-evolving communications with its hypothetical architecture. To demonstrate the power of self-evolving modules, we compare the performances of a communication system with and without evolution. We then provide some potential techniques that enable self-evolving communications.},
  keywords={Wireless communication;6G mobile communication;Knowledge engineering;Adaptive systems;Fuses;Large language models;Knowledge based systems;Machine learning;Market research;Robustness;6G;adaptive technologies;large language model;machine learning;self-evolving communications},
  doi={10.1109/FCN64323.2024.10984554},
  ISSN={},
  month={Nov},}@ARTICLE{10583860,
  author={Kamath, Sudiksha Kottachery and Pendekanti, Sanjeev Kushal and Rao, Divya},
  journal={IEEE Access}, 
  title={LivMarX: An Optimized Low-Cost Predictive Model Using Biomarkers for Interpretable Liver Cirrhosis Stage Classification}, 
  year={2024},
  volume={12},
  number={},
  pages={92506-92522},
  abstract={Liver cirrhosis, a progressive and irreversible condition characterized by the replacement of healthy liver tissue with scar tissue, presents a persistent challenge in healthcare. Leveraging a comprehensive dataset of 424 patients, including controlled trial data from 312 patients and real-world follow-up information from an additional 112 patients sourced from the Mayo Clinic trial on primary biliary cirrhosis, this study refines patient demographics and integrates biomarkers for a detailed analysis. Our paper introduces LivMarX, a novel model that utilises advanced machine learning algorithms within an interdisciplinary framework and strives to stage Liver Cirrhosis using biomarkers instead of images. Our study employs meticulous feature engineering techniques, the creation of synthetic variables and categorizations to unveil critical relationships between patient characteristics and disease stages. To further refine performance, hyperparameter optimization is implemented, combining Genetic Algorithm, Optuna, and GridSearchCV. The Random Forest Classifier in LivMarX outperformed other models with an accuracy of 84.33%, and post-optimization, an accuracy improvement of 86%. LivMarX demonstrates an AUC of 0.95, showing that it provides reliable staging of liver cirrhosis. By relying on common blood tests instead of expensive imaging, this offers a cost-effective and comfortable approach to Liver Cirrhosis diagnosis. This study positions LivMarX as a potential model for accurately classifying Liver Cirrhosis stages, particularly in areas with limited access to complex imaging equipment.},
  keywords={Liver;Accuracy;Liver diseases;Ultrasonic imaging;Medical services;Prediction algorithms;Predictive models;Liver cirrhosis;machine learning;health determinants;healthcare;disease prediction},
  doi={10.1109/ACCESS.2024.3422451},
  ISSN={2169-3536},
  month={},}@ARTICLE{10720621,
  author={Liu, Xiaoqian and Zhang, Yingjun and Wang, Hui and Qin, Sipei and Zhang, Zhenhua and Yang, Yanyan and Wang, Jingping},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={Long-Term Interpretable Air Quality Trend Forecasting via Directed Interval Fuzzy Cognitive Maps}, 
  year={2024},
  volume={32},
  number={12},
  pages={7129-7142},
  abstract={Accurate air quality forecasting is crucial for public health and addressing air pollution. However, the dynamic evolution trends, the cross-interference among different air quality indexes, and the error accumulation in the long-term prediction process are still open problems when establishing air quality forecasting models. Thus, we present a long-term interpretable air quality trend forecasting model to address these challenges via directed interval fuzzy cognitive maps, DE-DIFCM. Specifically, we design a time series trend extraction and representation learning module based on the interval fuzzy granules and the Cramer decomposition theorem in the first phase. Next, we formulate the interval information granules' time series forecasting as a DIFCM. In particular, we employ PM$_{2.5}$ as a benchmark to validate the performance of the proposed DE-DIFCM. Experimental results on six air quality monitoring datasets demonstrate the model's superior and competitive long-term prediction performance by comparison with some representative baselines.},
  keywords={Atmospheric modeling;Forecasting;Predictive models;Market research;Time series analysis;Fuzzy systems;Fuzzy cognitive maps;Data models;Air pollution;Computational modeling;Air quality forecasting;PM  $_{2.5}$   forecasting;directed interval fuzzy cognitive map (DIFCM);fuzzy cognitive map (FCM);granular representation},
  doi={10.1109/TFUZZ.2024.3482282},
  ISSN={1941-0034},
  month={Dec},}@INPROCEEDINGS{10408358,
  author={Hu, Da and Wang, Mengjun and Li, Shuai},
  booktitle={2023 Winter Simulation Conference (WSC)}, 
  title={3D Object Detection and Localization within Healthcare Facilities}, 
  year={2023},
  volume={},
  number={},
  pages={2710-2721},
  abstract={This study introduces a deep learning-based method for indoor 3D object detection and localization in healthcare facilities. This method incorporates spatial and channel attention mechanisms into the YOLOv5 architecture, ensuring a balance between accuracy and computational efficiency. The network achieves an AP50 of 67.6%, an mAP of 46.7%, and a real-time detection rate with an FPS of 67. Moreover, the study proposes a novel mechanism for estimating the 3D coordinates of detected objects and projecting them onto 3D maps, with an average error of 0.24 m and 0.28 m in the x and y directions, respectively. After being tested and validated with real-world data from a university campus, the proposed method shows promise for improving disinfection efficiency in healthcare facilities by enabling real-time object detection and localization for robot navigation.},
  keywords={Location awareness;Deep learning;Three-dimensional displays;Navigation;Robot kinematics;Medical services;Real-time systems},
  doi={10.1109/WSC60868.2023.10408358},
  ISSN={1558-4305},
  month={Dec},}@ARTICLE{10530230,
  author={Lv, Qinzhe and Fan, Hanxin and Liu, Junliang and Zhao, Yinghai and Xing, Mengdao and Quan, Yinghui},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Multilabel Deep Learning-Based Lightweight Radar Compound Jamming Recognition Method}, 
  year={2024},
  volume={73},
  number={},
  pages={1-15},
  abstract={With the rapid development of electronic countermeasure technology, many active jamming compound scenes pose severe challenges to traditional radar, synthetic aperture radar (SAR), and other detection technologies. The accurate monitoring and recognition of individual jamming types contained in the complex electromagnetic environment can provide valuable prior information for radar countermeasures. However, existing jamming recognition (JR) algorithms suffer from huge models, fewer recognizable jamming types, and weak robustness, which is difficult to apply effectively to the resource-constrained airborne pulse signal real-time analysis instruments. This article proposes a multilabel learning-based lightweight compound JR algorithm to solve these problems, including three key steps. First, the proposed method performs de-chirp, time-frequency (TF) transformation, and grayscale compression preprocessing for radar echoes. Then, an efficient hybrid attention (EHA) mechanism is designed and combined with ShuffleNet v2 to construct a recognition model. Finally, we generate independent multilabel discriminant thresholds based on dual evaluation metrics and a genetic algorithm to improve the recognition effect. The experiment shows that the floating-point operations (FLOPs) of the proposed method are only 0.11%–57.19% of the existing JR methods, the overall recognition accuracy of the measured jamming data is 92.25%, higher than the existing methods of 7.37%–16.73%, and has strong robustness to the fluctuation of radar waveform parameters.},
  keywords={Jamming;Radar;Compounds;Radar countermeasures;Frequency modulation;Time-frequency analysis;Deep learning;Attention mechanism (AM);convolutional neural network (CNN);jamming recognition (JR);multilabel learning (ML)},
  doi={10.1109/TIM.2024.3400337},
  ISSN={1557-9662},
  month={},}@ARTICLE{10044156,
  author={Yang, Lan and Qiao, Chen and Zhou, Huiyu and Calhoun, Vince D. and Stephen, Julia M. and Wilson, Tony W. and Wang, Yuping},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Explainable Multimodal Deep Dictionary Learning to Capture Developmental Differences From Three fMRI Paradigms}, 
  year={2023},
  volume={70},
  number={8},
  pages={2404-2415},
  abstract={Objective: Multimodal-based methods show great potential for neuroscience studies by integrating complementary information. There has been less multimodal work focussed on brain developmental changes. Methods: We propose an explainable multimodal deep dictionary learning method to uncover both the commonality and specificity of different modalities, which learns the shared dictionary and the modality-specific sparse representations based on the multimodal data and their encodings of a sparse deep autoencoder. Results: By regarding three fMRI paradigms collected during two tasks and resting state as modalities, we apply the proposed method on multimodal data to identify the brain developmental differences. The results show that the proposed model can not only achieve better performance in reconstruction, but also yield age-related differences in reoccurring patterns. Specifically, both children and young adults prefer to switch among states during two tasks while staying within a particular state during rest, but the difference is that children possess more diffuse functional connectivity patterns while young adults have more focused functional connectivity patterns. Conclusion and Significance: To uncover the commonality and specificity of three fMRI paradigms to developmental differences, multimodal data and their encodings are used to train the shared dictionary and the modality-specific sparse representations. Identifying brain network differences helps to understand how the neural circuits and brain networks form and develop with age.},
  keywords={Machine learning;Dictionaries;Functional magnetic resonance imaging;Encoding;Task analysis;Costs;Laplace equations;Brain development;dynamic functional connectivity;explainablity;multimodal dictionary learning},
  doi={10.1109/TBME.2023.3244921},
  ISSN={1558-2531},
  month={Aug},}@ARTICLE{10930744,
  author={Ma, Rao and Qian, Mengjie and Gales, Mark and Knill, Kate},
  journal={IEEE Transactions on Audio, Speech and Language Processing}, 
  title={ASR Error Correction Using Large Language Models}, 
  year={2025},
  volume={33},
  number={},
  pages={1389-1401},
  abstract={Error correction (EC) models play a crucial role in refining Automatic Speech Recognition (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC can improve performance and provide domain adaptation for black-box ASR systems. This work investigates the use of large language models (LLMs) for error correction across diverse scenarios. 1-best ASR hypotheses are commonly used as the input to EC models. We propose building high-performance EC models using ASR N-best lists which should provide more contextual information for the correction process. Additionally, the generation process of a standard EC model is unrestricted in the sense that any output sequence can be generated. For some scenarios, such as unseen domains, this flexibility may impact performance. To address this, we introduce a constrained decoding approach based on the N-best list or an ASR lattice. Finally, most EC models are trained for a specific ASR system requiring retraining whenever the underlying ASR system is changed. This paper explores the ability of EC models to operate on the output of different ASR systems. This concept is further extended to zero-shot error correction using LLMs, such as ChatGPT. Experiments on three standard datasets demonstrate the efficacy of our proposed methods for both Transducer and attention-based encoder-decoder ASR systems. In addition, the proposed method can serve as an effective method for model ensembling.},
  keywords={Error correction;Decoding;Hidden Markov models;Biological system modeling;Adaptation models;Training;Data models;Context modeling;Chatbots;Training data;Automatic speech recognition;error correction;large language model;supervised training;zero-shot prompting},
  doi={10.1109/TASLPRO.2025.3551083},
  ISSN={2998-4173},
  month={},}@INPROCEEDINGS{10356596,
  author={Fan, Cong and Guo, Shen and Wumaier, Aishan and Liu, Jiajun},
  booktitle={2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={A cross-attention and Siamese network based model for off-topic detection}, 
  year={2023},
  volume={},
  number={},
  pages={770-777},
  abstract={Off-topic detection analyzes the closeness between a user’s response and a specific topic in a given context or question. The development of off-topic detection techniques stems from the real-world needs of automated scoring systems, as feature-based machine learning methods are only based on shallow features of the text. For example, sentence length, number of words, etc., cannot reflect the relevance of the user’s answer to the topic from the semantic level, resulting in the content of the user’s answer containing some answers that do not have grammatical errors but are not relevant to the topic can also get a higher score, which significantly affects the impartiality and fairness of the automatic scoring system. Therefore, this paper proposes a deep learning model based on the combination of Siamese network and a cross-attention mechanism for the off-topic detection task, which is suitable for automatic short-answer off-topic detection scenarios. By conducting experiments using the publicly available dataset Mohler, the experimental results show that our proposed model is adequate. The experimental results in this paper achieve 91.9% F1 value, 92.5% recall, and 94.1% precision. The F1 value and precision are improved by 3.7% and 3%, respectively, compared to the best-performing baseline model, the Bert-based deep neural network model.},
  keywords={Deep learning;Analytical models;Correlation;Semantics;Feature extraction;Transformers;Data models;BERT;Siamese network;cross-attention;off-topic detection},
  doi={10.1109/ICTAI59109.2023.00118},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10331249,
  author={Ozmen, Goktug C. and Nichols, Christopher J. and Lan, Lan and Moise, Emily and Sugino, Christopher and Erturk, Alper and Inan, Omer T.},
  booktitle={2023 IEEE 19th International Conference on Body Sensor Networks (BSN)}, 
  title={Wearable Active Vibration Sensing for Mid-Activity Knee Health Assessment}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Non-invasive vibration measurements from the knee offer a convenient and affordable alternative to benchtop or biomechanics lab joint health monitoring systems. Recently, joint acoustic emissions (JAEs) measured from the knee were shown to be an indicator of knee health. However, the origin of JAEs is still not fully understood, which limits its acceptance and use by clinical experts. In this proof-of-concept study, rather than relying on the movements of the knee and corresponding frictional rubbing of internal surfaces to produce vibrations, we propose using an active vibration sensing approach with a known vibration source interrogating the knee. We aim to elucidate the linkage between knee vibration characteristics and structural changes in the joint following injuries. We measured tibial vibration responses of two participants using a laser vibrometer system to quantify the frequency band where the most repeatable tibial vibration measurement can be taken. Subsequently, a custom-designed wearable system measured mid-activity tibial vibration characteristics from four participants (five healthy knees and three knees with prior acute injury) during unloaded knee flexion-extensions. An active sensing knee health score was defined as the ratio of the changes in low- to high-frequency response during flexion-extension. Since changes in the boundary of tibia would alter low-frequency response more than high frequency response, we found that increased knee laxity with acute injuries resulted in an increased active sensing knee health score. Our findings demonstrate the potential of active vibration sensing as an interpretable, computationally inexpensive alternative to JAEs for wearable knee health assessment.},
  keywords={Vibrations;Atmospheric measurements;Vibrometers;Wearable computers;Vibration measurement;Particle measurements;Sensors;Frequency measurement;Biomedical monitoring;Injuries;wearable sensing;vibrations;joint health},
  doi={10.1109/BSN58485.2023.10331249},
  ISSN={2376-8894},
  month={Oct},}@INPROCEEDINGS{10612016,
  author={Djartov, Boris and Mostaghim, Sanaz and Papenfuß, Anne and Wies, Matthias},
  booktitle={2024 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={A Learning Classifier System Approach to Time-Critical Decision-Making in Dynamic Alternate Airport Selection}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The goal of the paper is to address the need for methods to handle time-sensitive, human-centered, multi-criteria decision-making problems. In the current literature, prevalent methods rely on expressing decision-maker/stakeholder preferences through weights, ideal points, and trade-off matrices. However, these conventional approaches prove unsuitable for time-constrained, atypical, and stressful situations, such as emergencies. In such scenarios, where both time and additional factors significantly affect decision-making abilities, the effective utilization of advanced decision-making techniques becomes chal-lenging. Therefore, this paper explores the possibility of how an intelligent agent might be used to provide possible courses of action to human decision-makers/stakeholders. The agent will be put to the test to tackle the dynamic alternate airport selection problem. In emergency and time-critical situations, like an engine fire or a medical emergency, there is often a need to select an alternate airport destination dynamically midflight. During such emergencies, a lot of information must be collected and evaluated by the pilots as a basis for the decision-making process. The pilots need to compare multiple characteristics of the available airports and weigh the pros and cons of each. Given the need for clear and interpretable retroactive analysis in decision-making in general and in the aviation field in particular, the focus was placed on more interpretable and explainable models from the field of AI. Due to this, the Learning Classifier System (LCS) is to be the primary model explored. The LCS is trained on a custom dataset composed of various decision-making scenarios. The approach shows promising results and appears to merit further investigation.},
  keywords={Analytical models;Atmospheric modeling;Computational modeling;Decision making;Evolutionary computation;Airports;Time factors;multi-criteria decision-making;multi-attribute decision-making;learning classifier system},
  doi={10.1109/CEC60901.2024.10612016},
  ISSN={},
  month={June},}@INPROCEEDINGS{10470977,
  author={Shen, Siyuan},
  booktitle={2024 IEEE 4th International Conference on Power, Electronics and Computer Applications (ICPECA)}, 
  title={Research on Small Object Detection Algorithm Based on YOLOv5}, 
  year={2024},
  volume={},
  number={},
  pages={937-942},
  abstract={This article introduces an improvement in the YOLOv5 architecture by incorporating the CBAM (Convolutional Block Attention Module) attention module at the neck network end. CBAM is added after each concatenation operation to enhance the focus on small targets and optimize the fusion features in the neck. The role of CBAM is to strengthen the extraction of features by automatically ignoring irrelevant information, focusing on the fusion of crucial features, thereby improving the model's analytical capabilities for complex scenes. Experimental results indicate that the addition of the CBAM module successfully enhances the YOLOv5s model by highlighting key features and suppressing unimportant ones. This results in output feature maps containing more valuable information, significantly improving the accuracy of object detection. This improvement has shown positive effects in small object detection, feature fusion, and model speed.},
  keywords={YOLO;Analytical models;Fuses;Computational modeling;Focusing;Interference;Feature extraction;YOLOv5;small object detection;algorithm},
  doi={10.1109/ICPECA60615.2024.10470977},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10775733,
  author={Liu, Lin and Chen, Qishuo and Jiang, Chuanwen},
  booktitle={2024 6th International Conference on Energy, Power and Grid (ICEPG)}, 
  title={Load Forecasting Method Based on CEEMDAN Signal Decomposition and Improved BiLSTM}, 
  year={2024},
  volume={},
  number={},
  pages={1870-1875},
  abstract={The accuracy of load forecasting in the power system becomes increasingly important with the step-by-step construction and improvement of new types of power systems. In this article, a method using the CEEMDAN algorithm for modal decomposition of sequences is proposed, where the decomposed sequences are separately analyzed for prediction and then reconstructed to form the complete signal sequence. The main prediction model uses BiLSTM. Compared to traditional LSTM, the ability of bidirectional reading allows BiLSTM to better understand and model the contextual information in the sequence, especially in tasks involving bidirectional dependencies where the effect is significant. Finally, the Love Evolutionary Optimization algorithm was used to improve the hyperparameters of the prediction model, further enhancing the prediction accuracy. By comparing the prediction accuracy results under different methods, further evidence was provided to support the advanced nature of this prediction method.},
  keywords={Training;Accuracy;Load forecasting;Predictive models;White noise;Prediction algorithms;Power systems;Signal resolution;Optimization;Load modeling;Load Forecasting;CEEMDAN;BiLSTM;LEA},
  doi={10.1109/ICEPG63230.2024.10775733},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10665643,
  author={Zhang, Qiyu and Han, Hongui and Li, Fangyu and Du, Yongping},
  booktitle={2024 14th Asian Control Conference (ASCC)}, 
  title={Efficient Differentiable Architecture Search with Backbone and FPN for Object Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1908-1913},
  abstract={Differentiable architecture search (DARTS) is an important branch of neural architecture search (NAS), aiming at searching the optimal network structure efficiently. However, the backbone and FPN of the detection model tend to suffer from over-parameterization effects during the search process, resulting in DARTS failing to efficiently search the detection network structure. In this paper, we propose an efficient differentiable architecture search (EDARTS-DET) with backbone and FPN for object detection. Firstly, the joint search space of backbone and FPN networks is designed to facilitate DARTS to search the detection network as a whole at one shot. Secondly, an architecture operation mask weight sharing mechanism is proposed to effectively reduce the search memory occupation and computational cost. Finally, an attention-based partial channel selection strategy is introduced to select important channels to be fed into the search space. The experimental results show that the proposed EDARTS-DET achieves lower search consumption time and memory utilization in the COCO dataset and the Haier appliance disassembly dataset compared with other SOTA methods. Meanwhile, the searched detection networks are also improved in terms of detection performance, verifying that the proposed EDARTS-DET method achieves a well-balanced performance and efficiency.},
  keywords={Performance evaluation;Attention mechanisms;Image edge detection;Memory management;Computer architecture;Object detection;Search problems;Neural architecture search;object detection;efficient;DARTS},
  doi={},
  ISSN={2770-8373},
  month={July},}@INPROCEEDINGS{9970688,
  author={Wang, Yilei and Wei, Wei and Xin, Zhenfang and Li, Dashuang},
  booktitle={2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)}, 
  title={Research on Small Face Detection in UAV Command and Control System}, 
  year={2022},
  volume={},
  number={},
  pages={69-72},
  abstract={Small face detection technology was introduced and researched in depth to meet the needs of crowd monitoring in specific scenes and corresponding decision-making in UAV command and control systems. At present, the focus of small face detection research is mainly on general deep neural network object detection algorithms, but there is a lack of relevant research on small face detection in UAV command and control systems. In this study, we analyze the current challenges of UAV small face detection and discuss the research ideas and optimization schemes in the direction of data enhancement and feature enhancement. The cutting-edge algorithm is followed up and the experimental results are verified.},
  keywords={Command and control systems;Deep learning;Neural networks;Object detection;Autonomous aerial vehicles;Feature extraction;Social intelligence;object detection;computer vision;command and control system;UAV(unmanned aerial vehicle);CNN(convolutional neural network)},
  doi={10.1109/ICCSI55536.2022.9970688},
  ISSN={},
  month={Nov},}@ARTICLE{9854114,
  author={Soliman, Moamen M. and Ganti, Venu G. and Inan, Omer T.},
  journal={IEEE Sensors Journal}, 
  title={Toward Wearable Estimation of Tidal Volume via Electrocardiogram and Seismocardiogram Signals}, 
  year={2022},
  volume={22},
  number={18},
  pages={18093-18103},
  abstract={The current coronavirus disease (COVID-19) pandemic highlights the critical importance of ubiquitous respiratory health monitoring. The two fundamental elements of monitoring respiration are respiration rate (the frequency of breathing) and tidal volume [(TV) the volume of air breathed by the lungs in each breath]. Wearable sensing systems have been demonstrated to provide accurate measurement of respiration rate, but TV remains challenging to measure accurately with wearable and unobtrusive technology. In this work, we leveraged electrocardiogram (ECG) and seismocardiogram (SCG) measurements obtained with a custom wearable sensing patch to derive an estimate of TV from healthy human participants. Specifically, we fused both an ECG-derived respiratory signal (EDR) and an SCG-derived respiratory signal (SDR) and trained a machine learning model with gas rebreathing as the ground truth to estimate TV. The respiration cycle modulates ECG and SCG signals in multiple different ways that are synergistic. Thus, here, we extract EDRs and SDRs using a multitude of different demodulation techniques. The extracted features are used to train a subject-independent machine learning model to accurately estimate TV. By fusing the extracted EDRs and SDRs, we were able to estimate the TV with a root-mean-square error (RMSE) of 181.45 mL and Pearson correlation coefficient ( ${r}$ ) of 0.61, with a global subject-independent model. We further show that SDRs are better TV estimators than EDRs. Among SDRs, amplitude modulation (AM) SCG features are the most correlated with TV. We demonstrated that fusing EDRs and SDRs can result in a moderately accurate estimation of the TV using a subject-independent model. Additionally, we highlight the most informative features for estimating TV. This work presents a significant step toward achieving continuous, calibration-free, and unobtrusive TV estimation, which could advance the state of the art in wearable respiratory monitoring.},
  keywords={TV;Electrocardiography;Feature extraction;Sensors;Monitoring;Biomedical monitoring;Band-pass filters;Continuous respiratory monitoring;electrocardiogram (ECG);seismocardiogram (SCG);tidal volume (TV) estimation;wearable sensing},
  doi={10.1109/JSEN.2022.3196601},
  ISSN={1558-1748},
  month={Sep.},}@INPROCEEDINGS{10392706,
  author={V, Viswanatha and A C, Ramachandra and B.D, Parameshachari and Sharma, Aditya Kumar},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={Brain Stroke Prediction using Decision Tree Algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Stroke is a condition that occurs due to interruption or reduction in the blood supply to the brain. Due to this, the neurons in the brain suffer hypoxia which leads to cell injury or even death of the brain cells if not treated in time. This causes permanent or long-term injury and affects the person’s life forever. Hence, early detection and prevention of stroke are essential as it is one amongst of the top causes of mortality and morbidity worldwide. Decision tree methods have been developed as a useful tool for this and machine learning (ML) models have shown promise in stroke prediction. Decision trees remain suitable for use in healthcare applications because they offer interpretability and can handle both categorical and numerical data. A dataset containing demographic figures, medical record, lifestyle features, and other pertinent variables are used by this ML model. The data set is analyzed using decision tree method, which identifies risk factors for strokes and captures intricate interactions between predictors. The handling of missing values and outliers eliminates the need of intensive preparation. By using different parameters in the decision tree algorithm this model has successfully given about 93.4% accuracy on the validation dataset. This accuracy was given when the depth of the tree was set to 4. Depths of 9 and 16 were also implemented. The depth of 9 gave us 91.4% accuracy on the validation data whereas the depth of 16 gave us 91% accuracy. It is to conclude that adopting a simple model by applying the decision tree approach provides us the best results after training this model on various parameters.},
  keywords={Training;Computational modeling;Neurons;Medical services;Predictive models;Prediction algorithms;Brain modeling;Stroke;machine learning;decision tree classification},
  doi={10.1109/EASCT59475.2023.10392706},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9870361,
  author={Delgado-Osuna, José A. and Garcia-Martinez, Carlos and Ventura, Sebastián},
  booktitle={2022 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Smart Operators for Inducing Colorectal Cancer Classification Trees with PonyGE2 Grammatical Evolution Python Package}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={Colorectal cancer is a disease that affects many people and requires a multidisciplinary approach, involving significant human and economic resources. We have been provided with a tabular dataset with 1.5 thousand cases of this disease. We are interested in producing interpretable classifiers for predicting the occurrence of complications. Grammatical Evolution has extensively been used for machine learning problems. In particular, it can be used to induce interpretable decision trees, with the advantage of allowing the practitioner to easily control the language by means of the grammar. PonyGE2 [1], [2] is a Python package that provides data scientists with Grammatical Evolution algorithms, which can be configured to their needs quite easily. In addition, and thanks to the benefits of the Python programming language, PonyGE2 is currently becoming more and more popular. However, the capabilities of PonyGE2 for inducing classification trees are still subject of improvement. In particular, it only uses simple equality conditions and requires to encode feature names and values with numbers. We have developed some smart operators for PonyGE2, which, not only enhance the framework in interpretability and performance when dealing with our colorectal cancer dataset, but also allows to produce results comparable to those of the widely known heuristic methods C4.5 and CART. We show how they could be applied to other datasets, and how they affect performance in our case.},
  keywords={Machine learning algorithms;Machine learning;Evolutionary computation;Germanium;Classification algorithms;Grammar;Task analysis;Grammatical Evolution;Classification Trees;Heterogeneous features;Colorectal Cancer},
  doi={10.1109/CEC55065.2022.9870361},
  ISSN={},
  month={July},}@ARTICLE{9913342,
  author={Moosbauer, Julia and Binder, Martin and Schneider, Lennart and Pfisterer, Florian and Becker, Marc and Lang, Michel and Kotthoff, Lars and Bischl, Bernd},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers}, 
  year={2022},
  volume={26},
  number={6},
  pages={1336-1350},
  abstract={Automated hyperparameter optimization (HPO) has gained great popularity and is an important component of most automated machine learning frameworks. However, the process of designing HPO algorithms is still an unsystematic and manual process: new algorithms are often built on top of prior work, where limitations are identified and improvements are proposed. Even though this approach is guided by expert knowledge, it is still somewhat arbitrary. The process rarely allows for gaining a holistic understanding of which algorithmic components drive performance and carries the risk of overlooking good algorithmic design choices. We present a principled approach to automated benchmark-driven algorithm design applied to multifidelity HPO (MF-HPO). First, we formalize a rich space of MF-HPO candidates that includes, but is not limited to, common existing HPO algorithms and then present a configurable framework covering this space. To find the best candidate automatically and systematically, we follow a programming-by-optimization approach and search over the space of algorithm candidates via Bayesian optimization. We challenge whether the found design choices are necessary or could be replaced by more naive and simpler ones by performing an ablation analysis. We observe that using a relatively simple configuration (in some ways, simpler than established methods) performs very well as long as some critical configuration parameters are set to the right value.},
  keywords={Optimization;Approximation algorithms;Prediction algorithms;Machine learning algorithms;Bayes methods;Software algorithms;Mathematical models;Algorithm analysis;algorithm design;automated machine learning (AutoML);hyperparameter optimization (HPO);multifidelity},
  doi={10.1109/TEVC.2022.3211336},
  ISSN={1941-0026},
  month={Dec},}@INPROCEEDINGS{10393526,
  author={Patil, Namratha V and Biradar, Rajashree V},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={An Innovative Fusion of Deep Learning and the Differential Analyzer Approach (DAA-Deep model) for Enhanced Skin Cancer Detection}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Skin cancer, a globally prevalent and potentially life-threatening disease, underscores the critical importance of early detection and treatment. Traditional diagnostic methods heavily rely on visual inspections conducted by dermatologists, which can be prone to human error and constrained by resource limitations. Recent advancements in machine learning and deep learning techniques offer promising prospects for automating the skin cancer detection process. However, this domain still faces persistent challenges, particularly the need for effective feature selection methods to improve model performance, interpretability, and computational efficiency. The primary goal of this research is to significantly elevate the precision and efficiency of skin cancer detection. To achieve this, a novel fusion of the Differential Analyzer Approach (DAA) with deep learning models has been proposed. This work leverages the ISIC 2018 dataset of skin cancer images, intricate convolutional neural network (CNN) architectures, and a seamlessly integrated DAA mechanism. The proposed DAA-Deep learning model outperforms conventional deep learning models, showcasing higher accuracy and improved diagnostic capabilities. The results achieved by the DAA-Deep model surpasses the performance of numerous existing models, exhibiting a remarkable accuracy rate of 96% along with an impressive AUC (Area under the ROC Curve) value of 0.99.},
  keywords={Deep learning;Computers;Computational modeling;Differential equations;Computer architecture;Medical services;Feature extraction;DAA-Deep;CNN;DAA;ResNet50;Skin cancer;Deep learning;feature extraction},
  doi={10.1109/EASCT59475.2023.10393526},
  ISSN={},
  month={Oct},}@ARTICLE{10666862,
  author={Bodenham, Matthew and Kung, Jaeha},
  journal={IEEE Access}, 
  title={Skipformer: Evolving Beyond Blocks for Extensively Searching On-Device Language Models With Learnable Attention Window}, 
  year={2024},
  volume={12},
  number={},
  pages={124428-124439},
  abstract={Deployment of language models to resource-constrained edge devices is an uphill battle against their ever-increasing size. The task transferability of language models makes deployment to the edge an attractive application. Prior neural architecture search (NAS) works have produced hardware-efficient transformers, but often overlook some architectural features in favor of efficient NAS. We propose a novel evolutionary NAS with large and flexible search space to encourage the exploration of previously unexplored transformer architectures. Our search space allows architectures to vary through their depth and skip connections to transfer information anywhere inside the architecture; Skipformer, the top searched model, displays these novel architectural features. To further increase Skipformer efficiency, we learn a CUDA-accelerated attention window size at each self-attention layer during training. Skipformer achieves 23.3% speed up and requires 19.2% less memory on NVIDIA Jetson Nano with negligible accuracy loss on GLEU benchmark compared to GPT-2 Small.},
  keywords={Computational modeling;Transformers;Natural language processing;Computer architecture;Context modeling;Training;Language models;neural architecture search;on-device inference;transformers},
  doi={10.1109/ACCESS.2024.3420232},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11035367,
  author={V, Bala Dhandayuthapani and Ravindran, Gobinath and Almoussawi, Zainababed and V, Pradeep and Gayathri, N.},
  booktitle={2025 4th International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE)}, 
  title={Bidirectional Long Short-Term Memory based Gated Convolutional Network with Attention Mechanism for Traffic Flow Prediction}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The traffic flow prediction becomes significant role in assuring a safety as well as solving traffic congestions. The difficult spatial temporal relationships of traffic road capturing are significant and this is the greater challenge. The manual approaches significantly capture the spatial and temporal relationships as well as enhance the model complexity through examining both connected and unconnected roads. To address to challenge, this research develops a Bi-directional Long Short-Term Memory (BiLSTM) with Attention Mechanism for the efficient traffic flow prediction. This research utilized the two datasets named PEMS04 and PEMS08 for validating a performance of the developed method. In the pre-processing phase, a normalization technique is utilized for normalizing the traffic data. A GCN is utilized for extracting the features and BiLSTM is utilized for classification. The developed approach reaches superior results through considering large performance indices and it achieves the accuracy of 0.9467 and 0.9534 in PEMS04 and PEMS08 dataset when compared to existing methods like Local and Global spatial GCN (T-LGGCN) and Spatial-Temporal Attention Graph Convolutional Network on Edge Cloud approach (STAGCN-EC).},
  keywords={Attention mechanisms;Accuracy;Roads;Computational modeling;Bidirectional long short term memory;Logic gates;Feature extraction;Prediction algorithms;Safety;Traffic congestion;bidirectional long short-term memory;gated convolutional network;spatial;temporal;traffic flow prediction},
  doi={10.1109/ICDCECE65353.2025.11035367},
  ISSN={},
  month={April},}@INPROCEEDINGS{10393716,
  author={Wu, Quanquan},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)}, 
  title={Development of Resources Library System Based on J2EE Layered Architecture}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={J2EE is essentially a distributed server application design environment that provides a component-based approach to designing, developing, assembling, and deploying enterprise-level applications. It provides a good mechanism for building application systems with scalability, flexibility and ease of maintenance. Based on the J2EE layered architecture, this paper designs the Chinese classics reading resource library system. The core functions of the system are composed of a resource storage subsystem, a resource maintenance subsystem, a resource utilization subsystem, and a resource management subsystem, which provide an integrated platform for the recitation of Chinese classics. At the same time, in order to improve the efficiency of resource use, a user-based collaborative filtering recommendation algorithm mathematical model is constructed to recommend among users with the same interests, so as to solve the user's resource selection problem. It is found that the most effective solution to solve the problems of cold start, sparsity, scalability, diversity and interpretability of traditional collaborative filtering algorithms is to use auxiliary information such as rating data, social network information and other information, combined with deep learning and data mining technologies, to improve the recommendation effect and enhance user satisfaction.},
  keywords={Java;Social networking (online);Collaborative filtering;Scalability;Systems architecture;Maintenance engineering;Libraries;J2EE layered architecture;classics reading;resources library system;collaborative filtering recommendation algorithm},
  doi={10.1109/EASCT59475.2023.10393716},
  ISSN={},
  month={Oct},}@ARTICLE{10384325,
  author={Zhou, Xiaokang and Yang, Qiuyue and Zheng, Xuzhe and Liang, Wei and Wang, Kevin I-Kai and Ma, Jianhua and Pan, Yi and Jin, Qun},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={Personalized Federated Learning With Model-Contrastive Learning for Multi-Modal User Modeling in Human-Centric Metaverse}, 
  year={2024},
  volume={42},
  number={4},
  pages={817-831},
  abstract={With the flourish of digital technologies and rapid development of 5G and beyond networks, Metaverse has become an increasingly hotly discussed topic, which offers users with multiple roles for diversified experience interacting with virtual services. How to capture and model users’ multi-platform or cross-space data/behaviors become essential to enrich people with more realistic and immersed experience in Metaverse-enabled smart applications over 5G and beyond networks. In this study, we propose a Personalized Federated Learning with Model-Contrastive Learning (PFL-MCL) framework, which may efficiently enhance the communication and interaction in human-centric Metaverse environments by making use of the large-scale, heterogeneous, and multi-modal Metaverse data. Differing from the conventional Federated Learning (FL) architecture, a multi-center aggregation structure to learn multiple global models based on the changes of dynamically updated local model weights, is developed in global, while a hierarchical neural network structure which includes a personalized module and a federated module to tackle both issues on data heterogeneity and model heterogeneity, is designed in local, so as to enhance the performance of PFL with unique characteristics of Metaverse data. In particular, a two-stage iterative clustering algorithm with a more precise initialization is developed to facilitate the personalized global aggregation with dynamically updated multiple aggregation centers. A personalized multi-modal fusion network is constructed to greatly reduce the computational cost and feature dimensions from the high-dimensional heterogeneous inputs for more efficient cross-modal fusion, based on a hierarchical shift-window attention mechanism and a newly designed bridge attention mechanism. A MCL scheme is then incorporated to speed up the model convergence with less communication overload between the local federated module and global model, while an embedding layer which effectively enables the delivered global model to better adapt to the local personality in each client is further integrated. Compared with five baseline methods, experiment and evaluation results based on two different real-world datasets demonstrate the excellent performance of our proposed PFL-MCL model in a fine-grain personalized training strategy, toward more efficient communication and networking among human-centric Metaverse enabled smart applications.},
  keywords={Metaverse;Data models;Adaptation models;Training;Solid modeling;Computational modeling;Convergence;Personalized federated learning;model-contrastive learning;attention mechanism;multi-modal fusion;human-centric;metaverse},
  doi={10.1109/JSAC.2023.3345431},
  ISSN={1558-0008},
  month={April},}@ARTICLE{10121673,
  author={Akbar, Shahid and Mohamed, Heba G. and Ali, Hashim and Saeed, Aamir and Khan, Aftab Ahmed and Gul, Sarah and Ahmad, Ashfaq and Ali, Farman and Ghadi, Yazeed Yasin and Assam, Muhammad},
  journal={IEEE Access}, 
  title={Identifying Neuropeptides via Evolutionary and Sequential Based Multi-Perspective Descriptors by Incorporation With Ensemble Classification Strategy}, 
  year={2023},
  volume={11},
  number={},
  pages={49024-49034},
  abstract={Neuropeptides (NPs) are a kind of neuromodulator/ neurotransmitter that works as signaling molecules in the central nervous system, and perform major roles in physiological and hormone regulation activities. Recently, machine learning-based therapeutic agents have gained the attention of researchers due to their high and reliable prediction results. However, the unsatisfactory performance of the existing predictors is due to their high execution cost and minimum predictive results. Therefore, the development of a reliable prediction is highly indispensable for scientists to effectively predict NPs. In this study, we presented an automatic and computationally effective model for identifying of NPs. The evolutionary information is formulated using a bigram position-specific scoring matrix (Bi-PSSM) and K-spaced bigram (KSB). Moreover, for noise reduction, a discrete wavelet transform (DWT) is utilized to form Bi-PSSM_DWT and KSB_DWT based high discriminative vectors. In addition, one-hot encoding is also employed to collect sequential features from peptide samples. Finally, a multi-perspective feature set of sequential and embedded evolutionary information is formed. The optimum features are chosen from the extracted features via Shapley Additive exPlanations (SHAP) by evaluating the contribution of the extracted features. The optimal features are trained via six classification models i.e., XGB, ETC, SVM, ADA, FKNN, and LGBM. The predicted labels of these learners are then provided to a genetic algorithm to form an ensemble classification approach. Hence, our model achieved a higher predictive accuracy of 94.47% and 92.55% using training sequences and independent sequences, respectively. Which is  $\sim $ 3% highest predictive accuracy than present methods. It is suggested that our presented tool will be beneficial and may execute a substantial role in drug development and research academia. The source code and all datasets are publicly available at https://github.com/shahidawkum/Target-ensC_NP.},
  keywords={Peptides;Amino acids;Feature extraction;Training;Proteins;Predictive models;Encoding;Neuropeptides;ensemble classification;multi-perspective vector;discrete wavelet transform;SHAP analysis;bigram-position specific scoring matrix},
  doi={10.1109/ACCESS.2023.3274601},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9945568,
  author={Ye, Te and Zhang, Zizhen and Chen, Jinbiao and Wang, Jiahai},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Weight-Specific-Decoder Attention Model to Solve Multiobjective Combinatorial Optimization Problems}, 
  year={2022},
  volume={},
  number={},
  pages={2839-2844},
  abstract={The multiobjective combinatorial optimization problems (MOCOPs) have a wide range of real-world applications. Designing an effective algorithm has an important and practical significance. Due to the huge search space and limited time, it is generally difficult to obtain the optimal solution of this kind of problem by traditional exact and heuristic algorithms. Recently, learning-based algorithms have achieved good results in solving MOCOPs, but the quality and diversity of found solutions can be further improved. In this paper, we propose a Weight-Specific-Decoder Attention Model (WSDAM) to better approximate the whole Pareto set. It embeds a weight-adaptive layer into the decoder to concentrate on the information of different weight vectors. During the model training, the weight vector is sampled from the Dirichlet distribution, which can further strengthen the learning of boundary solutions. We evaluate our method on two classic MOCOPs, i.e., the multiobjective traveling salesman problem (MOTSP) and multiobjective capacitated vehicle routing problem (MOCVRP). The experimental results show that our proposed method outperforms current state-of-the-art learning-based methods in both solution quality and generalization ability.},
  keywords={Training;Learning systems;Vehicle routing;Reinforcement learning;Traveling salesman problems;Approximation algorithms;Search problems;Multiobjective combinatorial optimization;Traveling Salesman Problem;Attention mechanism;deep reinforcement learning},
  doi={10.1109/SMC53654.2022.9945568},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{9764671,
  author={Su, Yuchao and Lin, Qiuzhen and Ming, Zhong and Tan, Kay Chen},
  journal={IEEE Transactions on Cybernetics}, 
  title={Adapting Decomposed Directions for Evolutionary Multiobjective Optimization}, 
  year={2023},
  volume={53},
  number={10},
  pages={6289-6302},
  abstract={Decomposition methods have been widely employed in evolutionary algorithms for tackling multiobjective optimization problems (MOPs) due to their good mathematical explanation and promising performance. However, most decomposition methods only use a single ideal or nadir point to guide the evolution, which are not so effective for solving MOPs with extremely convex/concave Pareto fronts (PFs). To solve this problem, this article proposes an effective method to adapt decomposed directions (ADDs) for solving MOPs. Instead of using one single ideal or nadir point, each weight vector has one exclusive ideal point in our method for decomposition, in which the decomposed directions are adapted during the search process. In this way, the adapted decomposed directions can evenly and entirely cover the PF of the target MOP. The effectiveness of our method is analyzed theoretically and verified experimentally when embedding it into three representative multiobjective evolutionary algorithms (MOEAs), which can significantly improve their performance. When compared to seven competitive MOEAs, the experiments also validate the advantages of our method for solving 39 artificial MOPs with various PFs and one real-world MOP.},
  keywords={Shape;Statistics;Sociology;Optimization;Convergence;Evolutionary computation;Software engineering;Decomposed direction;ideal point;multiobjective optimization;weight vector},
  doi={10.1109/TCYB.2022.3165557},
  ISSN={2168-2275},
  month={Oct},}@INPROCEEDINGS{10337166,
  author={Littlefield, Nickolas and Moradi, Hamidreza and Amirian, Soheyla and Kremers, Hilal Maradit and Plate, Johannes F. and Tafti, Ahmad P.},
  booktitle={2023 IEEE 11th International Conference on Healthcare Informatics (ICHI)}, 
  title={Enforcing Explainable Deep Few-Shot Learning to Analyze Plain Knee Radiographs: Data from the Osteoarthritis Initiative}, 
  year={2023},
  volume={},
  number={},
  pages={252-260},
  abstract={The use of fast, accurate, and automatic knee radiography analysis is becoming increasingly important in orthopedics, and it is becoming more important in improving patient-specific diagnosis, prognosis, and treatment. Precise characterization of plain knee radiographs can greatly impact patient care, as they are usually used in preoperative and intraoperative planning. Rapid yet, deep learning medical image analysis has already shown success in a variety of knee image analysis tasks, ranging from knee joint area localization to joint space segmentation and measurement, with almost a human-like performance. However, there are several fundamental challenges that stop deep learning methods to obtain their full potential in a clinical setting such as orthopedics. These include the need for a large number of gold-standard, manually annotated training images and a lack of explainability and interpretability. To address these challenges, this study is the first to present an explainable deep few-shot learning model that can localize the knee joint area and segment the joint space in plain knee radiographs, using only a small number of manually annotated radiographs. The accuracy performance of the proposed method was thoroughly and experimentally evaluated using various image localization and segmentation measures, and it was compared to baseline models that utilized large-scale fully-annotated training datasets. The current deep few-shot learning methods achieved an average Intersection over Union (IoU) of 0.94 and a mean Average Precision @0.5 of 0.98, using 10-shot learning in the localization of the knee joint area, and an average IoU of 0.91 in the knee joint space segmentation using only 10 manually annotated radiographs.},
  keywords={Location awareness;Training;Deep learning;Image segmentation;Image analysis;Planning;Diagnostic radiography;Few-shot learning;Knee joint space segmentation;Knee joint area localization;Artificial Intelligence},
  doi={10.1109/ICHI57859.2023.00042},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{10543897,
  author={Mann, Aryam and Kiran, M Vamsi and Nithya, B. and Das, Rishav},
  booktitle={2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)}, 
  title={Network Intrusion Detection with Feature Elimination and Selection Using Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the contemporary digital landscape, marked by an ever-increasing number of individuals seeking to exploit vulnerabilities in systems and cause immense damage to users, it becomes imperative for users to safeguard their systems, detect threats correctly, and carry out an appropriate response. Implementing Intrusion Detection Systems (IDS) is vital for monitoring network traffic, spotting threats, and swiftly alerting users to suspicious activities. It is categorized into two parts: Network-based (NIDS) and Host-based (HIDS). NIDS is strategically placed and monitors data flow across devices. A major NIDS challenge is the prevalence of false positives and, more critically, false negatives, potentially allowing undetected threats to cause significant harm. To improve intrusion detection, a new approach using Feature Selection and Deep Learning framework is used for Network Intrusion Detection System. The study analyzes metrics like accuracy, precision, F1 score, and confusion matrix.},
  keywords={Deep learning;Training;Measurement;Computational modeling;Telecommunication traffic;Predictive models;Network security},
  doi={10.1109/ICETCS61022.2024.10543897},
  ISSN={},
  month={April},}@INPROCEEDINGS{11020125,
  author={Dhanaraju, Muthumanickam and Ramalingam, Kumaraperumal and Moorthi, Nivas Raj and C, Poongodi},
  booktitle={2025 Fourth International Conference on Smart Technologies, Communication and Robotics (STCR)}, 
  title={Vertical Spatial Prediction of Soil Salinity and Management Zoning for Perambalur District of Tamil Nadu, India}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Monitoring and managing soil salinity is vital due to its negative impact on agricultural production. Electrical conductivity (EC) at different profile levels is a baseline for assessing salinity and salt accumulation across soil horizons. Random Forest algorithm was applied to predict EC and Exchangeable Sodium Percentage (ESP) in Perambalur district at depths of 0–20 cm, 20–40 cm, and 40–80 cm, using 1159 soil samples obtained via conditioned Latin hypercube sampling. SCORPAN-based environmental covariates (32 nos) generate the model variables, with analysis of the percentage contribution of each environmental variable prediction process through Permutation Feature Importance analysis for identifying Elevation, Physiography, Geomorphology, Annual Mean Air Temperature, and Annual Mean Precipitation. Model performance was validated using a 70:30 holdback procedure and metrics (RMSE, NSE, and RPIQ). For EC, RMSE ranged from 0.20 to 0.28 dS m-1, NSE from 0.40 to 0.58, and RPIQ from 1.06 to 1.26, respectively. For ESP, RMSE ranged from 3.40 to 3.54, NSE from 0.42 to 0.64, and RPIQ from 1.32 to 1.68. Using vertical predictions of EC and ESP, management zones were delineated via fuzzy C-means clustering at a 30 m resolution. These zones and spatial salinity distribution insights enhance the feasibility of targeted mitigation practices through informed policy decisions.},
  keywords={Measurement;Salt;Salinity (geophysical);Sea measurements;Soil;Predictive models;Prediction algorithms;Spatial resolution;Random forests;Monitoring;Salinity;Random Forest;EC;ESP;Management Zones},
  doi={10.1109/STCR62650.2025.11020125},
  ISSN={},
  month={May},}@INPROCEEDINGS{10761164,
  author={Cheng, Tao and Xiao, Jiarong},
  booktitle={2024 IEEE 7th International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
  title={Research on short-term power load forecasting method of microgrid based on machine learning}, 
  year={2024},
  volume={},
  number={},
  pages={1122-1127},
  abstract={Reasonable optimal scheduling can effectively guarantee the economy, environmental protection and stability of microgrid operation, and reliable load prediction data is the most powerful basis for formulating optimal scheduling. Therefore, the accuracy of load prediction of microgrid directly determines the feasibility of scheduling scheme. To solve the problem that the traditional load forecasting model is not effective in time series modeling, and the TCN-Attention short-term load forecasting model is proposed. This paper takes the time convolutional network as the basis model, so the origin and structure of LSTM network and TCN network are analyzed in detail, and the TCN prediction model is built and compared with the LSTM prediction model. The simulation results show that the TCN model has higher prediction accuracy. In order to further improve the accuracy of the TCN model, considering the advantage of Atention mechanism to assign weights adaptively according to the importance of features, a short-term load prediction model of TCN-ATTENTION was constructed and experimental simulation was carried out. The results show that compared with the single TCN model, the accuracy of the TCN-attention model is increased by 0.49%, reaching 96.42%. Secondly, aiming at the problem of information hybridisation of load data and not obvious change rule, a data decomposition method based on WOA-VMD is proposed. The minimum envelope entropy of the variational mode decomposition algorithm is taken as the fitness function of the whale optimization algorithm, and the optimal parameter combination of the mode number k of the VD algorithm and the quadratic penalty factor a is obtained through iterative optimization. Then, the optimal parameter combination is substituted into the VMD algorithm, and the original one-dimensional load data is decomposed into multiple eigenmode functions with different frequencies but with regular changes. Each IMF component is taken as the input load feature of the TCN-Attention model. After that, the above two models were combined. At the same time, in order to further improve the prediction accuracy of the model, Pearson correlation coefficient was used to screen out the daily maximum temperature, minimum temperature and average temperature as the meteorological feature input, and the date type factor was reconstructed as the date type feature input. Finally, based on all the above methods, a short-term load forecasting method based on WOA-VMD-CN-Attention model is proposed. Through experimental simulation, the MAE, RMSE, MAPE and R indexes of the proposed model in this paper are 87.11MW, 111.16MW, 1.01% and 99.26%, respectively. Compared with other models, the prediction accuracy of this model is the highest High.},
  keywords={Analytical models;Adaptation models;Accuracy;Load forecasting;Optimal scheduling;Microgrids;Predictive models;Data models;Long short term memory;Load modeling;microgrid;short-term load forecasting;sequential convolutional networks},
  doi={10.1109/ICISCAE62304.2024.10761164},
  ISSN={2770-663X},
  month={Sep.},}@INPROCEEDINGS{10213160,
  author={Yang, Xin and Shao, Jie and Yang, Shiyilin and Wang, Xingxing and Chen, Xin},
  booktitle={2023 3rd International Symposium on Computer Technology and Information Science (ISCTIS)}, 
  title={Feature Extraction and Classification Recognition of Ship Radiated Noise Based on 2D-ACVMD}, 
  year={2023},
  volume={},
  number={},
  pages={619-623},
  abstract={Due to the complex Marine environment and many noise components, it is very challenging to classify ship radiated noise. A time-frequency feature extraction and recognition method based on 2D-ACVMD and the Transformer network is proposed in this paper. After the ship radiated noise signal is preprocessed, it is analyzed by AOK time-frequency, and then the time-frequency graph is decomposed by 2D-ACVMD. Finally, the texture features of gray co-occurrence matrix are extracted from the decomposed time-frequency graph. Transformer network is selected as a classifier for classification recognition. Based on the ShipsEar dataset, with classification accuracy up to 98.8%, the experimental results show that the proposed method is feasible and advanced.},
  keywords={Time-frequency analysis;Information science;Feature extraction;Transformers;Matrix decomposition;Marine vehicles;Genetic algorithms;component;Ship radiated noise;2D-ACVMD;Optimal adaptation kernel;Transfomer},
  doi={10.1109/ISCTIS58954.2023.10213160},
  ISSN={},
  month={July},}@INPROCEEDINGS{10993030,
  author={Ghosh, Subhadip and Gebru, Endalk Y. and Kashyap, Chandramouli V. and Harjani, Ramesh and Sapatnekar, Sachin S.},
  booktitle={2025 Design, Automation & Test in Europe Conference (DATE)}, 
  title={Accelerating OTA Circuit Design: Transistor Sizing Based on a Transformer Model and Precomputed Lookup Tables}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Device sizing is crucial for meeting performance specifications in operational transconductance amplifiers (OTAs), and this work proposes an automated sizing framework based on a transformer model. The approach first leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit and its specifications into transformer-friendly sequential data. A specialized tokenization approach is applied to the sequential data to expedite the training of the transformer on a diverse range of OTA topologies, under multiple specifications. Under specific performance constraints, the trained transformer model is used to accurately predict DP-SFG parameters in the inference phase. The predicted DP-SFG parameters are then translated to transistor sizes using a precomputed look-up table-based approach inspired by the 9m / I d methodology. In contrast to previous conventional or machine-learning-based methods, the proposed framework achieves significant improvements in both speed and computational efficiency by reducing the need for expensive SPICE simulations within the optimization loop; instead, almost all SPICE simulations are confined to the one-time training phase. The method is validated on a variety of unseen specifications, and the sizing solution demonstrates over 90% success in meeting specifications with just one SPICE simulation for validation, and 100% success with 3–5 additional SPICE simulations.},
  keywords={Training;Computational modeling;Transformers;SPICE;Tokenization;Topology;Computational efficiency;Transistors;Integrated circuit modeling;Transconductance},
  doi={10.23919/DATE64628.2025.10993030},
  ISSN={1558-1101},
  month={March},}@ARTICLE{10975062,
  author={Huang, Sibo and Zhu, Guijie and Tang, Jiaming and Li, Weixiong and Fan, Zhun},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Multi-Perspective Semantic Segmentation of Ground Penetrating Radar Images for Pavement Subsurface Objects}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Effective infrastructure health monitoring is crucial within transportation cyber-physical systems, where accurate road health detection is vital for ensuring road safety and the stability of intelligent transportation systems. To address the challenges of identifying pavement subsurface objects using 3D ground penetrating radar (GPR) data, we propose a multi-perspective cascading recognition method that integrates B-scan and C-scan images. This method is built on a lightweight dual-stream semantic segmentation model called AttnGPRNet, developed in this work to improve feature extraction through attention mechanisms and enhance subsurface object recognition. Initially, the model segments B-scan images to identify potential target regions, followed by more precise segmentation of 3L-C-scan images based on preliminary results. Additionally, we constructed a multi-view dataset using 3D GPR scans from over 100 kilometers of urban roads and evaluated the effectiveness of the proposed method through experiments. Experimental results show that our model outperforms existing advanced methods, achieving mIoU of 78.80% and 83.96% on B-scan and 3L-C-scan, and F1 scores of 87.65% and 91.07%, respectively. Moreover, the method has been deployed in Xiaoning Road GPR image intelligent recognition system and verified through on-site drilling, demonstrating its practical potential for road health monitoring.},
  keywords={Three-dimensional displays;Roads;Feature extraction;Accuracy;Image recognition;Transportation;Target recognition;Semantic segmentation;Monitoring;Ground penetrating radar;Ground penetrating radar;multi-perspective features;pavement subsurface object;semantic segmentation;attention mechanism},
  doi={10.1109/TITS.2025.3559498},
  ISSN={1558-0016},
  month={},}@INPROCEEDINGS{10391131,
  author={Rastegarpanah, Alireza and Contreras, Cesar Alan and Stolkin, Rustam},
  booktitle={2023 Eleventh International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
  title={Harnessing CNN-DNC and CNN-LSTM-DNC Architectures for Enhanced Lithium-Ion Remaining Useful Life Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={116-121},
  abstract={This research explores the application of the Differentiable Neural Computer (DNC), a Neural Turing Machine model, in predicting the Remaining Useful Life (RUL) of Lithium-Ion Batteries, especially within Renewable Energy Systems and electric vehicle domains. The introduction of two novel models, CNN-DNC and CNN-LSTM-DNC, marks a significant advancement in this task. An extensive evaluation on a dataset comprising 124 Lithium-Ion Battery cells highlights the CNN-DNC’s exemplary performance, delivering an MAE of 80.133 cycles and a loss value of 0.0037, outperforming the CNN- LSTM-DNC which reported an MAE of 99.028 cycles and a loss value of 0.0064. Despite utilizing 95.2% fewer parameters and achieving a 19.1% improvement in prediction accuracy, the CNN-DNC demands more training time compared to the CNN- LSTM-DNC. This comprehensive study not only highlights the potential of integrating DNC for precise RUL predictions but also sets a pathway for future research. The emphasis on enhancing these promising models and testing their practical applicability aims to bolster the reliability and efficiency of Lithium-Ion Batteries in various critical applications.},
  keywords={Lithium-ion batteries;Training;Renewable energy sources;Computational modeling;Computer architecture;Predictive models;Transformers;Remaining Useful Life;lithium-ion battery;Differentiable Neural Computer;Convolutional Neural Network;Long Short-Term Memory},
  doi={10.1109/ICICIS58388.2023.10391131},
  ISSN={2831-5952},
  month={Nov},}@INPROCEEDINGS{10270899,
  author={Zheng, Jingyi and Feng, Ziqin and Li, Yuexin and Liang, Fan and Cao, Xuan and Ge, Linqiang},
  booktitle={2023 8th International Conference on Signal and Image Processing (ICSIP)}, 
  title={Topological Data Analysis for Scalp EEG Signal Processing}, 
  year={2023},
  volume={},
  number={},
  pages={549-553},
  abstract={Topological Data Analysis is a fast-growing and promising approach that recently gains popularity in the data science field. It utilizes topological and geometric measurements to describe the structure, for example the shape, of complex data, which is fundamental and important for modeling the data. Scalp Electroencephalography (EEG) is widely used in clinical trials and scientific research to measure the brain activities. However, analyzing and modeling scalp EEG signals is still an open field due to the complex and non-stationary nature of the EEG signal itself as well as the transformed signals. Therefore, in this paper, we propose a topological-based processing pipeline that utilizes persistent homology to capture the underlying system dynamic of the transformed EEG signals and further construct machine learning classifiers. A public available scalp EEG data is used to validate our algorithms, and the results show that the topological features successfully capture the subtle changes in the time-frequency representations revealed by Hilbert-Huang Transformation, with area under ROC curve reaching 0.96.},
  keywords={Wavelet transforms;Time-frequency analysis;System dynamics;Scalp;Pipelines;Signal processing algorithms;Feature extraction;Topological Data Analysis;Scalp EEG;Hilbert-Huang Transformation;Machine Learning},
  doi={10.1109/ICSIP57908.2023.10270899},
  ISSN={},
  month={July},}@INPROCEEDINGS{10849396,
  author={Karim, Muhammad Faraz and Deng, Yunjie and Niu, Luyao and Ramasubramanian, Bhaskar and Alexiou, Michail and Sahabandu, Dinuka and Poovendran, Radha and Mertoguno, Sukarno},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Rapid Autonomy Transfer in Reinforcement Learning with a Single Pre- Trained Critic}, 
  year={2024},
  volume={},
  number={},
  pages={1000-1007},
  abstract={Reinforcement learning (RL) is a well-studied framework to solve complex decision-making problems in unknown environments. The actor-critic model in RL facilitates autonomy transfer by allowing agents to iteratively update their policies using ongoing dynamic evaluations of value functions via a critic. In this paper, we examine the impact of using different pretrained critics on the performance of actor-critic algorithms. First, in any single given environment, we show that a pretrained critic can be effective in reducing the duration of an initial training phase, thereby accelerating convergence by a factor of up to 2×. In this setting, we identify the critical range of the number of episodes for which a critic will need to be trained in order for it to be an effective pretrained critic. Second, we show that a critic trained in one environment enables transfer of autonomy by aiding learning of behaviors in a different, yet related environment. We carry out extensive experiments on a bipedal locomotion task in the MuJoCo physics engine to verify our hypotheses. Our results in this paper mark the first step towards demonstrating the role and impact of pretrained critics to achieve rapid autonomy transfer for complex reinforcement learning tasks while minimizing costs associated with retraining in new environments.},
  keywords={Training;Legged locomotion;Decision making;Reinforcement learning;Learning (artificial intelligence);Information processing;Stairs;Robots;Physics;Optimization;pre trained critics;state action space;deep reinforcement learning;pre trained critic;initial training phase;actor critic reinforcement learning models;actor model;episodes by actor models;rewards attained against episodes;pretrained critic;baseline critic versus a pre;actor critic algorithms;total rewards attained;actor critic reinforcement learning;critic trained;extensive experiments on a bipedal locomotion task;rapid autonomy transfer for complex reinforcement;reward machines;normal plane;critic models;expert demonstrations;reinforcement learning rl;pre trained models;proximal policy optimization;rate of convergence;parallel actor critic;normal stairs environment;performance of actor critic reinforcement;related environment;neural information processing systems;relevant knowledge;proximal policy optimization algorithms;local maxima;tools with artificial intelligence;complex decision making problems;bouncy plane;normal stairs;baseline critic;critic s value function;global maxima of rewards;pre trained critic taken;pre trained critic models;bouncy plane critic model;train critics from scratch;multi critic actor learning;dependent on specific regions;selecting pre trained critics;japan virtual robotics challenge;training and deployed environments;complex reinforcement learning tasks;bipedal walking;flat plane;learning phase;complex environments},
  doi={10.1109/ICTAI62512.2024.00143},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10019010,
  author={Ohata, Kazuya and Kitada, Shunsuke and Iyatomi, Hitoshi},
  booktitle={2022 IEEE 19th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET)}, 
  title={Feedback is Needed for Retakes: An Explainable Poor Image Notification Framework for the Visually Impaired}, 
  year={2022},
  volume={},
  number={},
  pages={166-171},
  abstract={We propose a simple yet effective image captioning framework that can determine the quality of an image and notify the user of the reasons for any flaws in the image. Our framework first determines the quality of images and then generates captions using only those images that are determined to be of high quality. The user is notified by the flaws feature to retake if image quality is low, and this cycle is repeated until the input image is deemed to be of high quality. As a component of the framework, we trained and evaluated a low-quality image detection model that simultaneously learns difficulty in recognizing images and individual flaws, and we demonstrated that our proposal can explain the reasons for flaws with a sufficient score. We also evaluated a dataset with low-quality images removed by our framework and found improved values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr), confirming an improvement in general-purpose image captioning capability. Our framework would assist the visually impaired, who have difficulty judging image quality.},
  keywords={Image quality;Measurement;Image recognition;Smart cities;Proposals;Artificial intelligence;image captioning;image quality assessment;image recognition;multi-task learning},
  doi={10.1109/HONET56683.2022.10019010},
  ISSN={1949-4106},
  month={Dec},}@INPROCEEDINGS{11071223,
  author={Das, Rajdeep and Bhattacharjee, Sattwik and Raj, Abhishek and Patel, Vishal Kumar and Das, Sudipta and Mitra, Chandreyee and Laha, Mahormi and De, Indrajit and Roy, Deepsubhra Guha},
  booktitle={2025 4th OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 5.0}, 
  title={Navigating Brain Tumor Complexity: A Deep Learning Approach for Multimodal Segmentation Framework with 2D U-NET Architecture}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Significant advancement of deep learning is responsible for the development of tools that facilitates recognition and subdivision of Brain Tumors on magnetic resonance imaging (MRI). The importance of such physical examination at the beginning is invaluable to aid the patient’s subsequent clinical care, and also it is of help in doctor suspecting the illness of their patients and the particular area of the brain affected so that it could result in the accurate treatment given by the doctor. The aim is to develop an early brain-tumour detection model with a future approach toward deep learning methodology that will also focus on checking the accuracy, in both aspects - current or upcoming advancement in field. A model has been developed based on the 2D U-Net architecture, an advanced deep learning approach for multimodal segmentation in brain tumours, for providing precise tumour predictions and identification of affected areas for aiding in the survival prognosis of patients. The proposed model's accuracy using 2D U–Net architecture at the training data is 95.13% and it turns out that the testing data accuracy of the proposed model is 82.14%. This research will contribute a great deal to the multimodal medical image analysis in the near future.},
  keywords={Deep learning;Accuracy;Image analysis;Magnetic resonance imaging;Brain tumors;Predictive models;Brain modeling;Feature extraction;Data models;Convolutional neural networks;Brain Tumor Detection;Deep learning;Magnetic reasoning Images (MRI);Convolutional Neural Network (CNN)},
  doi={10.1109/OTCON65728.2025.11071223},
  ISSN={},
  month={April},}@INPROCEEDINGS{10714820,
  author={S, Akilesh Kumar and P, Hariharan and N, Sukhel Ahamed and S, Gokilavani.},
  booktitle={2024 8th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Lightweight Fog Attention Graph Convolutional Network for Secure Data Aggregation Mechanism to Fog based Cloud Assisted IoT Network}, 
  year={2024},
  volume={},
  number={},
  pages={176-181},
  abstract={Ever Increasing demand of Cloud Computing paradigm has resulted in widespread development and deployment of multiple fog nodes to cloud assisted internet of things networks as it is highly capable of providing the users with on demand and low latency services through mobile collaborative devices in the edge of multiple clouds. Mobile Collaborative Devices is used for fog computing with integrated storage, computing and communication capabilities. However it offers improved efficiency and increased flexibility. Despite of multiple advantageous of fog based cloud assisted internet of things, security of the user data and their privacy can be compromised during data aggregation. In addition, it faces huge challenges in effectively aggregating the data and transmitting to the cloud server on establishing the strong security. In order to enhance the security of the user data and to preserve the privacy of the user information along establishing a secure data aggregation and transmission, a new light weight convolutional attention network is proposed in this article. It establishes secure data aggregation and data transmission to the distributed cloud data centers efficiently. Fog attention graph convolutional network leverages neighbour fog data efficiently and securely from dataset and represents in form of graph. Attention coefficient is assigned to each fog data represented in graph structure and attention score is calculated to each update of the nodes in graph. Specifically softmax function employs ID3 decision tree classifier and K anonymization mechanism to aggregates fog data on basis of updates of the attributes of the fog nodes in terms of the attention score and transmits aggregated data to the distributed cloud server. Further proposed model enhance the security of the data aggregation and transmission with index function through hash mechanism. Experimental analysis is carried out on the Fog assisted IoT medical (FIoMT) dataset. Dataset composed of patient health data collected using fog devices on various locations. Finally performance analysis of the secure data aggregation of fog IoT data using proposed architecture provides increased data integrity and increased prediction accuracy on basis of completeness, consistency, precision, recall and measures against state of art approaches.},
  keywords={Measurement;Cloud computing;Data privacy;Distributed databases;Data aggregation;Data models;Performance analysis;Security;Servers;Internet of Things;Fog based Cloud Assisted IoT network;Graph convolutional Network;Attention Network;Secure Data Aggregation},
  doi={10.1109/I-SMAC61858.2024.10714820},
  ISSN={2768-0673},
  month={Oct},}@ARTICLE{10798430,
  author={Gabrielson, Ben and Yang, Hanlu and Vu, Trung and Calhoun, Vince and Adali, Tülay},
  journal={IEEE Access}, 
  title={Mode Coresets for Efficient, Interpretable Tensor Decompositions: An Application to Feature Selection in fMRI Analysis}, 
  year={2024},
  volume={12},
  number={},
  pages={192356-192376},
  abstract={Generalizations of matrix decompositions to multidimensional arrays, called tensor decompositions, are simple yet powerful methods for analyzing datasets in the form of tensors. These decompositions model a data tensor as a sum of rank-1 tensors, whose factors provide uses for a myriad of applications. Given the massive sizes of modern datasets, an important challenge is how well computational complexity scales with the data, balanced with how well decompositions approximate the data. Many efficient methods exploit a small subset of the tensor’s elements, representing most of the tensor’s variation via a basis over the subset. These methods’ efficiencies are often due to their randomized natures; however, deterministic methods can provide better approximations, and can perform feature selection, highlighting a meaningful subset that well-represents the entire tensor. In this paper, we introduce an efficient subset-based form of the Tucker decomposition, by selecting coresets from the tensor modes such that the resulting core tensor can well-approximate the full tensor. Furthermore, our method enables a novel feature selection scheme unlike other methods for tensor data. We introduce methods for random and deterministic coresets, minimizing error via a measure of discrepancy between the coreset and full tensor. We perform the decompositions on simulated data, and perform on real-world fMRI data to demonstrate our method’s feature selection ability. We demonstrate that compared with other similar decomposition methods, our methods can typically better approximate the tensor with comparably low computational complexities.},
  keywords={Tensors;Matrix decomposition;Vectors;Feature extraction;Functional magnetic resonance imaging;Costs;Computational complexity;Singular value decomposition;Optimization;Matrix converters;Tensor decomposition;tucker decomposition;higher order singular value decomposition;coresets;tensor CUR decomposition;subset selection;feature selection;fMRI},
  doi={10.1109/ACCESS.2024.3517338},
  ISSN={2169-3536},
  month={},}@ARTICLE{10516260,
  author={Liu, Huan and Gong, Liangyi and Mo, Xiuliang and Dong, Guozhong and Yu, Jie},
  journal={IEEE Internet of Things Journal}, 
  title={LTAChecker: Lightweight Android Malware Detection Based on Dalvik Opcode Sequences Using Attention Temporal Networks}, 
  year={2024},
  volume={11},
  number={14},
  pages={25371-25381},
  abstract={Android applications have emerged as a prime target for hackers. Android malware detection stands as a pivotal technology, crucial for safeguarding network security and thwarting anomalies. However, traditional static analysis makes it difficult to analyze new malicious applications, while dynamic analysis requires higher system resources. We propose a novel lightweight Android malware deep-learning detection framework based on attention temporal networks. This study delves into the Dalvik opcode sequences of Android malware, employing the N-gram algorithm to partition sequences and extract contextual information features. Then, long short-term memory and temporal convolutional network (TCN) algorithms are employed to capture long-term dependencies and local features, enabling comprehensive comprehension of temporal information within Dalvik opcode sequences. Especially, TCN facilitates feature extraction across various time scales, thereby enabling the detection of anomaly patterns across diverse temporal scales within Dalvik opcode sequences. Moreover, we introduce multihead attention mechanisms and reinforced learning to direct the model’s focus toward behavioral cues within malicious software sequences. Finally, extensive experiment results show that our proposed methodology and model exhibit higher detection accuracy and robustness, achieving an accuracy rate of 98.69% on average, surpassing traditional machine learning methods, such as random forest and pseudo-label deep neural networks.},
  keywords={Feature extraction;Malware;Operating systems;Codes;Static analysis;Genetic algorithms;Data models;Android malware;attention temporal networks;long short-term memory (LSTM)–temporal convolutional network (TCN);multihead-attention;opcode sequences},
  doi={10.1109/JIOT.2024.3394555},
  ISSN={2327-4662},
  month={July},}@INPROCEEDINGS{10340587,
  author={Hanif, Umaer and Gimenez, Ulysse and Cairns, Alyssa and Lewin, Daniel and Ashraf, Nisar and Mignot, Emmanuel},
  booktitle={2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Automatic Detection of Chronic Insomnia from Polysomnographic and Clinical Variables Using Machine Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Insomnia is defined subjectively by the presence and frequency of specific clinical symptoms and an association with distress. Although sleep study data has shown some weak associations, no objective test can currently be used to predict insomnia. The purpose of this study was to use previously reported and relatively crafted insomnia-related polysomnographic variables in machine learning models to classify groups with and without insomnia. Demographics, diagnosed depression, Epworth Sleepiness Scale (ESS), and features derived from electroencephalography (EEG), arousals, and sleep stages from 3,407 sleep clinic patients (2,617 without insomnia and 790 insomnia patients based on responses to a set of questions) were included in this analysis. The number of features were reduced using pair-wise correlation and recursive feature elimination. Predictive value of three machine learning models (logistic regression, neural network, and support vector machine) was investigated, and the best performance was achieved with logistic regression, yielding a balanced accuracy of 71%. The most important features in predicting insomnia were depression, age, sex, duration of longest arousal, ESS score, and EEG power in theta and sigma bands across all sleep stages. Results indicate potential of machine learning-based screening for insomnia using clinical variables and EEG.},
  keywords={Support vector machines;Logistic regression;Correlation;Sleep;Neural networks;Machine learning;Predictive models},
  doi={10.1109/EMBC40787.2023.10340587},
  ISSN={2694-0604},
  month={July},}@ARTICLE{11016818,
  author={Lee, Gyeonggeon and Shi, Lehong and Latif, Ehsan and Gao, Yizhu and Bewersdorff, Arne and Nyaaba, Matthew and Guo, Shuchen and Liu, Zhengliang and Mai, Gengchen and Liu, Tianming and Zhai, Xiaoming},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Multimodality of AI for Education: Toward Artificial General Intelligence}, 
  year={2025},
  volume={18},
  number={},
  pages={666-683},
  abstract={This article addresses the growing importance of understanding how multimodal artificial general intelligence (AGI) can be integrated into educational practices. We first reviewed the theoretical foundations of multimodality in human learning, encompassing its concept and history, dual coding theory and multimedia theory, VARK multimodality, and multimodal assessment (see Section II-A). After that, we revisited the essential components of AGI, particularly focusing on the multimodal nature of AGI that distinguished it from artificial narrow intelligence. Based on its conversational functionality, multimodal AGI is considered an educational agent already tested in various educational situations (see Section II-B). How significant text, image, audio, and video modalities are for education, the technological backgrounds of AGI for analyzing and generating them, and educational applications of artificial intelligence (AI) for each modality were thoroughly reviewed (Sections III–VI). Finally, we comprehensively investigated the ethics of AGI in education, originating from the ethics of AI and specified in three strands: first, data privacy and ethical integrity, second, explainability, transparency, and fairness, and third, responsibility and decision-making. Practical implementation of ethical AGI frameworks in education was reviewed (see Section VII). This article also discusses the implications for learning theories, derived operational design principles, current research gaps, practical constraints and institutional readiness, and future directions (see Section VIII). This exploration aims to provide an advanced understanding of the intersection between AI, multimodality, and education, setting a foundation for future research and development.},
  keywords={Artificial general intelligence;Education;Artificial intelligence;Visualization;Ethics;Media;Information processing;Training;Learning (artificial intelligence);History;Artificial general intelligence (AGI);artificial intelligence (AI);ChatGPT;education;generative pretrained Transformer (GPT)-4;Gemini;machine learning;multimodality},
  doi={10.1109/TLT.2025.3574466},
  ISSN={1939-1382},
  month={},}@INPROCEEDINGS{10612099,
  author={Harper, Matthew and Liu, Ivy and Xue, Bing and Vennell, Ross and Zhang, Mengjie},
  booktitle={2024 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Evaluating Machine Learning Techniques for Predicting Salinity}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Oyster farms provide a sustainable and profitable export for New Zealand. Oyster farms are sensitive to changes in salinity that can cause significant crop loss if they persist too long. Recent extreme weather events have been leading to increased periods of low salinity, putting the farms at risk. Machine learning based methods provide a way to predict these low salinity events and provide an early warning system, but this has not been investigated in aquaculture. In this paper, we investigate three different methods to assess the viability of salinity prediction systems. A simple statistical model, a genetic programming (GP) based symbolic regression model and a convolutional neural network (CNN) were compared as ways of solving this problem. The results show that GP based symbolic regression and CNNs are fairly good approaches to predicting salinity. However, as weather events get more extreme, the CNN approach tends to hold up better and can be generalised better, while the G P based symbolic regression models show better potential explainability with the tree based model structure. These results show promise and provide a good stepping off point at creating a generalised approach to predicting salinity in estuaries.},
  keywords={Training;Accuracy;Statistical analysis;Salinity (geophysical);System performance;Machine learning;Estuaries},
  doi={10.1109/CEC60901.2024.10612099},
  ISSN={},
  month={June},}@INPROCEEDINGS{10968388,
  author={R, Mahaveerakannan and Poonguzhali, P.K. and Dhar M S, Murali and Vidhya, R and Natteshan, N.V.S},
  booktitle={2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)}, 
  title={A Novel Deep Learning Framework for Intrusion Detection: Integrating Dual-Staged Attention and Squirrel Search Optimization for Enhanced Accuracy}, 
  year={2025},
  volume={},
  number={},
  pages={277-284},
  abstract={The rapid development of computer networks, such as the internet, has caused a sea change in the ways in which people communicate and share information. The advent of widespread computer networks like the internet has caused a sea change in traditional modes of communication and information sharing. On the other hand, this technical progress has also opened the door for malicious actors to take advantage of loopholes and steal sensitive information, disrupt operations, etc. Therefore, intrusion detection systems (IDSs) are essential for screening harmful traffic and warding off frequent attacks. These algorithms used to work by comparing current attacks to those of the past or by following a predetermined set of rules. But now that data and computing power are more accessible than ever before, machine learning seems to be the way to go. In this study, to present DSAM-CGLSTM, a dual-staged attention mechanism based on conversion-gated Long Short Term Memory network, to capture both short-term mutation and long-term dependence info in the study dataset. To enhance the network's capacity to retrieve the short-term mutation information, hyperbolic tangent functions are incorporated into the input-gate and forget-gate of LSTM. Additionally, the network incorporates a dual-staged attention mechanism, which encompasses both input attention and temporal attention. Consequently, a novel algorithm for optimising the parameters of DSAM-CGLSTM, SSA, has been suggested in this research, which is based on metaheuristics. to use a single Jupyter notebook in the Google Colaboratory environment to conduct the entire experiment. to imported and then implemented the necessary software packages, including seaborn, pandas, matplotlib, keras, besides Tensor Flow, and to used the CICIDS 2017 dataset. During the model's training and validation processes, accuracy and loss were taken into account as performance indicators. With a 98.85% accuracy score, the deep learning model performed quite well when tested on the CICIDS 2017 dataset for attack prediction.},
  keywords={Training;Deep learning;Accuracy;Attention mechanisms;Machine learning algorithms;Time series analysis;Intrusion detection;Predictive models;Feature extraction;Long short term memory;Intrusion detection systems;Squirrel search optimization algorithm;Privacy;Conversion-gated Long Short-Term Memory;Dual-staged mechanism},
  doi={10.1109/ICMLAS64557.2025.10968388},
  ISSN={},
  month={March},}@INPROCEEDINGS{10651301,
  author={Cui, Xueyao and Jiang, Huiyan and Zhou, Yang and Han, Xianhua and Li, Xuena and Pei, Yan},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Lesion Feature Extraction and Classification Optimization Method Using Dynamic Fusion of Global Attention and Local Attention}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In tumor diagnosis, due to subtle differences in the imaging appearance of different diseases, accurately classifying lesions based on solely imaging data proves challenging. Existing machine learning and deep learning methods face limitations due to the small sample size of medical datasets and the intricate nature of disease image manifestations. This paper proposes a novel lesion classification method to fully explore distinctions among confused lesion features associated with different diseases. The proposed method comprises three key steps: Firstly, a lesion feature calculation method using dynamic fusion of global attention and local attention is proposed. The weight of global attention and local attention is dynamically allocated, and the global and local features are fused by dynamic weight. Secondly, feature dimension reduction is realized to improve the effect of distinguishable features using sparse autoencoder and polynomial constraint loss function. Finally, to improve the performance of classification, the monarch butterfly optimization algorithm based on adaptive neighborhood search radius method is used to optimize the parameters of multi-kernel support vector machine. The private PET/CT image classification dataset of lymphoma and Still’s disease was used to validate our results. The experimental results demonstrate that the method's efficacy in lymphoma and Still’ disease classification tasks, achieving an accuracy (ACC) of 82.8% and an area under the curve (AUC) of 87.1%, respectively.},
  keywords={Support vector machines;Neural networks;Optimization methods;Feature extraction;Polynomials;Classification algorithms;Lesions;approximate disease classification;attention mechanism;dynamic fusion;adaptive optimization},
  doi={10.1109/IJCNN60899.2024.10651301},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10879491,
  author={Gao, Yuan and Tan, Tao and Wang, Xin and Beets-Tan, Regina and Zhang, Tianyu and Han, Luyi and Portaluri, Antonio and Lu, Chunyao and Liang, Xinglong and Teuwen, Jonas and Zhou, Hong-Yu and Mann, Ritse},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Multi-modal Longitudinal Representation Learning for Predicting Neoadjuvant Therapy Response in Breast Cancer Treatment}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={Longitudinal medical imaging is crucial for monitoring neoadjuvant therapy (NAT) response in clinical practice. However, mainstream artificial intelligence (AI) methods for disease monitoring commonly rely on extensive segmentation labels to evaluate lesion progression. While self-supervised vision-language (VL) learning efficiently captures medical knowledge from radiology reports, existing methods focus on single time points, missing opportunities to leverage temporal self-supervision for disease progression tracking. In addition, extracting dynamic progression from longitudinal unannotated images with corresponding textual data poses challenges. In this work, we explicitly account for longitudinal NAT examinations and accompanying reports, encompassing scans before NAT and follow-up scans during mid-/post-NAT. We introduce the multi-modal longitudinal representation learning pipeline (MLRL), a temporal foundation model, that employs multi-scale self-supervision scheme, including single-time scale vision-text alignment (VTA) learning and multi-time scale visual/textual progress (TVP/TTP) learning to extract temporal representations from each modality, thereby facilitates the downstream evaluation of tumor progress. Our method is evaluated against several state-of-the-art self-supervised longitudinal learning and multi-modal VL methods. Results from internal and external datasets demonstrate that our approach not only enhances label efficiency across the zero-, few- and full-shot regime experiments but also significantly improves tumor response prediction in diverse treatment scenarios. Furthermore, MLRL enables interpretable visual tracking of progressive areas in temporal examinations, offering insights into longitudinal VL foundation tools and potentially facilitating the temporal clinical decision-making process.},
  keywords={Visualization;Medical treatment;Representation learning;Monitoring;Predictive models;Image representation;Breast cancer;Bioinformatics;Training;Radiology;Temporal foundation model;vision-language representation learning;self-supervised learning;longitudinal medical imaging;neoadjuvant therapy response prediction;breast cancer},
  doi={10.1109/JBHI.2025.3540574},
  ISSN={2168-2208},
  month={},}@ARTICLE{9136839,
  author={Na, Xiaodong and Han, Min and Ren, Weijie and Zhong, Kai},
  journal={IEEE Transactions on Cybernetics}, 
  title={Modified BBO-Based Multivariate Time-Series Prediction System With Feature Subset Selection and Model Parameter Optimization}, 
  year={2022},
  volume={52},
  number={4},
  pages={2163-2173},
  abstract={Multivariate time-series prediction is a challenging research topic in the field of time-series analysis and modeling, and is continually under research. The echo state network (ESN), a type of efficient recurrent neural network, has been widely used in time-series prediction, but when using ESN, two crucial problems have to be confronted: 1) how to select the optimal subset of input features and 2) how to set the suitable parameters of the model. To solve this problem, the modified biogeography-based optimization ESN (MBBO-ESN) system is proposed for system modeling and multivariate time-series prediction, which can simultaneously achieve feature subset selection and model parameter optimization. The proposed MBBO algorithm is an improved evolutionary algorithm based on biogeography-based optimization (BBO), which utilizes an  $S$ -type population migration rate model, a covariance matrix migration strategy, and a Lévy distribution mutation strategy to enhance the rotation invariance and exploration ability. Furthermore, the MBBO algorithm cannot only optimize the key parameters of the ESN model but also uses a hybrid-metric feature selection method to remove the redundancies and distinguish the importance of the input features. Compared with the traditional methods, the proposed MBBO-ESN system can discover the relationship between the input features and the model parameters automatically and make the prediction more accurate. The experimental results on the benchmark and real-world datasets demonstrate that MBBO outperforms the other traditional evolutionary algorithms, and the MBBO-ESN system is more competitive in multivariate time-series prediction than other classic machine-learning models.},
  keywords={Sociology;Statistics;Optimization;Predictive models;Reservoirs;Feature extraction;Biological system modeling;Biogeography-based optimization (BBO);feature selection;multivariate time series;parameter optimization;prediction},
  doi={10.1109/TCYB.2020.2977375},
  ISSN={2168-2275},
  month={April},}@ARTICLE{10155473,
  author={Ma, Chong and Zhao, Lin and Chen, Yuzhong and Wang, Sheng and Guo, Lei and Zhang, Tuo and Shen, Dinggang and Jiang, Xi and Liu, Tianming},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Eye-Gaze-Guided Vision Transformer for Rectifying Shortcut Learning}, 
  year={2023},
  volume={42},
  number={11},
  pages={3384-3394},
  abstract={Learning harmful shortcuts such as spurious correlations and biases prevents deep neural networks from learning meaningful and useful representations, thus jeopardizing the generalizability and interpretability of the learned representation. The situation becomes even more serious in medical image analysis, where the clinical data are limited and scarce while the reliability, generalizability and transparency of the learned model are highly required. To rectify the harmful shortcuts in medical imaging applications, in this paper, we propose a novel eye-gaze-guided vision transformer (EG-ViT) model which infuses the visual attention from radiologists to proactively guide the vision transformer (ViT) model to focus on regions with potential pathology rather than spurious correlations. To do so, the EG-ViT model takes the masked image patches that are within the radiologists’ interest as input while has an additional residual connection to the last encoder layer to maintain the interactions of all patches. The experiments on two medical imaging datasets demonstrate that the proposed EG-ViT model can effectively rectify the harmful shortcut learning and improve the interpretability of the model. Meanwhile, infusing the experts’ domain knowledge can also improve the large-scale ViT model’s performance over all compared baseline methods with limited samples available. In general, EG-ViT takes the advantages of powerful deep neural networks while rectifies the harmful shortcut learning with human expert’s prior knowledge. This work also opens new avenues for advancing current artificial intelligence paradigms by infusing human intelligence.},
  keywords={Transformers;Analytical models;Medical diagnostic imaging;Deep learning;Task analysis;Solid modeling;Pathology;Eye tracking;generalizability;interpretability;shortcut learning;vision transformer},
  doi={10.1109/TMI.2023.3287572},
  ISSN={1558-254X},
  month={Nov},}@ARTICLE{10136815,
  author={Bartlett, Deaglan J. and Desmond, Harry and Ferreira, Pedro G.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Exhaustive Symbolic Regression}, 
  year={2024},
  volume={28},
  number={4},
  pages={950-964},
  abstract={Symbolic regression (SR) algorithms attempt to learn analytic expressions which fit data accurately and in a highly interpretable manner. Conventional SR suffers from two fundamental issues which we address here. First, these methods search the space stochastically (typically using genetic programming) and hence do not necessarily find the best function. Second, the criteria used to select the equation optimally balancing accuracy with simplicity have been variable and subjective. To address these issues we introduce exhaustive SR (ESR), which systematically and efficiently considers all possible equations—made with a given basis set of operators and up to a specified maximum complexity—and is therefore guaranteed to find the true optimum (if parameters are perfectly optimized) and a complete function ranking subject to these constraints. We implement the minimum description length principle as a rigorous method for combining these preferences into a single objective. To illustrate the power of ESR we apply it to a catalog of cosmic chronometers and the Pantheon+ sample of supernovae to learn the Hubble rate as a function of redshift, finding ~40 functions (out of 5.2 million trial functions) that fit the data more economically than the Friedmann equation. These low-redshift data therefore do not uniquely prefer the expansion history of the standard model of cosmology. We make our code and full equation sets publicly available.},
  keywords={Mathematical models;Complexity theory;Optimization;Numerical models;Biological system modeling;Standards;Search problems;Cosmology data analysis;minimum description length;model selection;symbolic regression (SR)},
  doi={10.1109/TEVC.2023.3280250},
  ISSN={1941-0026},
  month={Aug},}@INPROCEEDINGS{9825291,
  author={Cao, Bo and Li, Chenghai and Sun, Junfeng and Song, Yafei},
  booktitle={2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)}, 
  title={IoT intrusion detection technology based on Deep learning}, 
  year={2022},
  volume={},
  number={},
  pages={284-289},
  abstract={To address the problem of low accuracy of existing network intrusion detection models for multi-classification of intrusion behaviors and redundancy of data features, a network intrusion detection model incorporating convolutional neural networks and gated recurrent units is proposed. To solve the problem of feature redundancy, feature selection is performed by combining random forest algorithm and Pearson correlation analysis; after that, temporal features of data are extracted by TCN and GRU, while attention module is introduced to assign different weights to features, thus reducing overhead and improving model performance; finally, Softmax function is used for classification. In this paper, the proposed model is evaluated on the Bot-Lot dataset with an accuracy of 99.99%.},
  keywords={Deep learning;Fuses;Redundancy;Diversity reception;Network intrusion detection;Logic gates;Feature extraction;IoT Intrusion Detection;Deep Learning;TCN;GRU;PSO},
  doi={10.1109/CVIDLICCEA56201.2022.9825291},
  ISSN={},
  month={May},}@INPROCEEDINGS{9943519,
  author={Rahaman, Md Abdur and Garg, Yash and Iraji, Armin and Fu, Zening and Chen, Jiayu and Calhoun, Vince},
  booktitle={2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={Two-Dimensional Attentive Fusion for Multi-Modal Learning of Neuroimaging and Genomics Data}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Human exposure to reality is multi-modal, and the brain processes it through multi-sensory stimulation. As such, using multi-source intelligence can potentially improve results motivated by human learning. The key challenge in multi-modal learning is to integrate the modalities through a sensible fusion. We propose mBAM - a novel fusion technique inspired by the bottleneck attention module (BAM) to leverage the knowledge from diverse data modes. We combine this module with a deep multi-modal framework for classifying mental disorders. The joint architecture extracts relevant features from diverse inputs - from brain imagery to genomic variables to classify schizophrenia. The model's prediction accuracy is 95.6% (P < 0.0001), outperforming state-of-the-art unimodal and multi-modal models for the task. Moreover, the scheme provides inherent interpretability that helps identify concepts significant for the neural network's decision and explains the underlying factors of the diseases.},
  keywords={Neuroimaging;Mental disorders;Genomics;Predictive models;Signal processing;Feature extraction;Brain modeling;Task analysis;Bioinformatics;Diseases;Multi-modal deep learning;bottleneck attention module (BAM);imaging-genetics;schizophrenia;mental disorder classification},
  doi={10.1109/MLSP55214.2022.9943519},
  ISSN={2161-0371},
  month={Aug},}@INPROCEEDINGS{10878663,
  author={Wang, Jing and Gu, Shuhuai and Xi, Qi and Ou, Fenjie},
  booktitle={2024 11th International Forum on Electrical Engineering and Automation (IFEEA)}, 
  title={A Two-stage Self-adaptive Method for Optimal Neural Network Design for Various Time Series Datasets}, 
  year={2024},
  volume={},
  number={},
  pages={1223-1226},
  abstract={To deal with time series data prediction, engineers face the problem of designing suitable neural networks. The difficulties lie in three folds. First, many types of neural networks can be chosen as candidate neural networks. However, which one is the most suitable cannot be known in advance. Thus lots of try-and-errors happen. Second, even the type of neural network is determined, the structure of this type of neural network still be unknown. For example, how many layers should we use, how many neurons in each layer should be designed, and so on. This kind of problems cause another round of try-and-errors with limited return. Third, many types of neurons can be mixed in a neural network, for example, LSTM+GRU, TCN+LSTM and so forth. Those combinations are so many that engineers do not know which combination will lead to the best result, thus resulting in another round of unintended try-and-errors. Those above-mentioned phenomena make neural networks difficult to apply for engineers in real application. Thus this paper proposes a two-stage self-adaptive method for neural network design. The first stage will choose the best neural network structures and the second stage optimizes the network weights to further improve the performance of the neural network. Any dataset given to our method will obtain the most suitable neural network with satisfying results. The proposed method is tested on an open dataset and also successfully applied to a real engineering dataset.},
  keywords={Electrical engineering;Automation;Attention mechanisms;Design methodology;Time series analysis;Neurons;Biological neural networks;Faces;time series;neural network structure optimization;neural network weights optimization;NSGA-II;PSO;TCN;LSTM;GRU;Bi-LSTM;Bi-GRU},
  doi={10.1109/IFEEA64237.2024.10878663},
  ISSN={},
  month={Nov},}@ARTICLE{9178453,
  author={Pinckaers, Hans and van Ginneken, Bram and Litjens, Geert},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Streaming Convolutional Neural Networks for End-to-End Learning With Multi-Megapixel Images}, 
  year={2022},
  volume={44},
  number={3},
  pages={1581-1590},
  abstract={Due to memory constraints on current hardware, most convolution neural networks (CNN) are trained on sub-megapixel images. For example, most popular datasets in computer vision contain images much less than a megapixel in size (0.09MP for ImageNet and 0.001MP for CIFAR-10). In some domains such as medical imaging, multi-megapixel images are needed to identify the presence of disease accurately. We propose a novel method to directly train convolutional neural networks using any input image size end-to-end. This method exploits the locality of most operations in modern convolutional neural networks by performing the forward and backward pass on smaller tiles of the image. In this work, we show a proof of concept using images of up to 66-megapixels (8192×8192), saving approximately 50GB of memory per image. Using two public challenge datasets, we demonstrate that CNNs can learn to extract relevant information from these large images and benefit from increasing resolution. We improved the area under the receiver-operating characteristic curve from 0.580 (4MP) to 0.706 (66MP) for metastasis detection in breast cancer (CAMELYON17). We also obtained a Spearman correlation metric approaching state-of-the-art performance on the TUPAC16 dataset, from 0.485 (1MP) to 0.570 (16MP). Code to reproduce a subset of the experiments is available at https://github.com/DIAGNijmegen/StreamingCNN.},
  keywords={Memory management;Convolution;Convolutional neural networks;Backpropagation;Streaming media;Task analysis;Training;Deep learning;convolutional neural networks;image classification;high-resolution images},
  doi={10.1109/TPAMI.2020.3019563},
  ISSN={1939-3539},
  month={March},}@ARTICLE{9609681,
  author={Wang, Sutong and Zhu, Jiacheng and Yin, Yunqiang and Wang, Dujuan and Edwin Cheng, T.C. and Wang, Yanzhang},
  journal={IEEE Transactions on Multimedia}, 
  title={Interpretable Multi-Modal Stacking-Based Ensemble Learning Method for Real Estate Appraisal}, 
  year={2023},
  volume={25},
  number={},
  pages={315-328},
  abstract={With the development of online real estate trading platforms, multi-modal housing trading data, including structural information, location, and interior image data, are being accumulated. The accurate appraisal of real estate makes sense for government officials, urban policymakers, real estate sellers, and personal purchasers. In this study, we propose an interpretable multi-modal stacking-based ensemble learning (IMSEL) method that deals with various modalities for real estate appraisals. We crawl the structural and image data of real estate in Chengdu city, China from the nation's largest real estate transaction platform with the location information, including public services, within 2 km of the real estate using Baidu map. We then compare the predictive results from IMSEL with those from previous state-of-art methods in the literature in terms of the root mean square error, mean absolute percentage error, mean absolute error, and coefficient of determination (R2). The comparison results show that IMSEL outperformed the other methods. We verified the improvement of introducing a data transformation strategy and deep visual features through a 10-fold cross-validation. We also discuss the managerial implications of our research findings.},
  keywords={Appraisal;Learning systems;Predictive models;Costs;Regression tree analysis;Metadata;Genetic algorithms;Real estate appraisal;multi-modal convolutional neural network;interior images;geographical locations},
  doi={10.1109/TMM.2021.3126153},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10364142,
  author={Adalioglu, Ilke and Ahishali, Mete and Degerli, Aysen and Kiranyaz, Serkan and Gabbouj, Moncef},
  booktitle={2023 Computing in Cardiology (CinC)}, 
  title={SAF-Net: Self-Attention Fusion Network for Myocardial Infarction Detection Using Multi-View Echocardiography}, 
  year={2023},
  volume={50},
  number={},
  pages={1-4},
  abstract={Myocardial infarction (MI) is a severe case of coronary artery disease (CAD) and ultimately, its detection is sub-stantial to prevent progressive damage to the myocardium. In this study, we propose a novel view-fusion model named self-attention fusion network (SAF-Net) to detect MI from multi-view echocardiography recordings. The proposed framework utilizes apical 2-chamber (A2C) and apical 4-chamber (A4C) view echocardiography recordings for classification. Three reference frames are extracted from each recording of both views and deployed pre-trained deep networks to extract highly representative features. The SAF-Net model utilizes a self-attention mechanism to learn dependencies in extracted feature vectors. The proposed model is computationally efficient thanks to its compact architecture having three main parts: a feature embedding to reduce dimensionality, self-attention for view-pooling, and dense layers for the classification. Experimental evaluation is performed using the HMC-QU-TAU11The benchmark HMC-QU-TAU dataset is publicly shared at the repository https://www.kaggle.com/aysendegerli/hmcqu-dataset. dataset which consists of 160 patients with A2C and A4C view echocardiography recordings. The proposed SAF-Net model achieves a high-performance level with 88.26% precision, 77.64% sensitivity, and 78.13% accuracy. The results demonstrate that SAF-Net model achieves the most accurate MI detection over multi-view echocardiography recordings.},
  keywords={Solid modeling;Sensitivity;Echocardiography;Computational modeling;Medical services;Myocardium;Feature extraction},
  doi={10.22489/CinC.2023.240},
  ISSN={2325-887X},
  month={Oct},}@INPROCEEDINGS{10016374,
  author={Min, Rui and Wu, Ming and Xu, Mengqiu and Zhu, Xun},
  booktitle={2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)}, 
  title={Attention based Long Short-Term Memory Network for Coastal Visibility Forecast}, 
  year={2022},
  volume={},
  number={},
  pages={420-425},
  abstract={Visibility prediction in coastal areas has always been an important issue affecting the safety of residents and the efficiency of urban transportation. The visibility prediction methods currently used by meteorological centers are mainly based on the statistical forecast with relatively low prediction accuracy and high computational complexity. These methods cannot work well with large amounts of data. However, with the rapid development of deep learning technology, the use of deep learning has become a primary trend. In this paper, we propose our visibility prediction model based on (Long Short-Term Memory) LSTM network and self-attention mechanism. The model takes Medium-range Forecasts Data from European Centre for Mediumrange Weather Forecasting (ECMWF) which we use EC data to refer it for simplicity and observatory visibility data as input to predict and uses the LSTM network as the backbone to extract time series information. We also use self-attention mechanism to process the input data before the data is input to the model to let the model better focus on the valuable information for prediction. Compared with the predicted visibility in EC data, our proposed method improved the 3-hour prediction accuracy by 20%, 1.5 times, and 8 times for high-range, medium-range, and low-range visibility, respectively. We also find the data imbalance will greatly affect the prediction accuracy for low-visibility data and use the weighted-loss and mix-up data augmentation strategy model in our model training. We improved the accuracy of low-visibility data by 1.2 times while the prediction results of high-visibility and medium-visibility data remained almost the same. In addition, we conduct several experiments to verify the effectiveness of our model design and the rationality of data augmentation.},
  keywords={Training;Deep learning;Computational modeling;Time series analysis;Sea measurements;Weather forecasting;Transportation;Coastal visibility prediction;Deep learning;Long short-term memory;Self-attention;Data imbalance},
  doi={10.1109/CCIS57298.2022.10016374},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10039230,
  author={Kawano, Harunori and Shimizu, Sota},
  booktitle={2023 IEEE/SICE International Symposium on System Integration (SII)}, 
  title={ReAT: Long Text Encoder Representations with Attention Tokens}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={BERT is a Transformer-based language model with a self-attention mechanism and has made many advances in natural language processing. However, it is difficult for BERT to process long texts more than 1,000 words, and it is possible to process them only when reducing the number of sentences. In this paper, a novel language model, ReAT, is proposed and experimented. The authors aim at developing a language model which is as accurate as or more accurate than BERT when processing long texts without reducing the number of sentences. ReAT supports long texts by dividing a given long text into dozens of sentence units as its preprocessing. In order to verify effectiveness of ReAT, we conducted simple experiments using QQP and MRPC after pre-training. As their results, ReAT achieved 89.7 percent accuracy by the QQP test and 73.0 percent accuracy by the MRPC test, respectively, at a stage when pre-training had not been completed. ReAT achieved 32-times longer text processing than BERT (from 512 words to 16384 ones). The amount of computing the self-attention was reduced successfully. In addition, our proposed language model achieved to extract feature based on sentence-by-sentence.},
  keywords={Computational modeling;Bit error rate;System integration;Feature extraction;Transformers;Natural language processing;Task analysis},
  doi={10.1109/SII55687.2023.10039230},
  ISSN={2474-2325},
  month={Jan},}@INPROCEEDINGS{10150358,
  author={Wagh, Shubham Maroti},
  booktitle={2023 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, 
  title={Precise Human Activity Recognition for The OpenPack Challenge 2022}, 
  year={2023},
  volume={},
  number={},
  pages={259-261},
  abstract={This technical report provides an overview of our method for the OpenPack Challenge 2022, which is part of the Behavior Analysis and Recognition for Knowledge Discovery Workshop at PerCom 2023. The challenge involves proposing an activity recognition approach by predicting activity classes for each 1-second-long time slot using sensor data from packaging works in the logistics industry. To accomplish this, we employed various data augmentation techniques and trained a deep neural network that incorporates spatio-temporal features from multiple sensor time-series data and a self-attention mechanism for selecting and learning essential time points. Our model achieved an average macro F1-score of 91.12% and generalised well on the submission set. This experiment is a part of the challenge conducted by our team “Shubham Wagh” and secured 5th place in the competition.},
  keywords={Pervasive computing;Industries;Deep learning;Conferences;Neural networks;Writing;Packaging;ConvNet;Bi-LSTM;Self-Attention;Human-Activity-Recognition;Data-Augmentation},
  doi={10.1109/PerComWorkshops56833.2023.10150358},
  ISSN={2766-8576},
  month={March},}@ARTICLE{9565381,
  author={Guo, Liang and Yu, Yaoxiang and Gao, Hongli and Feng, Tingting and Liu, Yuekai},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Online Remaining Useful Life Prediction of Milling Cutters Based on Multisource Data and Feature Learning}, 
  year={2022},
  volume={18},
  number={8},
  pages={5199-5208},
  abstract={A milling cutter is one of the most important parts of machine tools. Its working status significantly influences the precision of workpiece. Due to the complex wear mechanism, the single sensor may be difficult to acquire the complete degradation information of milling cutters. Therefore, in this article, a feature learning based method is proposed to automatically extract features from multisource data and predict the remaining useful life of cutting tools in real time. First, a statistic-based method is constructed to detect and delete the outliers hidden in the monitoring data. Second, the clean data are input into a multiscale convolutional attention network (MSAN) to learn features and fuse multisource data. At last, the fused data are used to predict the remaining useful life of cutting tools in a regression layer. Compared with traditional tool life prediction methods, the proposed method is able to fuse multisource data through an attention feature learning model to conduct the life prediction of tools. Additionally, the data cleaning and model optimization methods are also proposed to promote engineering practicability. To validate the effectiveness of such method, the life testing experiments on milling cutters are conducted to obtain run-to-failure data. In those experiments, multisensor monitor data are acquired, which are used to conduct validation experiments testing the effectiveness of the proposed method. The results indicate the superiority of the proposed method in remaining useful life prediction milling cutters.},
  keywords={Tools;Milling;Feature extraction;Monitoring;Degradation;Vibrations;Predictive models;Convolutional attention network;genetic algorithm (GA);milling cutters;multisource data;remaining useful life prediction},
  doi={10.1109/TII.2021.3118994},
  ISSN={1941-0050},
  month={Aug},}@ARTICLE{9779572,
  author={Zhou, Lei and Yang, Dingye and Zhai, Xiaolin and Wu, Shichao and Hu, ZhengXi and Liu, Jingtai},
  journal={IEEE Robotics and Automation Letters}, 
  title={GA-STT: Human Trajectory Prediction With Group Aware Spatial-Temporal Transformer}, 
  year={2022},
  volume={7},
  number={3},
  pages={7660-7667},
  abstract={Human trajectory prediction is a crucial yet challenging problem, which is of fundamental importance to robotics and autonomous driving vehicles. The core challenge lies in effectively modeling the socially aware spatial interaction and complex temporal dependencies among crowds. However, previous methods usually model spatial and temporal information separately and only use individual features, while ignoring group-level motion characteristics. We propose a novel trajectory prediction framework termed GA-STT, a group aware spatial-temporal transformer network to address these issues. Specifically, we first get the individual representations supervised by group-based annotations. Then, we model the complex spatial-temporal interactions with spatial and temporal transformers separately and fuse spatial-temporal embedding through the cross-attention mechanism. Results on the publicly available ETH/UCY datasets show that our model outperforms the state-of-the-art method by 19.4% in average displacement error(ADE) and 16.9% in final displacement error(FDE) and successfully predicts complex spatial-temporal interactions.},
  keywords={Trajectory;Transformers;Feature extraction;Predictive models;Computational modeling;Task analysis;Annotations;Human trajectory predicton;social group;spatial-temporal transformer;cross attention},
  doi={10.1109/LRA.2022.3176064},
  ISSN={2377-3766},
  month={July},}@ARTICLE{10214047,
  author={Wu, Jing-Yuan and Huang, Ping and Luc, Quang-Ho and Ko, Hua-Lun and Chiang, Yung-Chun and Yu, Hsiang-Chan and Chen, Mu-Yu and Chang, Edward. Yi},
  journal={IEEE Transactions on Nanotechnology}, 
  title={High Performance Inversion-Mode In0.53Ga0.47As FinFETs for Logic and RF Applications}, 
  year={2023},
  volume={22},
  number={},
  pages={445-448},
  abstract={An inversion-mode In0.53Ga0.47As FinFET exhibiting a transition frequency (fT) of 271 GHz and a maximum oscillation frequency (fmax) of 78 GHz at Vds = 0.5 V is reported in this work. Simultaneously, the transconductance (gm) of 2056 (μS/μm) and drain induced barrier lowering (DIBL) of 121 (mV/V) obtained were also exhibited. Moreover, a subthreshold swing (SS) of 86 and 97 (mV/dec) was obtained at Vds = 0.05 and 0.5 V, respectively. These exceptional properties are the result of employing N2 remote plasma treatment, enhancing the quality of high-k/III-V interface and carrier mobility. The fin structure contributes to good suppression of short channel effects (SCEs), and the superior transfer characteristics are due to the improved gate controllability. We also extracted an effective electron velocity (νeff) of 2.18 × 107 (cm/s) by delay time analysis, providing a probable explanation for the obtained fT performance. This work explicitly exhibits the potential of high-performance inversion-mode InGaAs FinFETs, positioning them as promising candidates for future high-frequency system applications.},
  keywords={Delays;FinFETs;Logic gates;Indium gallium arsenide;Transconductance;Performance evaluation;InGaAs;FinFET;inversion channel layer;transition frequency},
  doi={10.1109/TNANO.2023.3303832},
  ISSN={1941-0085},
  month={},}
